@article{PLOUFFE201557,
title = {Comparing interpolation techniques for monthly rainfall mapping using multiple evaluation criteria and auxiliary data sources: A case study of Sri Lanka},
journal = {Environmental Modelling & Software},
volume = {67},
pages = {57-71},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000328},
author = {Cameron C.F. Plouffe and Colin Robertson and Lalith Chandrapala},
keywords = {Spatial interpolation, Rainfall prediction, Kriging, Raster comparison, Sri Lanka},
abstract = {Interpolating climatic variables such as rainfall is challenging due to the highly variable nature of meteorological processes, the effects of terrain and geography, and the difficulty in establishing a representative network of stations. While interpolation models are being adapted to include these effects, often the rainfall data contain significant gaps in coverage. In this paper, we evaluated rainfall data from an agro-ecological monitoring network for producing maps of total monthly rainfall in Sri Lanka. We compared four spatial interpolation techniques: inverse distance weighting, thin-plate splines, ordinary kriging, and Bayesian kriging. Error metrics were used to validate interpolations against independent data. Satellite data were used to assess the spatial pattern of rainfall. Results indicated that Bayesian kriging and splines performed best in low and high rainfall, respectively. Rainfall maps generated from the agro-ecological network were found to have accuracies consistent with previous studies in Sri Lanka.}
}
@article{SNYDER2004444,
title = {Exponential smoothing models: Means and variances for lead-time demand},
journal = {European Journal of Operational Research},
volume = {158},
number = {2},
pages = {444-455},
year = {2004},
note = {Methodological Foundations of Multi-Criteria Decision Making},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(03)00360-6},
url = {https://www.sciencedirect.com/science/article/pii/S0377221703003606},
author = {Ralph D Snyder and Anne B Koehler and Rob J Hyndman and J.Keith Ord},
keywords = {Forecasting, Inventory, Exponential smoothing, Forecast variance, Lead-time demand, Safety stocks},
abstract = {Exponential smoothing is often used to forecast lead-time demand (LTD) for inventory control. In this paper, formulae are provided for calculating means and variances of LTD for a wide variety of exponential smoothing methods. A feature of many of the formulae is that variances, as well as the means, depend on trends and seasonal effects. Thus, these formulae provide the opportunity to implement methods that ensure that safety stocks adjust to changes in trend or changes in season. An example using weekly sales shows how safety stocks can be seriously underestimated during peak sales periods.}
}
@article{BONGIOVANNI200713,
title = {Economics of site-specific nitrogen management for protein content in wheat},
journal = {Computers and Electronics in Agriculture},
volume = {58},
number = {1},
pages = {13-24},
year = {2007},
note = {Precision Agriculture in Latin America},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2007.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0168169907000592},
author = {R.G. Bongiovanni and C.W. Robledo and D.M. Lambert},
keywords = {Wheat, Yield monitor, Protein, Economically optimal nitrogen rates, Spatial econometrics},
abstract = {Wheat fields in the semiarid region of Argentina are spatially variable in soil nitrogen (N) fertility and crop productivity. By accounting for spatial variation in soil N levels, variable-rate fertilizer application may improve crop yield, protein content, and N use efficiency within fields. Therefore, there is interest in applying variable rates of N fertilizer across the landscape. The general objectives of this research are to determine relationships among yield, protein and N rates, using spatial regression analysis of yield monitor data; and to optimize variable-rate application (VRA) of nitrogen fertilizer for wheat. The data were drawn from an on-farm N trial of 10.2ha within a 44ha field conducted in Manfredi, Córdoba, Argentina, in 2003. The experimental design was a complete block strip trial that included two different types of soils in terms of landscape (Hilltop and Lowland) and two different antecessor crops (Corn and Soybeans). The fertilized strips were wider than the combine platform width, with zero N application as the control, and five other rates of elemental N (12, 37, 62, 88 and 112kgha−1). Yield data were obtained with a combine grain monitor, and grain samples for quality analysis were manually collected from the grain flow of a combine harvester, analyzed for quality in the laboratory and converted into a geographical information system (GIS) layer, together with the yield monitor data. Yield and grain quality variability was observed across the field, and among treatments. The combination of yield maps, soil moisture and protein content can help to determine management zones in order to maximize economic benefit. This approach offers opportunities to optimize grain protein on a site-specific basis by accounting for spatial variability of N fertility within individual fields.}
}
@article{MONTEROLORENZO201395,
title = {A spatio-temporal geostatistical approach to predicting pollution levels: The case of mono-nitrogen oxides in Madrid},
journal = {Computers, Environment and Urban Systems},
volume = {37},
pages = {95-106},
year = {2013},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2012.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971512000580},
author = {José-María Montero-Lorenzo and Gema Fernández-Avilés and José Mondéjar-Jiménez and Manuel Vargas-Vargas},
keywords = {Spatio-temporal kriging, Functional kriging, Composite weighted likelihood, Pollution, NO},
abstract = {In spite of the effort made in the last years, NOx is still one of the main pollution problems in large cities. This is why the literature related to predicting NOx levels is certainly extensive. However, most of this literature does not take into account the spatio-temporal dependencies of such NOx levels. As spatio-temporal dependencies are a core aspect of pollution, we propose both a spatio-temporal kriging and a functional kriging strategy to incorporate such dependencies into the prediction procedure. We also use an innovative method for estimating the parameters of the non separable space–time covariance function involved in the spatio-temporal kriging strategy, which significantly reduces the computational burden of traditional likelihood-based methods. The empirical study focuses on Madrid City and is backed by a massive hourly database. Results indicate that the functional strategy outperforms the spatio-temporal procedure at non peripheral sites, which is a remarkable finding due to the high computational requirements of spatio-temporal kriging.}
}
@article{SZOKE2017107,
title = {Empirical study on refactoring large-scale industrial systems and its effects on maintainability},
journal = {Journal of Systems and Software},
volume = {129},
pages = {107-126},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.08.071},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216301558},
author = {Gábor Szőke and Gábor Antal and Csaba Nagy and Rudolf Ferenc and Tibor Gyimóthy},
keywords = {Refactoring, Software quality, Maintainability, Coding issues, Antipatterns, ISO/IEC 25010},
abstract = {Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits which fixed 1273 coding issues. We found that single refactorings only make a very little difference (sometimes even decrease maintainability), but a whole refactoring period, in general, can significantly increase maintainability, which can result not only in the local, but also in the global improvement of the code.}
}
@article{MYOVELLA2021102224,
title = {Determinants of digitalization and digital divide in Sub-Saharan African economies: A spatial Durbin analysis},
journal = {Telecommunications Policy},
volume = {45},
number = {10},
pages = {102224},
year = {2021},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2021.102224},
url = {https://www.sciencedirect.com/science/article/pii/S0308596121001282},
author = {Godwin Myovella and Mehmet Karacuka and Justus Haucap},
keywords = {Digital divide, Spatial analysis, Sub-Saharan Africa, Spatial Durbin model},
abstract = {The aim of this paper is to analyse the determinants for digital divide in Sub-Saharan Africa (SSA) by considering inequalities in internet use and broadband subscriptions. The study considers 41 countries in the region which are geographically linked, and it allows for spatial interdependence. It also accounts for differences in demographic characteristics as well as social, political and economic infrastructure which affect ICT access and use. Globally, about half of the world's population is connected to the internet whereas the remaining half, i.e. about 3.8 billion people, are not (yet) connected. The problem is more severe in developing countries compared to developed countries; it is said to be unsatisfactory in SSA compared to other countries. We apply a spatial panel analysis using the spatial Durbin model (SDM) specifications for 451 observations from 2006 to 2016. Our estimation results show that there is a strong spatial interdependence among SSA, implying that internet access and broadband subscriptions in one country are affected by internet access and broadband subscriptions in another country, most likely due to spillover effects. Our results reveal that GDP per capita, gross capital formation, political stability, regulatory efficacy and electricity infrastructure directly affect the digital divide. Moreover, GDP per capita, population growth, government consumption, trade openness, and electricity infrastructure also indirectly affect the digital divide through spillover effects. Regarding policy implications we suggest that SSA governments should work closer together in ensuring internet openness and to increase the level of coordination between their countries in order to ensure digital inclusion in their countries.}
}
@article{SHORROCK2002319,
title = {Development and application of a human error identification tool for air traffic control},
journal = {Applied Ergonomics},
volume = {33},
number = {4},
pages = {319-336},
year = {2002},
issn = {0003-6870},
doi = {https://doi.org/10.1016/S0003-6870(02)00010-8},
url = {https://www.sciencedirect.com/science/article/pii/S0003687002000108},
author = {Steven T. Shorrock and Barry Kirwan},
keywords = {Human error, Air traffic control, Human error identification, Incident analysis},
abstract = {This paper outlines a human error identification (HEI) technique called TRACEr—technique for the retrospective and predictive analysis of cognitive errors in air traffic control (ATC). The paper firstly considers the need for an HEI tool in ATC, and key requirements for the technique are noted. The technique, which comprises a number of inter-related taxonomies, based around a simple cognitive framework, is then described. A study concerning a real-world application of TRACEr is outlined—the evaluation of several options for reduced separation minima in unregulated UK airspace. In this study, TRACEr was used predictively and retrospectively, looking forward to pre-empt potential problems and looking back to learn from experience. The paper concludes that TRACEr is a valuable aid to design, development and operations in UK ATC, and has indeed been used as a basis for further applications in ATC both in Europe and the USA.}
}
@article{GREEN2008422,
title = {Autocalibration in hydrologic modeling: Using SWAT2005 in small-scale watersheds},
journal = {Environmental Modelling & Software},
volume = {23},
number = {4},
pages = {422-434},
year = {2008},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815207001168},
author = {C.H. Green and A. {van Griensven}},
keywords = {SWAT, Hydrologic modeling, Autocalibration, Watershed, Nutrients, Sediment},
abstract = {SWAT is a physically based model that can simulate water quality and quantity at the watershed scale. Due to many of the processes involved in the manual- or autocalibration of model parameters and the knowledge of realistic input values, calibration can become difficult. An autocalibration-sensitivity analysis procedure was embedded in SWAT version 2005 (SWAT2005) to optimize parameter processing. This embedded procedure is applied to six small-scale watersheds (subwatersheds) in the central Texas Blackland Prairie. The objective of this study is to evaluate the effectiveness of the autocalibration-sensitivity analysis procedures at small-scale watersheds (4.0–8.4ha). Model simulations are completed using two data scenarios: (1) 1 year used for parameter calibration; (2) 5 years used for parameter calibration. The impact of manual parameter calibration versus autocalibration with manual adjustment on model simulation results is tested. The combination of autocalibration tool parameter values and manually adjusted parameters for the 2000–2004 simulation period resulted in the highest ENS and R2 values for discharge; however, the same 5-year period yielded better overall ENS, R2 and P-values for the simulation values that were manually adjusted. The disparity is most likely due to the limited number of parameters that are included in this version of the autocalibration tool (i.e. Nperco, Pperco, and nitrate). Overall, SWAT2005 simulated the hydrology and the water quality constituents at the subwatershed-scale more adequately when all of the available observed data were used for model simulation as evidenced by statistical measure when both the autocalibration and manually adjusted parameters were used in the simulation.}
}
@article{WEST2005532,
title = {Ensemble strategies for a medical diagnostic decision support system: A breast cancer diagnosis application},
journal = {European Journal of Operational Research},
volume = {162},
number = {2},
pages = {532-551},
year = {2005},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2003.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221703007410},
author = {David West and Paul Mangiameli and Rohit Rampal and Vivian West},
keywords = {Decision support systems, Medical informatics, Neural networks, Bootstrap aggregate models, Ensemble strategies},
abstract = {The model selection strategy is an important determinant of the performance and acceptance of a medical diagnostic decision support system based on supervised learning algorithms. This research investigates the potential of various selection strategies from a population of 24 classification models to form ensembles in order to increase the accuracy of decision support systems for the early detection and diagnosis of breast cancer. Our results suggest that ensembles formed from a diverse collection of models are generally more accurate than either pure-bagging ensembles (formed from a single model) or the selection of a “single best model.” We find that effective ensembles are formed from a small and selective subset of the population of available models with potential candidates identified by a multicriteria process that considers the properties of model generalization error, model instability, and the independence of model decisions relative to other ensemble members.}
}
@article{ALVAREZ2022104513,
title = {Incremental learning for property price estimation using location-based services and open data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {107},
pages = {104513},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104513},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621003614},
author = {Francisco Alvarez and Edgar Roman-Rangel and Luis V. Montiel},
keywords = {Incremental learning, Tree-based machine learning algorithms, Prop-Tech, Property pricing, Geographic data},
abstract = {This paper proposes a tree-based incremental-learning model to estimate house pricing using publicly available information on geography, city characteristics, transportation, and real estate for sale. Previous machine-learning models capture the marginal effects of property characteristics and location on prices using big datasets for training. In contrast, our scenario is constrained to small batches of data that become available in a daily basis, therefore our model learns from daily city data, employing incremental-learning to provide accurate price estimations each day. Our results show that property prices are highly influenced by the city characteristics and its connectivity, and that incremental models efficiently adapt to the nature of the house pricing estimation task.}
}
@article{LO2011644,
title = {Generating reliable meteorological data in mountainous areas with scarce presence of weather records: The performance of MTCLIM in interior British Columbia, Canada},
journal = {Environmental Modelling & Software},
volume = {26},
number = {5},
pages = {644-657},
year = {2011},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2010.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815210003099},
author = {Yueh-Hsin Lo and Juan A. Blanco and Brad Seely and Clive Welham and James P. {(Hamish) Kimmins}},
keywords = {Climate model, MTCLIM, Model validation, Climate downscaling, Weather data extrapolation, Forest climate},
abstract = {Climate models have an important role in biometeorological research in mountainous areas where few, dispersed and relatively short data records are the norm. Weather-extrapolator models are a possible solution and we tested the performance of the mountain microclimate simulation model (MTCLIM, a meteorological point data extrapolator) in three arid sites in southern interior British Columbia, representing a gradient of available meteorological information. Measures of several goodness-of-fit indices (Pearson’s correlation coefficient, coefficient of determination, mean error, mean absolute error, modeling efficiency and Theil’s inequality coefficient) and equivalence tests showed that MTCLIM simulated temperature better than precipitation and performed inside the accuracy requirements for our dendrochronological studies for both variables even with short data series. Histograms showed that predicted daily TMAX and TMIN in this arid area had some seasonal biases, probably influenced by the presence of a large water body nearby. In long-term ecological and dendrochronological studies, temporal changes of climate variables at monthly or yearly scales are usually more important than their absolute values at daily scale, and in the present study the histograms of observed and data simulated by MTCLIM at those scales were similar. Therefore, we conclude that MTCLIM can extrapolate reliable weather data for use in ecological studies in arid mountainous terrain, provided there is an adequate weather record at the reference station and proper information to calculate the input parameters: lapse rates and precipitation isohyets.}
}
@article{EFSTRATIADIS2014139,
title = {A multivariate stochastic model for the generation of synthetic time series at multiple time scales reproducing long-term persistence},
journal = {Environmental Modelling & Software},
volume = {62},
pages = {139-152},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002412},
author = {Andreas Efstratiadis and Yannis G. Dialynas and Stefanos Kozanis and Demetris Koutsoyiannis},
keywords = {Stochastic simulation, Hydrometeorological processes, Disaggregation, Long-term persistence, Intermittency, Hydrosystems},
abstract = {A time series generator is presented, employing a robust three-level multivariate scheme for stochastic simulation of correlated processes. It preserves the essential statistical characteristics of historical data at three time scales (annual, monthly, daily), using a disaggregation approach. It also reproduces key properties of hydrometeorological and geophysical processes, namely the long-term persistence (Hurst–Kolmogorov behaviour), the periodicity and intermittency. Its efficiency is illustrated through two case studies in Greece. The first aims to generate monthly runoff and rainfall data at three reservoirs of the hydrosystem of Athens. The second involves the generation of daily rainfall for flood simulation at five rain gauges. In the first emphasis is given to long-term persistence – a dominant characteristic in the management of large-scale hydrosystems, comprising reservoirs with carry-over storage capacity. In the second we highlight to the consistent representation of intermittency and asymmetry of daily rainfall, and the distribution of annual daily maxima.}
}
@article{BERTOLINO2018114,
title = {A categorization scheme for software engineering conference papers and its application},
journal = {Journal of Systems and Software},
volume = {137},
pages = {114-129},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302844},
author = {Antonia Bertolino and Antonello Calabrò and Francesca Lonetti and Eda Marchetti and Breno Miranda},
keywords = {Conference, Paper categorization, Paper type, Research contribution, Research problem, Validation},
abstract = {Background
In Software Engineering (SE), conference publications have high importance both in effective communication and in academic careers. Researchers actively discuss how a paper should be organized to be accepted in mainstream conferences.
Aiming
This work tackles the problem of generalizing and characterizing the type of papers accepted at SE conferences.
Method
The paper offers a new perspective in the analysis of SE literature: a categorization scheme for SE papers is obtained by merging, extending and revising related proposals from a few existing studies. The categorization scheme is used to classify the papers accepted at three top-tier SE conferences during five years (2012–2016).
Results
While a broader experience is certainly needed for validation and fine-tuning, preliminary outcomes can be observed relative to what problems and topics are addressed, what types of contributions are presented and how they are validated.
Conclusions
The results provide insights to paper writers, paper reviewers and conference organizers in focusing their future efforts, without any intent to provide judgments or authoritative guidelines.}
}
@article{TROUTT2005520,
title = {A distribution-free approach to estimating best response values with application to mutual fund performance modeling},
journal = {European Journal of Operational Research},
volume = {166},
number = {2},
pages = {520-527},
year = {2005},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S037722170400092X},
author = {Marvin D. Troutt and Michael Y. Hu and Murali S. Shanker},
keywords = {Finance, Financial DSS, Securities, Statistics, Parameter estimation, Frontier estimation},
abstract = {Frontier regression models seek to model and estimate best rather than average values of a response variable. Our proposed frontier model has similar intent, but also allows for an additional error term. The composed error approach uses the sum of two error terms, one an inefficiency error and the other as white noise. Previous research proposed assumptions on the distributions of the error components so that the distribution of this total error can be specified. Here we propose a distribution free approach to specifying these errors. In addition, our approach is completely data driven, rendering model specification an unnecessary step. We also outline, step-by-step, an approach to implementing this procedure. Our entire approach is illustrated with a mutual fund data set from the Morning Star database.}
}
@article{MESMAN2020104852,
title = {Performance of one-dimensional hydrodynamic lake models during short-term extreme weather events},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104852},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104852},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309099},
author = {J.P. Mesman and A.I. Ayala and R. Adrian and E. {De Eyto} and M.A. Frassl and S. Goyette and J. Kasparian and M. Perroud and J.A.A. Stelzer and D.C. Pierson and B.W. Ibelings},
keywords = {Storm, Heatwave, Model validation, Simstrat, GOTM, General lake model},
abstract = {Numerical lake models are useful tools to study hydrodynamics in lakes, and are increasingly applied to extreme weather events. However, little is known about the accuracy of such models during these short-term events. We used high-frequency data from three lakes to test the performance of three one-dimensional (1D) hydrodynamic models (Simstrat, GOTM, GLM) during storms and heatwaves. Models reproduced the overall direction and magnitude of changes during the extreme events, with accurate timing and little bias. Changes in volume-averaged and surface temperatures and Schmidt stability were simulated more accurately than changes in bottom temperature, maximum buoyancy frequency, or mixed layer depth. However, in most cases the model error was higher (30–100%) during extreme events compared to reference periods. As a consequence, while 1D lake models can be used to study effects of extreme weather events, the increased uncertainty in the simulations should be taken into account when interpreting results.}
}
@article{GRIBBEN2014504,
title = {Location error estimation in wireless ad hoc networks},
journal = {Ad Hoc Networks},
volume = {13},
pages = {504-515},
year = {2014},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2013.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1570870513002291},
author = {Jeremy Gribben and Azzedine Boukerche},
keywords = {Localization, Location estimation, CRLB, Wireless Sensor network},
abstract = {Many ad hoc network applications rely on nodes having accurate knowledge of their geographic locations. However, inherent in all localization systems is a degree of error in computed positions, which can compromise the accuracy and efficiency of location dependent applications and protocols. We propose a scheme in which nodes estimate the amount of error present in their derived positions with a certain probability. Localization error variance is modeled with a function based on the calculated theoretical lower bound on estimator variance, given by the Cramér–Rao Lower Bound (CRLB). Probabilistic methods then use this variance model to estimate upper bounds on localization error, which are computed locally by wireless devices. Best fits between the model and the actual location error variance using both time of arrival (TOA) and received signal strength (RSS) distance measurements were determined by a least squares estimator over repeated localization simulations. The proposed method was used to accurately estimate location error at given probabilities in a multitude of randomly generated network topologies within ±10% of the actual localization error. Once known, estimates can be integrated into location dependent schemes to improve on their robustness to localization error.}
}
@article{PETER2020104517,
title = {Improved lithology prediction in channelized reservoirs by integrating stratigraphic forward modelling: Towards improved model calibration in a case study of the Holocene Rhine-Meuse fluvio-deltaic system},
journal = {Computers & Geosciences},
volume = {141},
pages = {104517},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104517},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419310489},
author = {Costanzo Peter and Eloisa {Salina Borello} and Rory A.F. Dalman and Pantelis Karamitopoulos and Freek Busschers and Quinto Sacchi and Francesca Verga},
keywords = {Stratigraphic forward modeling, Basin modeling, Fluvio-deltaic, Inverse algorithm, Model calibration},
abstract = {Stratigraphic forward modelling (SFM) provides the means to produce geologically coherent and realistic models. In this paper, we demonstrate the possibility of matching lithological variability simulated with a basin-scale advection-diffusion SFM to a data-rich real-world setting, i.e. the Holocene Rhine-Meuse fluvio-deltaic system in the Netherlands. SFM model calibration to real-world data in general has proven non-trivial. This study focuses on a novel inversion process constrained by the top surface and the sand proportion observed at specific pseudo-wells in the study area. Goodness-of-fit expressed by a new fitness function gives the error calculated as the average of two calibration constraints. Computational efficiency was increased significantly by implementing a new optimization process in two hierarchical steps: a) optimization in terms of sediment load and discharge, which are the most influential parameters having the largest uncertainty and b) optimization with respect to the remaining uncertain parameters, these being sediment transport parameters. The calibration process described allows for the most optimal combination of achieving acceptable levels of goodness-of-fit, feasible runtimes and multiple (non-unique) solutions to obtain synthetic stratigraphic output best matching real-world datasets. By removing model realizations which are geologically unrealistic, calibrated SFM models provide a multiscale stratigraphic framework for reconstructing static models of reservoirs which are consistent with the palaeogeographic layout, basin-fill history and external drivers (e.g. sea level, sediment supply). The static reservoir models that are matched with highest certainty therefore contain the highest geological realism and may be used to improve deep subsurface reservoir or aquifer property prediction. The new methodology was applied to the well-established Holocene Rhine-Meuse dataset, which allows a rigorous testing of the optimization; the calibrated SFM allows investigation of controls of the Holocene development on the sedimentary system.}
}
@article{JASPERSEN2020221,
title = {On the learning patterns and adaptive behavior of terrorist organizations},
journal = {European Journal of Operational Research},
volume = {282},
number = {1},
pages = {221-234},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719307490},
author = {Johannes G. Jaspersen and Gilberto Montibeller},
keywords = {Decision processes, Behavioral OR, OR in defense, Adversarial risk analysis, Decision analysis},
abstract = {The threat to national security posed by terrorists makes the design of evidence-based counter-terrorism strategies paramount. As terrorist organizations are purposeful entities, it is crucial to understand their decision processes if we want to plan defenses and counter-measures. In particular, there is evidence that terrorist organizations are both adaptive in their behavior and driven by multiple objectives in their actions. In this paper, we use insights from learning theory and compare several different reinforcement learning models regarding their ability to predict terrorist organizations’ actions. Using data on target choices of terrorist attacks and two different objectives (renown and revenge), we show that a total reinforcement learning with power (Luce) choice probabilities and information discounting can be used to model the adaptive behavior of terrorist organizations. The model renders out-of-sample predictions which are comparable in their validity to those observed for learning in laboratory studies. We draw implications for counter-terrorism strategies by comparing the predictive validity of the different models and their calibrated parameters. Our results also offer a starting point for studying the convergence process in game theoretic analyses of conflicts involving terrorists.}
}
@article{ECKERT2021693,
title = {Forecasting Swiss exports using Bayesian forecast reconciliation},
journal = {European Journal of Operational Research},
volume = {291},
number = {2},
pages = {693-710},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.09.046},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030850X},
author = {Florian Eckert and Rob J. Hyndman and Anastasios Panagiotelis},
keywords = {Forecasting, Hierarchical reconciliation, Optimal combination, Decision-making},
abstract = {This paper proposes a novel forecast reconciliation framework using Bayesian state-space methods. It allows for the joint reconciliation at all forecast horizons and uses predictive distributions rather than past variation of forecast errors. Informative priors are used to assign weights to specific predictions, which makes it possible to reconcile forecasts such that they accommodate specific judgmental predictions or managerial decisions. The reconciled forecasts adhere to hierarchical constraints, which facilitates communication and supports aligned decision-making at all levels of complex hierarchical structures. An extensive forecasting study is conducted on a large collection of 13,118 time series that measure Swiss merchandise exports, grouped hierarchically by export destination and product category. We find strong evidence that in addition to producing coherent forecasts, reconciliation also leads to substantial improvements in forecast accuracy. The use of state-space methods is particularly promising for optimal decision-making under conditions with increased model uncertainty and data volatility.}
}
@article{NUNEZVARELA2017164,
title = {Source code metrics: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {128},
pages = {164-197},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.044},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300663},
author = {Alberto S. Nuñez-Varela and Héctor G. Pérez-Gonzalez and Francisco E. Martínez-Perez and Carlos Soubervielle-Montalvo},
keywords = {Source code metrics, Software metrics, Object-oriented metrics, Aspect-oriented metrics, Feature-oriented metrics, Systematic mapping study},
abstract = {Context
Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics.
Objectives
This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends.
Method
A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed.
Results
Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines.
Conclusions
Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.}
}
@article{IBRAHIM20122293,
title = {On the relationship between comment update practices and Software Bugs},
journal = {Journal of Systems and Software},
volume = {85},
number = {10},
pages = {2293-2304},
year = {2012},
note = {Automated Software Evolution},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S016412121100238X},
author = {Walid M. Ibrahim and Nicolas Bettenburg and Bram Adams and Ahmed E. Hassan},
keywords = {Code quality, Software bugs, Software evolution, Source code comments, Empirical studies},
abstract = {When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners.}
}
@article{SILVA2019196,
title = {Co-change patterns: A large scale empirical study},
journal = {Journal of Systems and Software},
volume = {152},
pages = {196-214},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300597},
author = {Luciana L. Silva and Marco Tulio Valente and Marcelo A. Maia},
keywords = {Modularity, Co-change clusters, Co-change patterns},
abstract = {Co-Change Clustering is a modularity assessment technique that reveals how often changes are localized in modules and whether a change propagation represents design problems. This technique is centered on co-change clusters, which are highly inter-related source code files considering co-change relations. In this paper, we conduct a series of empirical analysis in a large corpus of 133 popular software projects on GitHub. We describe six co-change patterns by projecting them over the directory structure. We mine 1802 co-change clusters and 1719 co-change clusters (95%) are covered by the six co-change patterns. In this study, we aim to answer two central questions: (i) Are co-change patterns detected in different programming languages? (ii) How do different co-change patterns relate to rippling, activity density, ownership, and team diversity on clusters? We conclude that Encapsulated and Well-Confined clusters (Wrapped) implement well-defined and confined concerns. Octopus clusters are proportionally numerous regarding to other patterns. They relate significantly with ripple effect, activity, ownership, and diversity in development teams. Although Crosscutting are scattered over directories, they implement well-defined concerns. Despite they present higher activity compared to Wrapped clusters, it is not necessarily easy to get rid of them, suggesting that support tools may play a crucial role.}
}
@article{WENG2018685,
title = {Macroeconomic indicators alone can predict the monthly closing price of major U.S. indices: Insights from artificial intelligence, time-series analysis and hybrid models},
journal = {Applied Soft Computing},
volume = {71},
pages = {685-697},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618304125},
author = {Bin Weng and Waldyn Martinez and Yao-Te Tsai and Chen Li and Lin Lu and James R. Barth and Fadel M. Megahed},
keywords = {ARIMA, Deep learning, Ensembles, GARCH, Long short-term memory (LSTM) networks},
abstract = {This paper proposes a two-stage approach that can be used to investigate whether the information hidden in macroeconomic variables (alone) can be used to accurately predict the one-month ahead price for major U.S stock and sector indices. Stage 1 is constructed to evaluate the hypothesis that the price for different indices is driven by different economic indicators. It consists of three phases. In phase I, the data is automatically acquired using freely available APIs (application programming interfaces) and prepared for analysis. Phase II reduces the set of potential predictors without the loss of information through several variable selection methods. The third phase employs four ensemble models and three time-series models for prediction. The prediction performance of the seven models are compared using the Mean Absolute Percent Error (and two additional metrics). If the hypothesis were to be true, one expects that the performance of the ensemble models to outperform the time-series models since the information in the economy is more important than the information in previous prices. In Stage 2, a hybrid approach of the recurring neural network used for time-series prediction (i.e., the LSTM) and the ensemble models is constructed to examine the secondary hypothesis that the residuals from the time-series models are not random and can be explained by the macroeconomic indicators. To test the two hypotheses, the monthly closing prices for 13 U.S. stock and sector indices and the corresponding values for 23 macroeconomic indicators were collected from 01/1992–10/2016. Based on the case study, the four ensembles prediction performance were superior to that of the three time-series models. The MAPE of the best model for a given index was < 1.87%. The Stage 2 results also show that the three evaluation metrics (RMSE, MAPE and MAE) can be typically improved by 25–50% by incorporating the information hidden in the macroeconomic indicators (through the ensemble approach). Thus, this paper shows that, for the analysis period and the indices studied, the macro-economic indicators are leading predictors of the price of 13 U.S. sector indices.}
}
@article{VERSTEGEN201230,
title = {Spatio-temporal uncertainty in Spatial Decision Support Systems: A case study of changing land availability for bioenergy crops in Mozambique},
journal = {Computers, Environment and Urban Systems},
volume = {36},
number = {1},
pages = {30-42},
year = {2012},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2011.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971511000883},
author = {Judith Anne Verstegen and Derek Karssenberg and Floor {van der Hilst} and André Faaij},
keywords = {Spatial Decision Support Systems, Uncertainty, Spatial modeling, Visualization, Land use change, Bioenergy},
abstract = {Spatial Decision Support Systems (SDSSs) often include models that can be used to assess the impact of possible decisions. These models usually simulate complex spatio-temporal phenomena, with input variables and parameters that are often hard to measure. The resulting model uncertainty is, however, rarely communicated to the user, so that current SDSSs yield clear, but therefore sometimes deceptively precise outputs. Inclusion of uncertainty in SDSSs requires modeling methods to calculate uncertainty and tools to visualize indicators of uncertainty that can be understood by its users, having mostly limited knowledge of spatial statistics. This research makes an important step towards a solution of this issue. It illustrates the construction of the PCRaster Land Use Change model (PLUC) that integrates simulation, uncertainty analysis and visualization. It uses the PCRaster Python framework, which comprises both a spatio-temporal modeling framework and a Monte Carlo analysis framework that together produce stochastic maps, which can be visualized with the Aguila software, included in the PCRaster Python distribution package. This is illustrated by a case study for Mozambique in which it is evaluated where bioenergy crops can be cultivated without endangering nature areas and food production now and in the near future, when population and food intake per capita will increase and thus arable land and pasture areas are likely to expand. It is shown how the uncertainty of the input variables and model parameters effects the model outcomes. Evaluation of spatio-temporal uncertainty patterns has provided new insights in the modeled land use system about, e.g., the shape of concentric rings around cities. In addition, the visualization modes give uncertainty information in an comprehensible way for users without specialist knowledge of statistics, for example by means of confidence intervals for potential bioenergy crop yields. The coupling of spatio-temporal uncertainty analysis to the simulation model is considered a major step forward in the exposure of uncertainty in SDSSs.}
}
@article{PARPAS2014359,
title = {A stochastic multiscale model for electricity generation capacity expansion},
journal = {European Journal of Operational Research},
volume = {232},
number = {2},
pages = {359-374},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0377221713006036},
author = {Panos Parpas and Mort Webster},
keywords = {(I) Control, (I) OR in energy, (I) Markov processes},
abstract = {Long-term planning for electric power systems, or capacity expansion, has traditionally been modeled using simplified models or heuristics to approximate the short-term dynamics. However, current trends such as increasing penetration of intermittent renewable generation and increased demand response requires a coupling of both the long and short term dynamics. We present an efficient method for coupling multiple temporal scales using the framework of singular perturbation theory for the control of Markov processes in continuous time. We show that the uncertainties that exist in many energy planning problems, in particular load demand uncertainty and uncertainties in generation availability, can be captured with a multiscale model. We then use a dimensionality reduction technique, which is valid if the scale separation present in the model is large enough, to derive a computationally tractable model. We show that both wind data and electricity demand data do exhibit sufficient scale separation. A numerical example using real data and a finite difference approximation of the Hamilton–Jacobi–Bellman equation is used to illustrate the proposed method. We compare the results of our approximate model with those of the exact model. We also show that the proposed approximation outperforms a commonly used heuristic used in capacity expansion models.}
}
@article{CLARK2011429,
title = {The selection of appropriate spectrally bright pseudo-invariant ground targets for use in empirical line calibration of SPOT satellite imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {66},
number = {4},
pages = {429-445},
year = {2011},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2011.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0924271611000268},
author = {Barnaby Clark and Juha Suomalainen and Petri Pellikka},
keywords = {Calibration, SPOT, Radiometric, Multispectral, Close range},
abstract = {The appropriate utilization of multi-temporal SPOT multispectral satellite imagery in quantitative remote sensing studies requires the removal of atmospheric effects. One widely used and potentially very accurate way of achieving absolute atmospheric correction is the calibration of at-satellite radiance data to field measures of the surface reflectance factor (ρs). There are a number of variations in this technique, which are known collectively as empirical line (EL) approaches. However, the successful application of an EL spectral calibration requires the presence and careful selection of appropriate pseudo-invariant ground targets within each scene area. Real surfaces, even those that are man-made and vegetation-free, display non-Lambertian reflectance behaviour to some extent. Because of the ±31° off-nadir incidence angle range of the SPOT sensors, this is a crucial consideration. In favourable circumstances, it may be possible to utilize a goniometer to collect multiangular ρs measurements, but for widespread lower cost application of EL approaches currently, the use of a handheld spectrometer to measure nadir only ρs is a more realistic proposition. In either case, the selection of targets that have more limited and stable multiangular reflectance behaviour is preferable. Details are given of the reflectance properties of a variety of spectrally bright potential calibration surface types, encompassing sands, gravel, asphalts, and managed and artificial grass turf surfaces, measured in the field using the Finnish Geodetic Institute Field Goniospectrometer (FIGIFIGO). Bright calibration site selection requirements for SPOT data are discussed and the physical mechanisms behind the varying reflectance characteristics of the surfaces are considered. The most desirable properties for useful calibration targets are identified. The results of this study will assist other workers in the identification of likely suitable EL calibration sites for medium and high resolution optical satellite data, and therefore help optimize efforts in the time consuming and costly process of measuring ρs in the field.}
}
@article{SNYDER2002684,
title = {Forecasting sales of slow and fast moving inventories},
journal = {European Journal of Operational Research},
volume = {140},
number = {3},
pages = {684-699},
year = {2002},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(01)00231-4},
url = {https://www.sciencedirect.com/science/article/pii/S0377221701002314},
author = {Ralph Snyder},
keywords = {Demand forecasting, Inventory control, Simulation, Parametric bootstrapping, Time series analysis},
abstract = {Traditional computerised inventory control systems usually rely on exponential smoothing to forecast the demand for fast moving inventories. Practices in relation to slow moving inventories are more varied, but the Croston method is often used. It is an adaptation of exponential smoothing that (1) incorporates a Bernoulli process to capture the sporadic nature of demand and (2) allows the average variability to change over time. The Croston approach is critically appraised in this paper. Corrections are made to underlying theory and modifications are proposed to overcome certain implementation difficulties. A parametric bootstrap approach is outlined that integrates demand forecasting with inventory control. The approach is illustrated on real demand data for car parts.}
}
@article{MALHOTRA200781,
title = {Workflow modeling in critical care: Piecing together your own puzzle},
journal = {Journal of Biomedical Informatics},
volume = {40},
number = {2},
pages = {81-92},
year = {2007},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2006.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046406000633},
author = {Sameer Malhotra and Desmond Jordan and Edward Shortliffe and Vimla L. Patel},
keywords = {Medical errors, Critical care workflow, Error prediction, Decision making models},
abstract = {The intensive care unit (ICU) is an instance of a very dynamic health care setting where critically ill patients are being managed. To provide good care, an extensive and coordinated communication amongst the role players, use of numerous information systems and operation of devices for monitoring and treatment purposes are required. The purpose of this research is to study error evolution and management within this environment. The focus is on representing the workflow of critical care environment, which emphasizes the importance such a representation may play in strategizing the management of medical errors. We used ethnographic observation and interview data to build individual pieces of the workflow, dependent on the individual and the activity concerned. Key personnel were intensively followed during their respective patient care activities and the related actions. All interactions were recorded for analysis. These clinicians and nurses were interviewed to complement the observation data and to delineate their individual workflows. These pieces of the ICU workflow were used to develop a generalize-able cognitive model to represent the intricate workflow applicable to other health care settings. The proposed model can be used to identify and characterize medical errors and for error prediction in practice.}
}
@article{MACK2014105,
title = {An econometric approach for evaluating the linkages between broadband and knowledge intensive firms},
journal = {Telecommunications Policy},
volume = {38},
number = {1},
pages = {105-118},
year = {2014},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2013.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0308596113000992},
author = {Elizabeth A. Mack and Sergio J. Rey},
keywords = {Firms, Broadband, Knowledge intensive, Regional},
abstract = {In addition to highlighting improvements in the availability and speed of broadband as a national priority, the National Broadband Plan also includes several recommendations for improving access and use of broadband by small businesses. The plan also recommends economic development officials include broadband in their local development strategies. While these are certainly import goals, more research is needed to evaluate how broadband impacts the regional business environment, and regional capacity to retain and attract businesses in particular industries. In order to further our understanding about the linkages between broadband and businesses, and the ability of places to retain and attract businesses in particular industries, this study will develop and estimate econometric models to better understand the linkages between broadband and firms in the knowledge intensive sector. Specifically, 54 metropolitan area specific models will be developed to examine regional variations in the linkages between broadband and firms in the knowledge intensive sector in 2004. Model results highlight the importance of broadband to knowledge firms in all but five metropolitan areas across the U.S. They also reveal variations in the impact of broadband on knowledge firm presence related to metropolitan area size and industrial legacy.}
}
@article{CHA201490,
title = {A Bayesian network incorporating observation error to predict phosphorus and chlorophyll a in Saginaw Bay},
journal = {Environmental Modelling & Software},
volume = {57},
pages = {90-100},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000619},
author = {YoonKyung Cha and Craig A. Stow},
keywords = {Phosphorus targets, Water quality criteria, Dreissenid invasion, Bayesian hierarchical modeling, Observation error, Bayesian network, Saginaw Bay},
abstract = {Empirical relationships between lake chlorophyll a and total phosphorus concentrations are widely used to develop predictive models. These models are often estimated using sample averages as implicit surrogates for unknown lake-wide means, a practice than can result in biased parameter estimation and inaccurate predictive uncertainty. We develop a Bayesian network model based on empirical chlorophyll-phosphorus relationships for Saginaw Bay, an embayment on Lake Huron. The model treats the means as unknown parameters, and includes structure to accommodate the observation error associated with estimating those means. Compared with results from an analogous simple model using sample averages, the observation error model has a lower predictive uncertainty and predicts lower chlorophyll and phosphorus concentrations under contemporary lake conditions. These models will be useful to guide pending decision-making pursuant to the 2012 Great Lakes Water Quality Agreement.}
}
@article{DRAGICEVIC2017109,
title = {Bayesian network model for task effort estimation in agile software development},
journal = {Journal of Systems and Software},
volume = {127},
pages = {109-119},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300171},
author = {Srdjana Dragicevic and Stipe Celar and Mili Turic},
keywords = {Bayesian network, Effort prediction, Agile software development},
abstract = {Even though the use of agile methods in software development is increasing, the problem of effort estimation remains quite a challenge, mostly due to the lack of many standard metrics to be used for effort prediction in plan-driven software development. The Bayesian network model presented in this paper is suitable for effort prediction in any agile method. Simple and small, with inputs that can be easily gathered, the suggested model has no practical impact on agility. This model can be used as early as possible, during the planning stage. The structure of the proposed model is defined by the authors, while the parameter estimation is automatically learned from a dataset. The data are elicited from completed agile projects of a single software company. This paper describes various statistics used to assess the precision of the model: mean magnitude of relative error, prediction at level m, accuracy (the percentage of successfully predicted instances over the total number of instances), mean absolute error, root mean squared error, relative absolute error and root relative squared error. The obtained results indicate very good prediction accuracy.}
}
@article{JUTLA2013148,
title = {A framework for predicting endemic cholera using satellite derived environmental determinants},
journal = {Environmental Modelling & Software},
volume = {47},
pages = {148-158},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213001291},
author = {Antarpreet S. Jutla and Ali S. Akanda and Shafiqul Islam},
keywords = {Cholera prediction, Hydrology, Remote sensing, SeaWiFS, Chlorophyll, CDOM, River discharge, Bangladesh, },
abstract = {Cholera remains one of the most prevalent water-related infections in many tropical regions of the world. Macro-environmental processes provide a natural ecological niche for Vibrio cholerae and because powerful evidence of new biotypes is emerging, it is unlikely that the bacteria will be fully eradicated. Consequently, to develop effective intervention and mitigation strategies, it is necessary to develop cholera prediction models with several months' lead time. Almost all cholera outbreaks originate near the coastal areas and cholera bacteria exhibit a strong relationship with coastal plankton. Using chlorophyll as a surrogate for plankton bloom in coastal areas, recent studies have postulated a relationship between chlorophyll and cholera incidence. Here, we show that seasonal cholera outbreaks in the Bengal Delta can be predicted two to three months in advance with an overall prediction accuracy of over 75% by using satellite-derived chlorophyll and air temperature data. Such high prediction accuracy is achievable because the two seasonal peaks of cholera are predicted using two separate models representing distinctive macro-scale environmental processes. We have shown that interannual variability of pre-monsoon cholera outbreaks can be satisfactorily explained with coastal plankton blooms and a cascade of hydro-coastal processes. Post-monsoon cholera outbreaks, on the other hand, are related to macro-scale monsoon processes and subsequent breakdown of sanitary conditions. Our results demonstrate that satellite data over a range of space and time scales are effective in developing a cholera prediction model for the Bengal Delta with several months' lead time. We anticipate our modeling framework and findings will provide the impetus to explore the utility of satellite derived macro-scale variables for cholera prediction in other cholera endemic regions.}
}
@article{ATKINSON20211165,
title = {Generalized estimation of productivity with multiple bad outputs: The importance of materials balance constraints},
journal = {European Journal of Operational Research},
volume = {292},
number = {3},
pages = {1165-1186},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720309711},
author = {Scott E. Atkinson and Mike G. Tsionas},
keywords = {Productivity and competitiveness, Directional technology distance function, Productivity change with goods and bads, Materials-balance equations},
abstract = {Previous research has frequently estimated the directional technology distance function (DTDF) to more flexibly model multiple-input and multiple-output production, firm inefficiency, and productivity growth. For example, with firms such as electric utilities, one must model the production of good and bad outputs using good and bad inputs. Typically, all inputs and outputs are potentially endogenous. In previous work, we show how to identify a DTDF system using price equations based on profit maximization and compute optimal directions for measuring productivity change. However, this work has not imposed restrictions that limit substitution possibilities among inputs and outputs to a feasible set that is consistent with materials-balance constraints. Such constraints require that the weight of all inputs equals the weight of all outputs. The major innovation of this paper is that we include two types of functional relationships that impose the parametric analog of materials balance by modeling the generation of bad outputs and the use of bad inputs. The first requires that bad outputs are functionally related to good inputs and bad inputs. The second requires that bad inputs are functionally related to good inputs. We illustrate these methods using a balanced panel of 80 U.S. coal-fired electric generating plants from 1995–2005. Substantial differences are observed between the specification that includes the materials-balance constraints and the conventional approach that omits them, based on Bayes factors as well as measures of productivity and inefficiency. For many plants, improved management practices can reduce substantial inefficiencies in meeting emission constraints without reducing productivity growth.}
}
@article{NAGATA2017474,
title = {Adaptive Spelling Error Correction Models for Learner English},
journal = {Procedia Computer Science},
volume = {112},
pages = {474-483},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314096},
author = {Ryo Nagata and Hiroya Takamura and Graham Neubig},
keywords = {spelling errors, word embeddings, learners of English},
abstract = {Spelling errors are a characteristic of learner English and degrade the performances of natural language processing systems targeting English learners. This paper describes a method specially designed for automatically correcting spelling errors in learner English that reduces the effects from noise (e.g., grammatical and spelling errors) by adaptively creating spelling error correction models from raw learner corpora. An evaluation shows that the proposed method outperforms previous edit-distance-based and language-model-based methods. We also report results of an investigation into what types of spelling errors English learners tend to make, using the spelling error models created by the proposed method as a tool for our analysis.}
}
@article{CHIOU20093490,
title = {Effects of Financial Holding Company Act on bank efficiency and productivity in Taiwan},
journal = {Neurocomputing},
volume = {72},
number = {16},
pages = {3490-3506},
year = {2009},
note = {Financial Engineering Computational and Ambient Intelligence (IWANN 2007)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2009.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925231209001854},
author = {Chei-Chang Chiou},
keywords = {Efficiency, Productivity change, Financial holding companies},
abstract = {Taiwan's banking industry has experienced greatly structural changes since the implementing of Financial Holding Company Act in July 2001. The paper investigates whether Taiwan's commercial banks establishing or joining in financial holding companies (FHCs) could promote their efficiency and productivity, as well as the determinants of efficiency and productivity changes of commercial banks. The paper applies a data envelopment analysis approach for calculating bank efficiency and a Malmquist total factor productivity index for measuring productivity change of banks. The results show that except for pure technical efficiency, other efficiencies and productivity of commercial banks do not improved because of their establishment of or joining in FHCs; on the contrary, it is just because of their better efficiency that they can firstly establish or join in FHCs. The results also reveal that, in the aspect of determinants of bank efficiency, bank size and overdue ratio have significant negative relations to technical efficiency of establishing or joining in FHCs’ banks, while equity-to-total asset ratio and loan-to-deposit ratio have significant positive relations to their technical efficiency; moreover, bank size and overdue ratio have significant negative relations to technical efficiency of banks with no established FHCs, while business diversification and loan-to-deposit ratio have significant negative relations to scale efficiency for these banks. Finally, in the aspect of determinants of bank productivity changes, overdue ratio has a significant negative relation to productivity growth of establishing or joining in FHCs’ banks, while business diversification has significant negative relations to both technological growth and productivity growth for banks with established and no established FHCs; furthermore, equity-to-total asset ratio and loan-to-deposit ratio have slightly significant negative relations to technical efficiency growth of banks with no established FHCs, while business diversification has a slightly significant positive contribution to their technical efficiency growth.}
}
@article{DUMONT2014121,
title = {Parameter identification of the STICS crop model, using an accelerated formal MCMC approach},
journal = {Environmental Modelling & Software},
volume = {52},
pages = {121-135},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S136481521300265X},
author = {B. Dumont and V. Leemans and M. Mansouri and B. Bodson and J.-P. Destain and M.-F. Destain},
keywords = {Crop model, Parameter estimation, Bayes, STICS, DREAM},
abstract = {This study presents a Bayesian approach for the parameters' identification of the STICS crop model based on the recently developed Differential Evolution Adaptive Metropolis (DREAM) algorithm. The posterior distributions of nine specific crop parameters of the STICS model were sampled with the aim to improve the growth simulations of a winter wheat (Triticum aestivum L.) culture. The results obtained with the DREAM algorithm were initially compared to those obtained with a Nelder-Mead Simplex algorithm embedded within the OptimiSTICS package. Then, three types of likelihood functions implemented within the DREAM algorithm were compared, namely the standard least square, the weighted least square, and a transformed likelihood function that makes explicit use of the coefficient of variation (CV). The results showed that the proposed CV likelihood function allowed taking into account both noise on measurements and heteroscedasticity which are regularly encountered in crop modelling.}
}
@article{CRUZ2020104818,
title = {Evaluating the 10% wind speed rule of thumb for estimating a wildfire's forward rate of spread against an extensive independent set of observations},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104818},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104818},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220306964},
author = {Miguel G. Cruz and Martin E. Alexander and Paulo M. Fernandes and Musa Kilinc and Ângelo Sil},
keywords = {Crown fire, Fine dead fuel moisture content, Fire behaviour, Fire prediction, Fire propagation, Fire weather, Fuel type, Model error},
abstract = {The prediction of wildfire rate of spread and growth under high wind speeds and dry fuel moisture conditions is key to taking proactive actions to warn and in turn protect communities. We used two datasets of wildfires spreading under critical fire weather conditions to evaluate an existing rule of thumb that equates the forward rate of fire spread to 10% of the average open wind speed. The rule predicted the observed rates of fire spread with an overall mean absolute error of 1.7 km h−1. The absolute error magnitude was consistent across the range in observed rates of fire spread, resulting in a reduction in percent error with an increase in spread rates. Mean absolute percent errors close to 20% were obtained for wildfires spreading faster than 2.0 km h−1. The implications of model errors in the forecasting of fire spread with respect to community warning and safety are discussed.}
}
@article{WOUNGANG20121682,
title = {Coding-error based defects in enterprise resource planning software: Prevention, discovery, elimination and mitigation},
journal = {Journal of Systems and Software},
volume = {85},
number = {7},
pages = {1682-1698},
year = {2012},
note = {Software Ecosystems},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212000544},
author = {Isaac Woungang and Felix O. Akinladejo and David W. White and Mohammad S. Obaidat},
keywords = {Defect density, Coding defects, ERP, Software development, Defect reduction, Code auditing, Static code analysis, Software testing},
abstract = {Software defects due to coding errors continue to plague the industry with disastrous impact, especially in the enterprise application software category. Identifying how much of these defects are specifically due to coding errors is a challenging problem. In this paper, we investigate the best methods for preventing new coding defects in enterprise resource planning (ERP) software, and discovering and fixing existing coding defects. A large-scale survey-based ex-post-facto study coupled with experiments involving static code analysis tools on both sample code and real-life million lines of code open-source ERP software were conducted for such purpose. The survey-based methodology consisted of respondents who had experience developing ERP software. This research sought to determine if software defects could be merely mitigated or totally eliminated, and what supporting policies, procedures and infrastructure were needed to remedy the problem. In this paper, we introduce a hypothetical framework developed to address our research questions, the hypotheses we have conjectured, the research methodology we have used, and the data analysis methods used to validate the stated hypotheses. Our study revealed that: (a) the best way for ERP developers to discover coding-error based defects in existing programs is to choose an appropriate programming language; perform a combination of manual and automated code auditing, static code analysis, and formal test case design, execution and analysis, (b) the most effective ways to mitigate defects in an ERP system is to track the defect densities in the ERP software, fix the defects found, perform regression testing, and update the resulting defect density statistics, and (c) the impact of epistemological and legal commitments on the defect densities of ERP systems is inconclusive. We feel that our proposed model has the potential to vastly improve the quality of ERP and other similar software by reducing the coding-error defects, and recommend that future research aimed at testing the model in actual production environments.}
}
@article{BRUCE2018274,
title = {A multi-lake comparative analysis of the General Lake Model (GLM): Stress-testing across a global observatory network},
journal = {Environmental Modelling & Software},
volume = {102},
pages = {274-291},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216311562},
author = {Louise C. Bruce and Marieke A. Frassl and George B. Arhonditsis and Gideon Gal and David P. Hamilton and Paul C. Hanson and Amy L. Hetherington and John M. Melack and Jordan S. Read and Karsten Rinke and Anna Rigosi and Dennis Trolle and Luke Winslow and Rita Adrian and Ana I. Ayala and Serghei A. Bocaniov and Bertram Boehrer and Casper Boon and Justin D. Brookes and Thomas Bueche and Brendan D. Busch and Diego Copetti and Alicia Cortés and Elvira {de Eyto} and J. Alex Elliott and Nicole Gallina and Yael Gilboa and Nicolas Guyennon and Lei Huang and Onur Kerimoglu and John D. Lenters and Sally MacIntyre and Vardit Makler-Pick and Chris G. McBride and Santiago Moreira and Deniz Özkundakci and Marco Pilotti and Francisco J. Rueda and James A. Rusak and Nihar R. Samal and Martin Schmid and Tom Shatwell and Craig Snorthheim and Frédéric Soulignac and Giulia Valerio and Leon {van der Linden} and Mark Vetter and Brigitte Vinçon-Leite and Junbo Wang and Michael Weber and Chaturangi Wickramaratne and R. Iestyn Woolway and Huaxia Yao and Matthew R. Hipsey},
keywords = {Lake model, Stratification, GLM, Model assessment, Global observatory data, Network science},
abstract = {The modelling community has identified challenges for the integration and assessment of lake models due to the diversity of modelling approaches and lakes. In this study, we develop and assess a one-dimensional lake model and apply it to 32 lakes from a global observatory network. The data set included lakes over broad ranges in latitude, climatic zones, size, residence time, mixing regime and trophic level. Model performance was evaluated using several error assessment metrics, and a sensitivity analysis was conducted for nine parameters that governed the surface heat exchange and mixing efficiency. There was low correlation between input data uncertainty and model performance and predictions of temperature were less sensitive to model parameters than prediction of thermocline depth and Schmidt stability. The study provides guidance to where the general model approach and associated assumptions work, and cases where adjustments to model parameterisations and/or structure are required.}
}
@article{AMIEL2004711,
title = {Individual differences in Internet usage motives},
journal = {Computers in Human Behavior},
volume = {20},
number = {6},
pages = {711-726},
year = {2004},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0747563204001220},
author = {Tel Amiel and Stephanie Lee Sargent},
keywords = {Personality types, Eysenck, Internet usage motives, Individual differences},
abstract = {The relationship between the psychobiological model of personality types (psychoticism, extraversion, and neuroticism) devised by Eysenck and Eysenck [Personality and individual differences: A natural science approach, Plenum Press, New York, 1985] and Internet use and usage motives was examined. A sample of 210 undergraduate students were asked to report on their motives for using the Internet and how often they engaged in a variety of Internet and web-based activities. The findings demonstrate distinctive patterns of Internet use and usage motives for those of different personality types. Specifically, those scoring high in neuroticism reported using the Internet to feel a sense of “belonging” and to be informed. Extraverts rejected the communal aspects of the Internet, and made more instrumental and goal-oriented use of Internet services. Finally, those scoring high in psychoticism demonstrated an interest in more deviant, defiant, and sophisticated Internet applications. Implications of the findings as well as suggestions for future research are included.}
}
@article{CAMACHO2018218,
title = {A framework for uncertainty and risk analysis in Total Maximum Daily Load applications},
journal = {Environmental Modelling & Software},
volume = {101},
pages = {218-235},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217307612},
author = {Rene A. Camacho and James L. Martin and Tim Wool and Vijay P. Singh},
keywords = {Total Maximum Daily Load, Margin of safety, Uncertainty analysis, Risk assessment, Bayesian analysis},
abstract = {In the United States, the computation of Total Maximum Daily Loads (TMDL) must include a Margin of Safety (MOS) to account for different sources of uncertainty. In practice however, TMDL studies rarely include an explicit uncertainty analysis and the estimation of the MOS is often subjective and even arbitrary. Such approaches are difficult to replicate and preclude the comparison of results between studies. To overcome these limitations, a Bayesian framework to compute TMDLs and MOSs including an explicit evaluation of uncertainty and risk is proposed in this investigation. The proposed framework uses the concept of Predictive Uncertainty to calculate a TMDL from an equation of allowable risk of non-compliance of a target water quality standard. The framework is illustrated in a synthetic example and in a real TMDL study for nutrients in Sawgrass Lake, Florida.}
}
@article{CORCORAN2007623,
title = {The use of spatial analytical techniques to explore patterns of fire incidence: A South Wales case study},
journal = {Computers, Environment and Urban Systems},
volume = {31},
number = {6},
pages = {623-647},
year = {2007},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2007.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0198971507000130},
author = {Jonathan Corcoran and Gary Higgs and Chris Brunsdon and Andrew Ware and Paul Norman},
keywords = {GIS, Fire incidence, Spatial statistics, Regression, Visualisation},
abstract = {The application of mapping and spatial analytical techniques to explore geographical patterns of crime incidence is well established. In contrast, the analysis of operational incident data routinely collected by fire brigades has received relatively less research attention, certainly in the UK academic literature. The aim of this paper is to redress this balance through the application of spatial analytical techniques that permit an exploration of the spatial dynamics of fire incidents and their relationships with socio-economic variables. By examining patterns for different fire incident types, including household fires, vehicle fires, secondary fires and malicious false alarms in relation to 2001 Census of Population data for an area of South Wales, we demonstrate the potential of such techniques to reveal spatial patterns that may be worthy of further contextual study. Further research is needed to establish how transferable these findings are to other geographical settings and how replicable the findings are at different geographical scales. The paper concludes by drawing attention to the current gaps in knowledge in analysing trends in fire incidence and proposes an agenda to advance such research using spatial analytical techniques.}
}
@article{MAMMADLI2017602,
title = {Financial time series prediction using artificial neural network based on Levenberg-Marquardt algorithm},
journal = {Procedia Computer Science},
volume = {120},
pages = {602-607},
year = {2017},
note = {9th International Conference on Theory and Application of Soft Computing, Computing with Words and Perception, ICSCCW 2017, 22-23 August 2017, Budapest, Hungary},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917324973},
author = {Sadig Mammadli},
keywords = {Financial time series, neural network, prediction, optimization, Levenberg-Marquardt},
abstract = {This paper discusses the applications of Artificial Neural Networks (ANN) using Levenberg-Marquardt optimization algorithm for prediction of financial time series. ANN based on Levenberg-Marquardt training algorithm outperforms gradient decent, conjugate gradient and other algorithms that use the first order derivative of performance index to optimize ANN weights. Levenberg-Marquardt algorithm uses a second order derivative of performance index (curvature information on error surface) as a Guassi-Newton algorithm, but it approximate Hessian matrix by the Jacobian (gradient). Experimental results shows efficiency using ANN based on Levenberg-Marquardt algorithm}
}
@article{JARVIS2001753,
title = {GEO_BUG: a geographical modelling environment for assessing the likelihood of pest development},
journal = {Environmental Modelling & Software},
volume = {16},
number = {8},
pages = {753-765},
year = {2001},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(01)00040-8},
url = {https://www.sciencedirect.com/science/article/pii/S1364815201000408},
author = {Claire H Jarvis},
keywords = {Interpolation, Daily temperatures, Geographical information systems (GIS), Pest phenology},
abstract = {This paper describes software designed to explore pest phenology (development) over space and time. The framework presented links sequences of interpolated daily maximum and minimum temperatures with a variety of process-based phenology and accumulated temperature models. The flexibility offered by this approach is demonstrated using examples of gridded maps of pest phenology on target dates, graphs of the sequences of pest development at individual locations and assessments of error in the predicted dates over the course of a model run. Finally, the potential application of the software in support of agricultural management systems, policy development and integrated research is discussed.}
}
@article{KAUPP2021492,
title = {CONTEXT: An Industry 4.0 Dataset of Contextual Faults in a Smart Factory},
journal = {Procedia Computer Science},
volume = {180},
pages = {492-501},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.265},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003148},
author = {Lukas Kaupp and Heiko Webert and Kawa Nazemi and Bernhard Humm and Stephan Simons},
keywords = {contextual faults, smart factory, cyber-physical systems, fault diagnosis, anomaly detection},
abstract = {Cyber-physical systems in smart factories get more and more integrated and interconnected. Industry 4.0 accelerates this trend even further. Through the broad interconnectivity a new class of faults arise, the contextual faults, where contextual knowledge is needed to find the underlying reason. Fully-automated systems and the production line in a smart factory form a complex environment making the fault diagnosis non-trivial. Along with a dataset, we give a first definition of contextual faults in the smart factory and name initial use cases. Additionally, the dataset encompasses all the data recorded in a current state-of-the-art smart factory. We also add additional information measured by our developed sensing units to enrich the smart factory data even further. In the end, we show a first approach to detect the contextual faults in a manual preliminary analysis of the recorded log data.}
}
@article{MCCLEAN2014190,
title = {Using phase-type models to cost stroke patient care across health, social and community services},
journal = {European Journal of Operational Research},
volume = {236},
number = {1},
pages = {190-199},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.01.063},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714001027},
author = {Sally McClean and Jennifer Gillespie and Lalit Garg and Maria Barton and Bryan Scotney and Ken Kullerton},
keywords = {Applied probability, Cost benefit analysis, Decision analysis, Markov processes, OR in health services, Coxian phase-type models},
abstract = {Stroke disease places a heavy burden on society, incurring long periods of time in hospital and community care, and associated costs. Also stroke is a highly complex disease with diverse outcomes and multiple strategies for therapy and care. Previously a modeling framework has been developed which clusters patients into classes with respect to their length of stay (LOS) in hospital. Phase-type models were then used to describe patient flows for each cluster. Also multiple outcomes, such as discharge to normal residence, nursing home, or death can be permitted. We here add costs to this model and obtain the Moment Generating Function for the total cost of a system consisting of multiple transient phase-type classes with multiple absorbing states. This system represents different classes of patients in different hospital and community services states. Based on stroke patients’ data from the Belfast City Hospital, various scenarios are explored with a focus on comparing the cost of thrombolysis treatment under different regimes. The overall modeling framework characterizes the behavior of stroke patient populations, with a focus on integrated system-wide costing and planning, encompassing hospital and community services. Within this general framework we have developed models which take account of patient heterogeneity and multiple care options. Such complex strategies depend crucially on developing a deep engagement with the health care professionals and underpinning the models with detailed patient-specific data.}
}
@article{RODRIGUEZ2013133,
title = {Generating time-series of dry weather loads to sewers},
journal = {Environmental Modelling & Software},
volume = {43},
pages = {133-143},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213000479},
author = {Juan Pablo Rodríguez and Neil McIntyre and Mario Díaz-Granados and Stefan Achleitner and Martin Hochedlinger and Čedo Maksimović},
keywords = {Dry weather flow patterns, Integrated urban drainage modelling, Sewer systems, Stochastic time series generation, Uncertainty analysis},
abstract = {Availability of appropriate methods for quantifying temporal and spatial variations of inflows to sewer systems is a prerequisite to effective sewer system modelling. To contribute to this goal, an empirical generator of sub-catchment wastewater outputs, for use as flow and water quality inputs to dynamic simulations of the larger sewerage system, is developed and evaluated. The deterministic part of the model is represented by means of Fourier series to generate diurnal profiles and a linear regression to generalise between sites, while a novel application of a multivariate error model with a lag-one autoregression term provides a stochastic component. Using a case study of Bogotá (Colombia), the validities of model assumptions are analysed and model results are compared with available dry weather measurements. The transferability of the methodology to other drainage systems is partially assessed using Linz (Austria) as a case study. It is concluded that the stochastic generator is a useful tool for generating flow and water quality at gauged and ungauged sub-catchment outlets in Bogotá and potentially other catchments.}
}
@article{KARSSENBERG2010489,
title = {A software framework for construction of process-based stochastic spatio-temporal models and data assimilation},
journal = {Environmental Modelling & Software},
volume = {25},
number = {4},
pages = {489-502},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209002643},
author = {Derek Karssenberg and Oliver Schmitz and Peter Salamon and Kor {de Jong} and Marc F.P. Bierkens},
keywords = {Data assimilation, Particle filter, Ensemble kalman filter, Hydrology, PCRaster, Python, Snow, Environmental model, Calibration, Spatio-temporal model},
abstract = {Process-based spatio-temporal models simulate changes over time using equations that represent real world processes. They are widely applied in geography and earth science. Software implementation of the model itself and integrating model results with observations through data assimilation are two important steps in the model development cycle. Unlike most software frameworks that provide tools for either implementation of the model or data assimilation, this paper describes a software framework that integrates both steps. The software framework includes generic operations on 2D map and 3D block data that can be combined in a Python script using a framework for time iterations and Monte Carlo simulation. In addition, the framework contains components for data assimilation with the Ensemble Kalman Filter and the Particle filter. Two case studies of distributed hydrological models show how the framework integrates model construction and data assimilation.}
}
@article{KROBEL2010583,
title = {Modelling water dynamics with DNDC and DAISY in a soil of the North China Plain: A comparative study},
journal = {Environmental Modelling & Software},
volume = {25},
number = {4},
pages = {583-601},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209002400},
author = {Roland Kröbel and Qinping Sun and Joachim Ingwersen and Xinping Chen and Fusuo Zhang and Torsten Müller and Volker Römheld},
keywords = {DNDC, Daisy, North China Plain, China, Soil water content, Modelling, Model evaluation, Model comparison},
abstract = {The performance of the DNDC and Daisy model to simulate the water dynamics in a floodplain soil of the North China Plain was tested and compared. While the DNDC model uses a simple cascade approach, the Daisy model applies the physically based Richard's equation for simulating water movement in soil. For model testing a three years record of the soil water content from the Dong Bei Wang experimental station near Beijing was used. There, the effect of nitrogen fertilization, irrigation and straw removal on soil water and nitrogen dynamics was investigated in a three factorial field experiment applying a split-split-plot design with 4 replications. The dataset of one treatment was used for model testing and calibration. Two other independent datasets from further treatments were employed for validating the models. For both models, the simulation results were not satisfying using default parameters. After parameter optimisation and the use of site-specific van Genuchten parameters, however, the Daisy model performed well. But, for the DNDC model, parameter optimisation failed to improve the simulation result. Owing to the fact that many biological processes such as plant growth, nitrification or denitrification depend strongly on the soil water content, our findings bring us to the conclusion that the site-specific suitability of the DNDC model for simulating the soil water dynamics should be tested before further simulation of other processes.}
}
@article{GUO2017167,
title = {Implications of accelerated self-healing as a key design knob for cross-layer resilience},
journal = {Integration},
volume = {56},
pages = {167-180},
year = {2017},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167926016300840},
author = {Xinfei Guo and Mircea R. Stan},
keywords = {Wearout, BTI, Accelerated self-healing, Frequency dependence, Cross-layer},
abstract = {In this paper we propose a cross-layer accelerated self-healing (CLASH) system which “repairs” its wearout issues in a physical sense through accelerated and active recovery, by which wearout can be reversed while actively applying several accelerated self-healing techniques, such as high temperature and negative voltages. Different from previous solutions of coping with wearout issues (e.g. BTI) by “tolerating”, “slowing down” or “compensating”, which still leave the irreversible (permanent) wearout component unchecked, the proposed solution is able to fully avoid the irreversible wearout through periodic rejuvenation, and this is inspired by the explored frequency dependent behaviors of wearout and (accelerated and active) recovery based on measurements on FPGAs. We demonstrate a case where the chip can always be brought back to the fresh status by employing a pattern of 31-h regular operation (under room temperature and nominal voltage) followed by a 1-h accelerated self-healing (under high temperature and negative voltage). The proposed system integrates the notions of accelerated self-healing across multiple layers of the system stack. At the circuit level, a negative voltage generator and heating elements are designed and implemented; at the architecture level, the core can be allocated in a way such that the dark silicon or redundant resources can be healed by active elements; at the system level, right balance of stress and accelerated/active recovery can be employed by the system scheduler to fully mitigate the wearout; various wearout sensors act as the media between different layers. Overall, these techniques work together to guarantee that the whole system performs for more of the time at higher levels of performance and power efficiency by fully taking advantage of the extra opportunities enabled by the accelerated self-healing.}
}
@article{HA2011230,
title = {Analysis of traffic hazard intensity: A spatial epidemiology case study of urban pedestrians},
journal = {Computers, Environment and Urban Systems},
volume = {35},
number = {3},
pages = {230-240},
year = {2011},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971511000020},
author = {Hoe-Hun Ha and Jean-Claude Thill},
keywords = {Pedestrian collision, Urban sustainability, Pedestrian mobility, Spatial econometrics},
abstract = {Traffic safety studies have underscored the hazardous conditions of pedestrians in the United States. This situation calls for increased public awareness of the pedestrian safety issue and better knowledge of the main factors contributing to traffic hazard for urban pedestrians. The purpose of this spatial epidemiology research is to gain greater insights into the geographic dimension exhibited by the intensity of traffic collisions involving urban pedestrians. Pedestrian crashes are studied in Buffalo, NY for years 2003 and 2004. Factors of hazard intensity are determined and compared for three age cohorts as well as for collisions occurring at intersections versus mid-block locations. Physical road characteristics and density of development, as well as socio-economic and demographic variables and potential trip attractors are examined. Spatial regression models are used to account for spatial dependencies. Econometric analysis underscores that all classes of environmental factors tested are significant drivers of pedestrian traffic hazard intensity. Results of the geographic analysis indicate that young and adult pedestrian traffic hazard intensities follow rather distinct logics. In addition, intersection and mid-block crashes differ by their socio-economic correlates, as well as their spatial distribution in the urban fabric.}
}
@article{WEISS2014106,
title = {An effective approach for gap-filling continental scale remotely sensed time-series},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {98},
pages = {106-118},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271614002512},
author = {Daniel J. Weiss and Peter M. Atkinson and Samir Bhatt and Bonnie Mappin and Simon I. Hay and Peter W. Gething},
keywords = {Gap-filling, MODIS, EVI, LST, Africa},
abstract = {The archives of imagery and modeled data products derived from remote sensing programs with high temporal resolution provide powerful resources for characterizing inter- and intra-annual environmental dynamics. The impressive depth of available time-series from such missions (e.g., MODIS and AVHRR) affords new opportunities for improving data usability by leveraging spatial and temporal information inherent to longitudinal geospatial datasets. In this research we develop an approach for filling gaps in imagery time-series that result primarily from cloud cover, which is particularly problematic in forested equatorial regions. Our approach consists of two, complementary gap-filling algorithms and a variety of run-time options that allow users to balance competing demands of model accuracy and processing time. We applied the gap-filling methodology to MODIS Enhanced Vegetation Index (EVI) and daytime and nighttime Land Surface Temperature (LST) datasets for the African continent for 2000–2012, with a 1km spatial resolution, and an 8-day temporal resolution. We validated the method by introducing and filling artificial gaps, and then comparing the original data with model predictions. Our approach achieved R2 values above 0.87 even for pixels within 500km wide introduced gaps. Furthermore, the structure of our approach allows estimation of the error associated with each gap-filled pixel based on the distance to the non-gap pixels used to model its fill value, thus providing a mechanism for including uncertainty associated with the gap-filling process in downstream applications of the resulting datasets.}
}
@article{AHMAD2019147,
title = {A generic data-driven technique for forecasting of reservoir inflow: Application for hydropower maximization},
journal = {Environmental Modelling & Software},
volume = {119},
pages = {147-165},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219302373},
author = {Shahryar Khalique Ahmad and Faisal Hossain},
keywords = {Reservoir inflow forecasting, Artificial neural network, Numerical weather prediction, Baseflow separation, Hydropower maximization, Global scalability},
abstract = {A generic and scalable scheme is proposed for forecasting reservoir inflow to optimize reservoir operations for hydropower maximization. Short-term weather forecasts and antecedent hydrological variables were inputs to a three-layered hydrologically-relevant Artificial Neural Network (ANN) to forecast inflow for 7-days of lead-time. Application of the scheme was demonstrated over 23 dams in U.S. with varying hydrological characteristics and climate regimes. Probabilistic forecast was also explored by feeding ANN with ensembles of weather forecast fields. Results suggest forecasting skill improves with decreasing coefficient of variation in inflow and increasing drainage area. Forecast-informed operations were simulated using a rolling horizon scheme and assessed against benchmark control rules. Over two years of operations from Pensacola dam (Oklahoma), additional 47,253 MWh of energy could have been harvested without compromising flood risk with optimal operations. This study reinforces the potential of a numerically efficient and skillful reservoir inflow forecasting scheme to address water-energy security challenges.}
}
@article{BOSNIC2014178,
title = {Enhancing data stream predictions with reliability estimators and explanation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {34},
pages = {178-192},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614001237},
author = {Zoran Bosnić and Jaka Demšar and Grega Kešpret and Pedro {Pereira Rodrigues} and João Gama and Igor Kononenko},
keywords = {Data stream, Incremental learning, Prediction accuracy, Prediction correction, Prediction explanation},
abstract = {Incremental learning from data streams is increasingly attracting research focus due to many real streaming problems (such as learning from transactions, sensors or other sequential observations) that require processing and forecasting in the real time. In this paper we deal with two issues related to incremental learning – prediction accuracy and prediction explanation – and demonstrate their applicability on several streaming problems for predicting electricity load in the future. For improving prediction accuracy we propose and evaluate the use of two reliability estimators that allow us to estimate prediction error and correct predictions. For improving interpretability of the incremental model and its predictions we propose an adaptation of the existing prediction explanation methodology, which was originally developed for batch learning from stationary data. The explanation methodology is combined with a state-of-the-art concept drift detector and a visualization technique to enhance the explanation in dynamic streaming settings. The results show that the proposed approaches can improve prediction accuracy and allow transparent insight into the modeled concept.}
}
@article{WOLFS201560,
title = {Modular conceptual modelling approach and software for river hydraulic simulations},
journal = {Environmental Modelling & Software},
volume = {71},
pages = {60-77},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215001577},
author = {Vincent Wolfs and Pieter Meert and Patrick Willems},
keywords = {River, Floods, Computational efficiency, Emulation modelling, Modelling software, Calculation scheme},
abstract = {Numerous applications in river management require computationally efficient models that can accurately simulate the state of a river. This paper presents a reduced complexity modelling approach that emulates the results of detailed full hydrodynamic models. Its modular design based on virtual reservoirs allows users to combine different model structures depending on the river dynamics and intended use. A semi-automatic software tool (Conceptual Model Developer, CMD) was developed to facilitate model set-up. To prevent instabilities during simulations, a highly efficient discrete calculation scheme is presented with a variable time step. To illustrate the effectiveness of the presented approach, the Marke River in Belgium was conceptualized based on simulation results of a detailed model. Results show that the derived conceptual model mimics the detailed model closely, while the calculation time is reduced by more than 2000 times. Finally, several applications are discussed that employ conceptual models built according to the presented approach.}
}
@article{CATOLINO2019165,
title = {Not all bugs are the same: Understanding, characterizing, and classifying bug types},
journal = {Journal of Systems and Software},
volume = {152},
pages = {165-181},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300536},
author = {Gemma Catolino and Fabio Palomba and Andy Zaidman and Filomena Ferrucci},
keywords = {Bug classification, Taxonomy, Empirical study},
abstract = {Modern version control systems, e.g., GitHub, include bug tracking mechanisms that developers can use to highlight the presence of bugs. This is done by means of bug reports, i.e., textual descriptions reporting the problem and the steps that led to a failure. In past and recent years, the research community deeply investigated methods for easing bug triage, that is, the process of assigning the fixing of a reported bug to the most qualified developer. Nevertheless, only a few studies have reported on how to support developers in the process of understanding the type of a reported bug, which is the first and most time-consuming step to perform before assigning a bug-fix operation. In this paper, we target this problem in two ways: first, we analyze 1280 bug reports of 119 popular projects belonging to three ecosystems such as Mozilla, Apache, and Eclipse, with the aim of building a taxonomy of the types of reported bugs; then, we devise and evaluate an automated classification model able to classify reported bugs according to the defined taxonomy. As a result, we found nine main common bug types over the considered systems. Moreover, our model achieves high F-Measure and AUC-ROC (64% and 74% on overall, respectively).}
}
@article{SIMALATSAR2018101,
title = {Robustness analysis of personalised delivery rate computation for IV administered anesthetic},
journal = {Smart Health},
volume = {9-10},
pages = {101-114},
year = {2018},
note = {CHASE 2018 Special Issue},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352648318300382},
author = {Alena Simalatsar and Monia Guidi and Pierre Roduit and Thierry Buclin},
keywords = {Individualized anesthesia, Drug delivery, Closed-loop control, Kalman filer, Robustness analysis},
abstract = {Controlled delivery of intravenous (IV) anesthetics aims at fast and safe achievement and maintenance of a suitable depth of hypnosis (DOH), by ensuring appropriate effect site (i.e. brain) exposure to the drug. Today, such drugs are regularly injected by Target Controlled Infusion (TCI) systems, piloted by an open-loop algorithm based on Pharmacokinetic (PK) models. Yet the inaccuracy of concentration prediction of current TCI can reach up to 100%. The situation could be improved by closing the loop with sensors providing regular real measurements of the anesthetic concentration in body fluids. In this paper we present a closed-loop algorithm based on the classic open-loop algorithm combined with a Kalman filter. The latter estimates plasma drug concentration based on PK model and sensor measurements. The estimates are then used in the open-loop algorithm. To validate our approach measurements are generated by means of modulating the population-based plasma concentration values with the maximum inter- and intra-patient variability of the statistical Eleveld׳s (Eleveld et al., 2014) PK model. This allows us to stress the system to a maximum level prior to testing it on patients. We also perform robustness analysis of this algorithm by accounting for realistic measurement periods and delays.}
}
@article{CHANTRE201295,
title = {Modeling Avena fatua seedling emergence dynamics: An artificial neural network approach},
journal = {Computers and Electronics in Agriculture},
volume = {88},
pages = {95-102},
year = {2012},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2012.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0168169912001901},
author = {Guillermo R. Chantre and Aníbal M. Blanco and Mariela V. Lodovichi and Alberto J. Bandoni and Mario R. Sabbatini and Ricardo L. López and Mario R. Vigna and Ramón Gigón},
keywords = {Wild oat, Hydrothermal-time, Semiarid region, Emergence prediction, Non-linear regression},
abstract = {Avena fatua is an invasive weed of the semiarid region of Argentina. Seedling emergence patterns are very irregular along the season showing a great year-to-year variability mainly due to a highly unpredictable precipitation regime. Non-linear regression techniques are usually unable to accurately predict field emergence under such environmental conditions. Artificial Neural Networks (ANNs) are known for their capacity to describe highly non-linear relationships among variables thus showing a high potential applicability in ecological systems. The objectives of the present work were to develop different ANN models for A. fatua seedling emergence prediction and to compare their predictive capability against non-linear regression techniques. Classical hydrothermal-time indices were used as input variable for the development of univariate models, while thermal-time and hydro-time were used as independent input variables for developing bivariate models. The accumulated proportion of seedling emergence was the output variable in all cases. A total of 528 input/output data pairs corresponding to 11years of data collection were used in this study. Obtained results indicate a higher accuracy and generalization performance of the optimal ANN model in comparison to non-linear regression approaches. It is also demonstrated that the use of thermal-time and hydro-time as independent explanatory variables in ANN models yields better prediction than using combined hydrothermal-time indices in classical NLR models. The best obtained ANN model outperformed in 43.3% the best NLR model in terms of RMSE of the test set. Moreover, the best obtained ANN predicted accumulated emergence within the first 50% of total emergence 48.3% better in average than the best developed NLR model. These outcomes suggest the potential applicability of the proposed modeling approach in weed management decision support systems design.}
}
@article{COLBACH2016184,
title = {Uncertainty analysis and evaluation of a complex, multi-specific weed dynamics model with diverse and incomplete data sets},
journal = {Environmental Modelling & Software},
volume = {86},
pages = {184-203},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216307307},
author = {Nathalie Colbach and Michel Bertrand and Hugues Busset and Floriane Colas and François Dugué and Pascal Farcy and Guillaume Fried and Sylvie Granger and Dominique Meunier and Nicolas M. Munier-Jolain and Camille Noilhan and Florence Strbik and Antoine Gardarin},
keywords = {Weed, Population dynamics, Mechanistic model, FS, Validation, Cropping system, Uncertainty analysis},
abstract = {Weed dynamics models are needed to test prospective cropping systems but are rarely evaluated with independent data (“validated”). Here, we evaluated the FlorSys model which quantifies the effects of cropping systems and pedoclimate on multispecific weed dynamics with a daily time step. We adapted existing validation methodologies and uncertainty analyses to account for multi-specific, multi-annual and diverse outputs, focusing on missing input data, incomplete and imprecise weed time series. Field data ranged from entirely monitored cropping system trials to annual snapshots recorded on farm fields by the French Biovigilance-Flore network. FlorSys satisfactorily predicted weed seed bank, plant densities and crop yields, at daily and multi-annual scales, at well monitored sites. It overestimated plant biomass and underestimated total flora density. Missing processes (photoperiod dependency in flowering, crop:weed competition for nitrogen) and inadequately predicted scenarios (weed dynamics in untilled fields, floras with summer-emerging species) were identified. Guidelines for model use were proposed.}
}
@article{SKIENA2004313,
title = {Shift error detection in standardized exams},
journal = {Journal of Discrete Algorithms},
volume = {2},
number = {2},
pages = {313-331},
year = {2004},
note = {Combinatiorial Pattern Matching},
issn = {1570-8667},
doi = {https://doi.org/10.1016/S1570-8667(03)00083-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570866703000832},
author = {Steven Skiena and Pavel Sumazin},
keywords = {Multiple-choice examinations, String alignment, Standardized testing, Adaptive sequences},
abstract = {Hundreds of millions of multiple choice exams are given every year in the United States. These exams permit form-filling shift errors, where an absent-minded mismarking displaces a long run of correct answers. A shift error can substantially alter the exam's score, and thus invalidate it. In this paper, we develop algorithms to accurately detect and correct shift errors, while guaranteeing few false detections. We propose a shift error model, and probabilistic methods to identify shifted exam regions. We describe the results of our search for shift errors in undergraduate Stony Brook exam sets, and in over 100,000 Scholastic Amplitude Tests. These results suggest that approximately 2% of all tests contain shift errors. Extrapolating these results over all multiple choice exams and forms leads us to conclude that exam takers make millions of undetected shift errors each year. Employing probabilistic shift correcting systems is inherently dangerous. Such systems may be taken advantage of by clever examinees, who seek to increase the probability of correct guessing. We conclude our paper with a short study of optimal guessing strategies when faced with a generous shift error correcting system.}
}
@article{DEMELO2019110425,
title = {Characterization of implied scenarios as families of common behavior},
journal = {Journal of Systems and Software},
volume = {158},
pages = {110425},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110425},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301992},
author = {Caio Batista {de Melo} and André Luiz Fernandes Cançado and Genaína Nunes Rodrigues},
keywords = {Dependability, Implied scenarios, Concurrent systems, Smith-Waterman algorithm, Hierarchical clustering},
abstract = {Concurrent systems face a threat to their reliability in emergent behaviors, which are not included in the specification but can happen during runtime. When concurrent systems are modeled in a scenario-based manner, it is possible to detect emergent behaviors as implied scenarios (ISs) which, analogously, are unexpected scenarios that can happen due to the concurrent nature of the system. Until now, the process of dealing with ISs can demand significant time and effort from the user, as they are detected and dealt with in a one by one basis. In this paper, a new methodology is proposed to deal with various ISs at a time, by finding Common Behaviors (CBs) among them. Additionally, we propose a novel way to group CBs into families utilizing a clustering technique using the Smith-Waterman algorithm as a similarity measure. Thus allowing the removal of multiple ISs with a single fix, decreasing the time and effort required to achieve higher system reliability. A total of 1798 ISs were collected across seven case studies, from which 14 families of CBs were defined. Consequently, only 14 constraints were needed to resolve all collected ISs, applying our approach. These results support the validity and effectiveness of our methodology.}
}
@article{BARBEZ2020110486,
title = {A machine-learning based ensemble method for anti-patterns detection},
journal = {Journal of Systems and Software},
volume = {161},
pages = {110486},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110486},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219302602},
author = {Antoine Barbez and Foutse Khomh and Yann-Gaël Guéhéneuc},
keywords = {Software quality, Anti-patterns, Machine learning, Ensemble methods},
abstract = {Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods.}
}
@article{BLOSCHL2008464,
title = {A spatially distributed flash flood forecasting model},
journal = {Environmental Modelling & Software},
volume = {23},
number = {4},
pages = {464-478},
year = {2008},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2007.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815207001247},
author = {Günter Blöschl and Christian Reszler and Jürgen Komma},
keywords = {Forecasting, Parameter identification, Kalman Filter, Floods, Distributed modelling, Stream routing, Model accuracy, Dominant processes concept},
abstract = {This paper presents a distributed model that is in operational use for forecasting flash floods in northern Austria. The main challenge in developing the model was parameter identification which was addressed by a modelling strategy that involved a model structure defined at the model element scale and multi-source model identification. The model represents runoff generation on a grid basis and lumped routing in the river reaches. Ensemble Kalman Filtering is used to update the model states (grid soil moisture) based on observed runoff. The forecast errors as a function of forecast lead time are evaluated for a number of major events in the 622km2 Kamp catchment and range from 10% to 30% for 4–24h lead times, respectively.}
}
@article{JI2016982,
title = {The economic effects of domestic search engines on the development of the online advertising market},
journal = {Telecommunications Policy},
volume = {40},
number = {10},
pages = {982-995},
year = {2016},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2016.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308596116300465},
author = {Sung Wook Ji and Young-jun Choi and Min Ho Ryu},
keywords = {Domestic search engine, Online advertising market, Two-sided market},
abstract = {A few global search engine platforms, notably Google and Yahoo!, have achieved worldwide dominance in the search engine market. However, some domestic search engine platforms, such as Naver in South Korea and Baidu in China, have come to dominate their domestic markets in competition with global search engine platforms. This study quantified the economic effects of domestic search engines on the development of the online advertising market. Using a country-level dynamic panel of 46 countries from 2009 to 2013, we investigated the change in the size of the online advertising market caused by the existence of a domestic search engine. The results show that the development of a domestic search engine may lead to an increase in the size of the online advertising market: A country with its own domestic search engine platform(s) may have an average of 0.018% more online advertising intensity—which is defined as online advertising spending/GDP—than one without this type of platform. The reasons behind these results and the policy implications are also discussed.}
}
@article{PLEAU2005401,
title = {Global optimal real-time control of the Quebec urban drainage system},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {401-413},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000556},
author = {Martin Pleau and Hubert Colas and Pierre Lavallée and Geneviève Pelletier and Richard Bonin},
keywords = {Combined sewer overflow, Global optimal control, Real-time control, Sewer networks, Urban drainage},
abstract = {A global optimal control (GOC) system was implemented in 1999 on the Quebec Urban Community's (QUC) Westerly sewer network to manage flows and water levels in real-time in order to, among others, reduce the frequency and volumes of combined sewer overflows discharged into the St. Charles River and St. Lawrence River. This paper presents some of the salient results of the first three years of operation. The configuration of the GOC system is discussed and operational observations are made about the reliability of some of the major components. Furthermore, an economical analysis presents how cost-effective the real-time control system is for the QUC.}
}
@article{CHO201434,
title = {Developing an amenity value calculator for urban forest landscapes},
journal = {Computers, Environment and Urban Systems},
volume = {43},
pages = {34-41},
year = {2014},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971513000902},
author = {Seong-Hoon Cho and Taeyoung Kim and Roland K. Roberts and Chad Hellwinckel and Seung Gyu Kim and Brad Wilson},
keywords = {Amenity value calculator, Hedonic price model, Urban forest landscapes},
abstract = {The goal of this research is to develop a framework that can be used by landscape and urban planners to implement an “amenity value calculator” for urban forest landscapes across a metropolitan county. By balancing the pros and cons of using typical hedonic frameworks versus urban forest inventory and management software systems, we (1) construct a data-driven approach to estimate the total amenity value associated with access to, views of, and existence of a particular forest landscape from among all available forest sites in a community and (2) develop a framework for an amenity value calculator for numerous community forest landscapes within a metropolitan county, using the amenity values generated from objective (1), that can be accessed and understood by anyone who is interested in the benefits provided by nearby community forests. Our research suggests that (i) residential household’s amenity value per acre of forest landscape decreases asymptotically towards zero as the driving time from a residential house increases, (ii) an amenity value calculator can be developed to sum the amenity values across all detached single-family houses within a range of driving times from any selected forest landscape, and (iii) a user-friendly, web-based application, that allows users to view the estimated amenity values of forest landscapes that interest them, can be created to better inform the public about the values of forest landscapes of interest to them.}
}
@article{WANG2021102173,
title = {ICT and socio-economic development: Evidence from a spatial panel data analysis in China},
journal = {Telecommunications Policy},
volume = {45},
number = {7},
pages = {102173},
year = {2021},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2021.102173},
url = {https://www.sciencedirect.com/science/article/pii/S030859612100077X},
author = {Di Wang and Tao Zhou and Feng Lan and Mengmeng Wang},
keywords = {ICT, Socio-economic development, Spatial analysis, China},
abstract = {Over recent decades, information and communication technology (ICT) has had a profound impact on the economy and on society more broadly. The purpose of this study was to explore the effects of ICT on socio-economic development in China by considering spatial effects. To understand the overall influence of ICT on socio-economic development, the study used principal component analysis (PCA) to establish composite ICT and socio-economic development indexes based on data collected from 31 provinces in the period 2009–2018. In applying the spatial panel data analysis, this article argues that a spatial Durbin model (SDM) with spatial fixed effects is the most suitable model for the purpose of making estimations. The results of the SDM suggest the socio-economic development of provinces contains strong spatial correlations. More specifically, although ICT plays an essential role in improving socio-economic development, the spatial spillover effects of ICT negatively affect the socio-economic development in adjacent areas, implying that a digital divide exists among China's provinces and that this digital gap can lead to unbalanced socio-economic development. The article concludes by outlining some practical policy recommendations for the advancement of ICT going forward to help alleviate the adverse effects of the digital divide and enhance the benefits of ICT-based socio-economic development.}
}
@article{JIANG2020220,
title = {Achieving high data reliability at low scrubbing cost via failure-aware scrubbing},
journal = {Journal of Parallel and Distributed Computing},
volume = {144},
pages = {220-229},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302860},
author = {Tianming Jiang and Ping Huang and Ke Zhou},
keywords = {Hard disk, Reliability, Latent sector error, Scrubbing, Machine learning},
abstract = {Latent Sector Errors (LSEs) happen at a significant frequency in the field and can impose a huge risk to data reliability. Disk scrubbing is a background process that reads disks periodically to detect LSEs timely, thus shortening the window of vulnerability to data loss. Nowadays, proactive error prediction, using machine learning techniques, has been proposed to improve storage system reliability by increasing the scrubbing rate for disks with higher error rates. Unfortunately, the majority of works incur non-trivial scrubbing costs and overlook the relationship between complete disk failures and LSEs. In this paper, we attempt to maintain or improve data reliability at reduced scrubbing costs. In particular, we design a novel adaptive approach that enforces a lower scrubbing rate for healthy disks and a higher scrubbing rate for disks which are subject to LSEs. Besides LSEs that are specific to partial disk failures, we also adjust scrubbing rates according to complete disk failure rates, because disks typically develop LSEs before they finally fail. Moreover, a voting-based method that exploits the periodic characteristic of scrubbing is proposed to ensure prediction accuracy. Experimental results on a real-world field dataset have demonstrated the effectiveness of our proposed approach. Specifically, the results show that we can achieve the same level of reliability, in terms of Mean-Time-To-Detection (MTTD), as the traditional fixed-rate scrubbing scheme with almost 49% less scrubbing costs or we can improve the reliability by a factor of 2.4X without extra scrubbing costs. Compared with the state-of-the-art approaches, our method can achieve the same level of reliability with nearly 32% less scrubbing costs.}
}
@article{VANDENBERG2021100293,
title = {Interaction effects of race and gender in elementary CS attitudes: A validation and cross-sectional study},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100293},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100293},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000283},
author = {Jessica Vandenberg and Arif Rachmatullah and Collin Lynch and Kristy E. Boyer and Eric Wiebe},
keywords = {Computer Science Attitudes, Invariance test, Scale validation, Primary school},
abstract = {Computer science (CS) initiatives for elementary students, including brief Hour of Code activities and longer in- and after-school programs that emphasize robotics and coding, have continued to increase in popularity. Many of these initiatives are intended to increase CS exposure to students who historically have been underrepresented in CS academic trajectories, including women and students of color. This study aimed at examining the gender and race difference in elementary students’ attitudes toward CS. To that end we developed and validated a survey instrument called Elementary Computer Science Attitudes (E-CSA) which consisted of the constructs of CS self-efficacy and outcome expectancy, through a combination of classical test theory and item response theory. The target audience for this instrument and study was upper elementary students (grades 4 and 5, ages 8 to 11). The E-CSA was found to be a gender and race bias-free instrument. A two-way ANOVA test was then used to answer research questions. We found no significant interaction effect between gender and race in the two constructs of CS Attitudes. We also did not see a significant difference based on race. However, a significant difference was found in both CS attitudes constructs based on gender, whereby male students had higher CS attitudes than female students. We discuss our findings from the perspective of the equity issue in CS education. Furthermore, we believe the E-CSA instrument can inform classroom-based interventions, the development of curricular materials, and reinforce findings from cross-sectional CS studies.}
}
@article{SANNA2015286,
title = {A new method for analysing the interrelationship between performance indicators with an application to agrometeorological models},
journal = {Environmental Modelling & Software},
volume = {73},
pages = {286-304},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215300438},
author = {Mattia Sanna and Gianni Bellocchi and Mattia Fumagalli and Marco Acutis},
keywords = {Model evaluation, Performance indicators, Stable correlation},
abstract = {The use of a variety of metrics is advocated to assess model performance but correlated metrics may convey the same information, thus leading to redundancy. Starting from this assumption, a method was developed for selecting, from among a collection of performance indicators, one or more subsets providing the same information as the entire set. The method, based on the definition of “stable correlation”, was applied to 23 performance indicators of agrometeorological models, calculated on large sets of simulated and observed data of four agronomic and meteorological variables: above-ground biomass, leaf area index, hourly air relative humidity and daily solar radiation. Two subsets were determined: {Squared Bias, Root Mean Squared Relative Error, Coefficient of Determination, Pattern Index, Modified Modelling Efficiency}, {Persistence Model Efficiency, Root Mean Squared Relative Error, Coefficient of Determination, Pattern Index}. The method needs corroboration but is statistically founded and can support the implementation of standardized evaluation tools.}
}
@article{LU201841,
title = {Improvements to the calibration of a geographically weighted regression with parameter-specific distance metrics and bandwidths},
journal = {Computers, Environment and Urban Systems},
volume = {71},
pages = {41-57},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517303447},
author = {Binbin Lu and Wenbai Yang and Yong Ge and Paul Harris},
keywords = {Local regression, Spatial heterogeneity, Bandwidth selection, Multi-scale, GWmodel},
abstract = {In standard geographically weighted regression (GWR), the spatially-varying relationships between the dependent and each independent variable are explored under a constant and fixed scale, but for many processes their variation intensity may differ with respect to location and direction. To address this short-coming, a GWR model with parameter-specific distance metrics (PSDM GWR) can be used, which by default, also specifies parameter specific bandwidths. In doing so, PSDM GWR provides a scale-dependent extension of GWR. Commonly however, an ideal distance metric for a given independent variable parameter is not immediately obvious. Thus, in this article, PSDM GWR is investigated with respect to distance metric choice. Here, it is demonstrated that the optimum (distance metric specific) bandwidth corresponding to a given independent variable remains essentially constant, independent of the choices made for the other independent variables. This result allows for a considerable saving in computational overheads, permitting a much simpler searching procedure for multiple bandwidth optimization. Results are first demonstrated empirically, and then a simulation experiment is conducted to objectively verify the same findings. Computational savings are vital to the uptake of PSDM GWR, where ultimately, it should be considered the default choice in any GWR-based study of spatially-varying relationships, as standard GWR, mixed (or semi-parametric) GWR, flexible bandwidth (or multi-scale) GWR and the global regression are specific cases thereof.}
}
@article{ARRAS2001131,
title = {Multisensor on-the-fly localization:: Precision and reliability for applications},
journal = {Robotics and Autonomous Systems},
volume = {34},
number = {2},
pages = {131-143},
year = {2001},
note = {European Workshop on Advanced Mobile Robots},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(00)00117-2},
url = {https://www.sciencedirect.com/science/article/pii/S0921889000001172},
author = {Kai O. Arras and Nicola Tomatis and Björn T. Jensen and Roland Siegwart},
keywords = {Mobile robot localization, On-the-fly localization, Position tracking, Multisensor data fusion, Kalman filtering},
abstract = {This paper presents an approach for localization using geometric features from a 360° laser range finder and a monocular vision system. Its practicability under conditions of continuous localization during motion in real time (referred to as on-the-fly localization) is investigated in large-scale experiments. The features are infinite horizontal lines for the laser and vertical lines for the camera. They are extracted using physically well-grounded models for all sensors and passed to a Kalman filter for fusion and position estimation. Positioning accuracy close to subcentimeter has been achieved with an environment model requiring 30bytes/m2. Already with a moderate number of matched features, the vision information was found to further increase this precision, particularly in the orientation. The results were obtained with a fully self-contained system where extensive tests with an overall length of more than 6.4km and 150,000 localization cycles have been conducted. The final testbed for this localization system was the Computer 2000 event, an annual computer tradeshow in Lausanne, Switzerland, where during 4 days visitors could give high-level navigation commands to the robot via a web interface. This gave us the opportunity to obtain results on long-term reliability and verify the practicability of the approach under application-like conditions. Furthermore, general aspects and limitations of multisensor on-the-fly localization are discussed.}
}
@article{BARBOSA201491,
title = {Remotely sensed biomass over steep slopes: An evaluation among successional stands of the Atlantic Forest, Brazil},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {88},
pages = {91-100},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2013.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0924271613002815},
author = {Jomar Magalhães Barbosa and Ignacio Melendez-Pastor and Jose Navarro-Pedreño and Marisa Dantas Bitencourt},
keywords = {Aboveground biomass, Forest succession, Tropical forest, Steep slope, Remote sensing},
abstract = {Remotely sensed images have been widely used to model biomass and carbon content on large spatial scales. Nevertheless, modeling biomass using remotely sensed data from steep slopes is still poorly understood. We investigated how topographical features affect biomass estimation using remotely sensed data and how such estimates can be used in the characterization of successional stands in the Atlantic Rainforest in southeastern Brazil. We estimated forest biomass using a modeling approach that included the use of both satellite data (LANDSAT) and topographic features derived from a digital elevation model (TOPODATA). Biomass estimations exhibited low error predictions (Adj. R2=0.67 and RMSE=35Mg/ha) when combining satellite data with a secondary geomorphometric variable, the illumination factor, which is based on hill shading patterns. This improved biomass prediction helped us to determine carbon stock in different forest successional stands. Our results provide an important source of modeling information about large-scale biomass in remaining forests over steep slopes.}
}
@article{GALMES20113863,
title = {Randomized Data-Gathering protocol for time-driven sensor networks},
journal = {Computer Networks},
volume = {55},
number = {17},
pages = {3863-3885},
year = {2011},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2011.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389128611002970},
author = {Sebastià Galmés and Ramon Puigjaner},
keywords = {Sensor network, MAC layer protocol, Energy efficiency, Synchronization, Scalability, Low power listening (LPL)},
abstract = {Proactive or time-driven sensor networks are devoted to the continuous reporting of environmental data to a sink or base station. An important issue associated with these networks is the management of a potentially large number of packets that are regularly generated by the set of nodes. A common solution at the MAC level is to use scheduled TDMA-based protocols, which minimize the communication duty cycle. However, TDMA schemes have strong synchronization requirements and exhibit low adaptability to changing traffic conditions. Thus, this paper proposes an alternative MAC protocol that overcomes the limitations of TDMA-based protocols while still approaching their performance in terms of lifetime. Essentially, this proposed solution combines randomization of the sensing and transmission process with a mechanism based on transmission announcements between adjacent nodes. The result is a lightweight protocol, called Randomized Data-Gathering (RDG), which exhibits desirable characteristics with respect to energy efficiency, synchronization avoidance, performance, scalability and adaptability to traffic changes in both time and space. The role of the randomization distribution is considered in detail, and an effective randomization scheme is selected. Both analytical and simulation methods are applied.}
}
@article{SHIM201481,
title = {Semiparametric spatial effects kernel minimum squared error model for predicting housing sales prices},
journal = {Neurocomputing},
volume = {124},
pages = {81-88},
year = {2014},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2013.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0925231213007881},
author = {Jooyong Shim and Okmyung Bin and Changha Hwang},
keywords = {Housing sale price, Kernel minimum squared error, Least squares support vector machine, Prediction, Semiparametric, Spatial effect},
abstract = {Semiparametric regression models have been extensively used to predict housing sales prices, but semiparametric kernel machines with spatial effect have not been studied yet. This paper proposes the semiparametric spatial effect kernel minimum squared error model (SSEKMSEM) and the semiparametric spatial effect least squares support vector machine (SSELS-SVM) for estimating a hedonic price function and compares the price prediction performance with the conventional parametric models and a semiparametric generalized additive model (GAM). This paper utilizes two data sets. One is a large data set representing 5966 single-family residential home sales between July 2000 and August 2008 from Pitt County, North Carolina. The other is a data set of residential property sales records from September 2000 to September 2004 in Carteret County, North Carolina. The results show that the SSEKMSEM and SSELS-SVM outperform the parametric counterparts and the semiparametric GAM in both in-sample and out-of-sample price predictions, indicating that these kernel machines can be useful for measurement and prediction of housing sales prices.}
}
@article{LANE2006669,
title = {Applying hierarchical task analysis to medication administration errors},
journal = {Applied Ergonomics},
volume = {37},
number = {5},
pages = {669-679},
year = {2006},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2005.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687005001225},
author = {Rhonda Lane and Neville A. Stanton and David Harrison},
keywords = {Medication administration errors, HTA, SHERPA, Human error},
abstract = {Medication use in hospitals is a complex process and is dependent on the successful interaction of health professionals functioning within different disciplines. Errors can occur at any one of the five main stages of prescribing, documenting, dispensing or preparation, administering and monitoring. The responsibility for the error is often placed on the nurse, as she or he is the last person in the drug administration chain whilst more pressing underlying causal factors remain unresolved. This paper demonstrates how hierarchical task analysis can be used to model drug administration and then uses the systematic human error reduction and prediction approach to predict which errors are likely to occur. The paper also puts forward design solutions to mitigate these errors.}
}
@article{MUSTAFA2020104654,
title = {Integrated Bayesian Multi-model approach to quantify input, parameter and conceptual model structure uncertainty in groundwater modeling},
journal = {Environmental Modelling & Software},
volume = {126},
pages = {104654},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104654},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219305596},
author = {Syed Md Touhidul Mustafa and Jiri Nossent and Gert Ghysels and Marijke Huysmans},
keywords = {Conceptual model structure uncertainty, Bayesian approach, Input uncertainty, Bayesian model averaging, Uncertainty quantification, Groundwater flow model},
abstract = {A flexible Integrated Bayesian Multi-model Uncertainty Estimation Framework (IBMUEF) is presented to simultaneously quantify conceptual model structure, input and parameter uncertainty of a groundwater flow model. In this fully Bayesian framework, the DiffeRential Evolution Adaptive Metropolis (DREAM) algorithm with a novel likelihood function is combined with Bayesian Model Averaging (BMA). Four alternative conceptual models, representing different geological representations of an overexploited aquifer, have been developed. The uncertainty of the input of the model is represented by multipliers. A novel likelihood function based on a new heteroscedastic error model is included to extend the applicability of the framework. The results of the study confirm that neglecting conceptual model structure uncertainty results in unreliable prediction. Consideration of both model structure and input uncertainty are important to obtain confident parameter sets and better model predictions. This study shows that the IBMUEF provides more reliable model predictions and accurate uncertainty bounds.}
}
@article{CHEN2021101708,
title = {Entangled footprints: Understanding urban neighbourhoods by measuring distance, diversity, and direction of flows in Singapore},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101708},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101708},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001150},
author = {Qingqing Chen and I-Ting Chuang and Ate Poorthuis},
keywords = {Human mobility, Location-based-services (LBS), Activity space twitter, Spatial networks, Urban Neighbourhoods, Singapore},
abstract = {Traditional approaches to human mobility analysis in Geography often rely on census or survey data that is resource-intensive to collect and often has a limited spatio-temporal scope. The advent of new technologies (e.g. geosocial media platforms) provides opportunities to overcome these limitations and, if properly leveraged, can yield more granular insights about human mobility. In this paper, we use an anonymized Twitter dataset collected in Singapore from 2012 to 2016 to investigate this potential to help understand the footprints of urban neighbourhoods from both a spatial and a relational perspective. We construct home-to-destination networks of individual users based on their inferred home locations. In aggregated form, these networks allow us to analyze three specific mobility indicators at the neighbourhood level, namely the distance, diversity, and direction of urban interactions. By mapping these three indicators of the spatial footprint of each neighbourhood, we can capture the nuances in the position of individual neighbourhoods within the larger urban network. An exploratory spatial regression reveals that socio-economic characteristics (e.g. share of rental housing) and the built environment (i.e. land use) only partially explain these three indicators and a residual analysis points to the need to explicitly include each neighbourhood's position within the transportation network in future work.}
}
@article{BATER2009289,
title = {Evaluating error associated with lidar-derived DEM interpolation},
journal = {Computers & Geosciences},
volume = {35},
number = {2},
pages = {289-300},
year = {2009},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2008.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300408002537},
author = {Christopher W. Bater and Nicholas C. Coops},
keywords = {Remote sensing, DEM validation, Prediction uncertainty, CART analysis},
abstract = {Light detection and ranging (lidar) technology is capable of precisely measuring a variety of vegetation metrics, the estimates of which are usually based on relative heights above a digital elevation model (DEM). As a result, the development of these elevation models is a critical step when processing lidar observations. A number of different algorithms exist to interpolate lidar ground hits into a terrain surface. We tested seven interpolation routines, using small footprint lidar data, collected over a range of vegetation classes on Vancouver Island, British Columbia, Canada. The lidar data were randomly subsetted into a prediction dataset and a validation dataset. A suite of DEMs were then generated using linear, quintic, natural neighbour, regularized spline, spline with tension, a finite difference approach (ANUDEM), and inverse distance weighted interpolation routines, at spatial resolutions of 0.5, 1.0 and 1.5m. In order to examine the effects of terrain and ground cover on interpolation accuracies, the study area was stratified by terrain slope, vegetation structural class, lidar ground return density, and normalized difference vegetation indices (NDVI) derived from Quickbird and Landsat7 ETM+ imagery. The root mean square (RMS) and mean absolute errors of the residuals between the surfaces and the validation points indicated that the 0.5m DEMs were the most accurate. Of the tested approaches, the regularized spline and IDW algorithms produced the most extreme outliers, sometimes in excess of ±6m in sloping terrain. Overall, the natural neighbour algorithm provided the best results with a minimum of effort. Finally, a method to create prediction uncertainty maps using classification and regression tree (CART) analysis is proposed.}
}
@article{GIFFORD2012853,
title = {Subglacial water presence classification from polar radar data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {25},
number = {4},
pages = {853-868},
year = {2012},
note = {Special Section: Dependable System Modelling and Analysis},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2011.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0952197611002351},
author = {Christopher M. Gifford and Arvin Agah},
keywords = {Pattern recognition, Machine learning, Ensemble classification, Radar remote sensing, Subglacial water},
abstract = {Ground and airborne radar depth-sounding of the Greenland and Antarctic ice sheets have been used for many years to remotely determine characteristics such as ice thickness, subglacial topography, and mass balance of large bodies of ice. Ice coring efforts have supported these radar data to provide ground truth for validation of the state (wet or frozen) of the interface between the bottom of the ice sheet and the underlying bedrock. Subglacial state governs the friction, flow speed, transport of material, and overall change of the ice sheet. In this paper, we utilize machine learning and classifier combination to model water presence from airborne polar radar data acquired on Greenland in 1999 and 2007. The underlying method results in radar independence, allowing model transfer from 1999 to 2007 radar data to produce water presence maps of the Greenland ice sheet with differing radars. We focus on how to construct a successful set of classifiers capable of high classification accuracy. Utilizing multiple machine learning algorithms is shown to be successful for this classification problem, achieving 86% classification accuracy in the best case. Several heuristics are presented for constructing teams of multiple classifiers for predicting subglacial water presence. The presented methodology could also be applied to radar data acquired over the Antarctic ice sheet.}
}
@article{LOWRY201459,
title = {Comparing spatial metrics that quantify urban form},
journal = {Computers, Environment and Urban Systems},
volume = {44},
pages = {59-67},
year = {2014},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971513001142},
author = {John H. Lowry and Michael B. Lowry},
keywords = {Urban form, Urban growth policy, Density, Street pattern, Accessibility, Urban sprawl},
abstract = {Measuring and characterizing urban form is an important task for planners and policy analysts. This paper compares eighteen metrics of urban form for 542 neighborhoods in Salt Lake County, Utah. The comparison was made in the context of characterizing three neighborhood types from different time periods: pre-suburban (1891–1944), suburban (1945–1990), and late-suburban (1990–2007). We used correlation analysis, within and across time periods, to assess each metric’s ability to uniquely characterize urban form; and we used linear regression to assess the ability to distinguish neighborhood type. Three of the metrics show redundancy and two did not capture differences in urban form for the case study. Based on our findings, we recommend thirteen of the eighteen metrics for planners and policy analysts who want to quantify urban form using spatial data that are commonly available. Furthermore, our case study shows that despite policy efforts to encourage “smart growth,” urban neighborhoods in Salt Lake County continue to exhibit characteristics of “sprawl.” These findings suggest the effectiveness of smart growth policies in Salt Lake County have had limited effect.}
}
@article{WEI2018580,
title = {Architecture-level hazard analysis using AADL},
journal = {Journal of Systems and Software},
volume = {137},
pages = {580-604},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301188},
author = {Xiaomin Wei and Yunwei Dong and Xuelin Li and W. Eric Wong},
keywords = {Hazard analysis, Model transformation, Semantic preservation, AADL, Hazard model annex},
abstract = {Software systems are becoming increasingly important in safety-critical areas. Designing safe software requires a significant emphasis on hazards in the early design phase of software development. In this paper, we propose a hazard analysis approach based on Architecture Analysis and Design Language (AADL). First, to make up the deficiencies of Error Model Annex (EMV2), we create Hazard Model Annex (HMA) to specify the hazard sources, hazards, hazard trigger mechanisms, and mishaps. By using HMA, a safety model can be built by annotating an architecture model with the error model and hazard model. Then, an architecture-level hazard analysis approach is proposed to automatically generate the hazard analysis table. The approach contains the model transformation from a safety model to a Deterministic Stochastic Petri Nets (DSPNs) model for calculating the occurrence probability of hazards and mishaps. In addition, we present the formal semantics for each constituent part of the safety model, define the model mapping rules, and verify the semantic preservation of the transformation. Finally, HMA is implemented to build safety models and two Eclipse plug-ins of our methodology are also implemented. A case study on a flight control software system has been employed to demonstrate the feasibility of our proposed technique.}
}
@article{RODRIGOCOMINO2019147,
title = {Determining the best ISUM (Improved stock unearthing Method) sampling point number to model long-term soil transport and micro-topographical changes in vineyards},
journal = {Computers and Electronics in Agriculture},
volume = {159},
pages = {147-156},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918308925},
author = {Jesús Rodrigo-Comino and Ali Keshavarzi and Mojtaba Zeraatpisheh and Yeboah Gyasi-Agyei and Artemi Cerdà},
keywords = {Soil erosion, Vineyards, ISUM, Measuring point interval, Interpolation methods},
abstract = {Advances in soil erosion measuring tools and micro-topography modelling will contribute to our understanding of land degradation processes and help to design correct erosion mitigation measures in agricultural fields. Vineyards being one of the most degraded agricultural landscapes, it is necessary to accurately predict soil erosion levels within them. One possible method to achieve this goal in vine plantations is ISUM (improved stock unearthing method). To apply ISUM, it is necessary to detect the graft unions which are recognised as passive bioindicators of the original micro-topography at the time of planting. In this paper, we propose a methodology to determine: (i) how many measuring points are necessary to reach the best estimate of soil erosion for developing current soil surface level maps; and (ii) which spatial interpolation method is the best to map the micro-topographical changes. ISUM was applied in the Ruwer-Mosel valley vineyards (Germany) using 18 measuring points at 10 cm intervals between opposite pair graft unions of 1.7 m inter-row distance. Several interpolation methods were used to map the micro-topography changes and anisotropic ordinary kriging (OK) emerged as the best as judged by the performance statistics of the coefficient of determination and the root-mean-square-error. Our findings demonstrated that soil erosion rates were 40.1, 39.4, 25.0, 38.9, 37.9, to 64.8 Mg ha−1 yr−1 over the 40 years since the establishment of the vineyard studied, when using 18, 15, 10, 7, 5 and 2 measuring points, respectively. We propose that ISUM can be standardised as using measuring points at 10 cm intervals.}
}
@article{KOVANEN201839,
title = {Near real-time coastal flood inundation simulation with uncertainty analysis and GPU acceleration in a web environment},
journal = {Computers & Geosciences},
volume = {119},
pages = {39-48},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0098300418303376},
author = {Janne Kovanen and Juha Oksanen and Tapani Sarjakoski},
keywords = {Inundation forecasting, Flood risk management, Spatial analysis, Digital elevation model, Graphics processing unit, Uncertainty},
abstract = {A proof of concept is presented on how to produce uncertainty-aware near real-time coastal flood inundation Web maps from water-level observations and predictions, which have been computed for tide gauge sites and made publicly accessible. The stochastic inundation simulation takes into account several sources of uncertainty, which have until now not been employed in either bathtub models or hydrodynamic models. The simulation is based on the Monte Carlo method. The feasibility of the proposed approach is demonstrated by an implementation using general-purpose computing on graphics processing units. The outcome of the research is that the current technologies enable the building of a novel system that connects to official data sources and that takes into account sources of uncertainty whose inclusion in the past has been avoided by being either weakly known or computationally too expensive.}
}
@article{MEINEN2021105229,
title = {From hillslopes to watersheds: Variability in model outcomes with the USLE},
journal = {Environmental Modelling & Software},
volume = {146},
pages = {105229},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105229},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002711},
author = {Benjamin U. Meinen and Derek T. Robinson},
keywords = {Universal soil loss equation, Erosion, Watershed, Uncertainty, Soil degradation, GIS},
abstract = {The Universal Soil Loss Equation (USLE) has been the de-facto standard for soil erosion management studies since its seminal publication in the 1970's. Widespread use of the model is due, in part, to its simple empirical modelling structure and parsimonious parameterization; however, these benefits have also led to criticism of its relevance as a tool for estimating soil erosion. While the USLE has a strong empirical basis, it is regularly used beyond its intended design space, i.e., predicting soil loss from planar hillslopes, to predict distributed soil erosion rates at large spatial extents, which introduces uncertainty in model outcomes. In this paper, we use a case study for up-scaling the USLE to a large spatial extent to assess the variability in model outcomes from different model user's design choices. Our analysis demonstrates that a standardized and accredited methodology for up-scaling the USLE is needed to reduce uncertainty in model outcomes.}
}
@article{KOUTSAKIS2006681,
title = {Prediction-based resource allocation for multimedia traffic over high-speed wireless networks},
journal = {AEU - International Journal of Electronics and Communications},
volume = {60},
number = {10},
pages = {681-689},
year = {2006},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2006.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1434841106000070},
author = {P. Koutsakis and M. Vafiadis and H. Papadakis},
abstract = {Both call admission control (CAC) and efficient scheduling are of great importance in next generation wireless networks, which are expected to handle various types of highly demanding multimedia users. In this paper, we present and evaluate a new mechanism which combines CAC with bandwidth allocation in a high-speed downlink time division multiple access (TDMA) wireless channel with errors; our mechanism incorporates predictions of the wireless channel condition in its decision making and our results show that, with the use of the feedback between the scheduler and the admission controller, system performance is significantly enhanced (in terms of voice-WAP-SMS-H.263 video QoS) compared to a scheme without prediction on the channel condition.}
}
@article{GOMES2021106508,
title = {On the prediction of long-lived bugs: An analysis and comparative study using FLOSS projects},
journal = {Information and Software Technology},
volume = {132},
pages = {106508},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106508},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920302482},
author = {Luiz Alberto Ferreira Gomes and Ricardo {da Silva Torres} and Mario Lúcio Côrtes},
keywords = {Software maintenance, Bug Tracking System, Long-lived bugs, Machine learning, Text mining},
abstract = {Context:
Software evolution and maintenance activities in today’s Free/Libre Open Source Software (FLOSS) rely primarily on information extracted from bug reports registered in bug tracking systems. Many studies point out that most bugs that adversely affect the user’s experience across versions of FLOSS projects are long-lived bugs. However, proposed approaches that support bug fixing procedures do not consider the real-world lifecycle of a bug, in which bugs are often fixed very fast. This may lead to useless efforts to automate the bug management process.
Objective:
This study aims to confirm whether the number of long-lived bugs is significantly high in popular open-source projects and to characterize the population of long-lived bugs by considering the attributes of bug reports. We also aim to conduct a comparative study evaluating the prediction accuracy of five well-known machine learning algorithms and text mining techniques in the task of predicting long-lived bugs.
Methods:
We collected bug reports from six popular open-source projects repositories (Eclipse, Freedesktop, Gnome, GCC, Mozilla, and WineHQ) and used the following machine learning algorithms to predict long-lived bugs: K-Nearest Neighbor, Naïve Bayes, Neural Networks, Random Forest, and Support Vector Machines.
Results:
Our results show that long-lived bugs are relatively frequent (varying from 7.2% to 40.7%) and have unique characteristics, confirming the need to study solutions to support bug fixing management. We found that the Neural Network classifier yielded the best results in comparison to the other algorithms evaluated.
Conclusion:
Research efforts regarding long-lived bugs are needed and our results demonstrate that it is possible to predict long-lived bugs with a high accuracy (around 70.7%) despite the use of simple prediction algorithms and text mining methods.}
}
@article{SANTOS2018450,
title = {A systematic review on the code smell effect},
journal = {Journal of Systems and Software},
volume = {144},
pages = {450-477},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301444},
author = {José Amancio M. Santos and João B. Rocha-Junior and Luciana Carla Lins Prates and Rogeres Santos do Nascimento and Mydiã Falcão Freitas and Manoel Gomes de Mendonça},
keywords = {Code smell, Systematic review, Thematic synthesis},
abstract = {Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics.}
}
@article{GUILLET2020237,
title = {Camera orientation, calibration and inverse perspective with uncertainties: A Bayesian method applied to area estimation from diverse photographs},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {159},
pages = {237-255},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619302734},
author = {Grégoire Guillet and Thomas Guillet and Ludovic Ravanel},
keywords = {Inverse perspective, Spatial resection, Camera calibration, Bayesian methods, Lens distortion, Digital elevation models},
abstract = {Large collections of images have become readily available through modern digital catalogs, from sources as diverse as historical photographs, aerial surveys, or user-contributed pictures. Exploiting the quantitative information present in such wide-ranging collections can greatly benefit studies that follow the evolution of landscape features over decades, such as measuring areas of glaciers to study their shrinking under climate change. However, many available images were taken with low-quality lenses and unknown camera parameters. Useful quantitative data may still be extracted, but it becomes important to both account for imperfect optics, and estimate the uncertainty of the derived quantities. In this paper, we present a method to address both these goals, and apply it to the estimation of the area of a landscape feature traced as a polygon on the image of interest. The technique is based on a Bayesian formulation of the camera calibration problem. First, the probability density function (PDF) of the unknown camera parameters is determined for the image, based on matches between 2D (image) and 3D (world) points together with any available prior information. In a second step, the posterior distribution of the feature area of interest is derived from the PDF of camera parameters. In this step, we also model systematic errors arising in the polygon tracing process, as well as uncertainties in the digital elevation model. The resulting area PDF therefore accounts for most sources of uncertainty. We present validation experiments, and show that the model produces accurate and consistent results. We also demonstrate that in some cases, accounting for optical lens distortions is crucial for accurate area determination with consumer-grade lenses. The technique can be applied to many other types of quantitative features to be extracted from photographs when careful error estimation is important.}
}
@article{FUGLSANG20131,
title = {Modelling land-use effects of future urbanization using cellular automata: An Eastern Danish case},
journal = {Environmental Modelling & Software},
volume = {50},
pages = {1-11},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213001813},
author = {Morten Fuglsang and Bernd Münier and Henning Sten Hansen},
keywords = {Urban modelling, Scenario interpretations, Transportation paradigm shift, Cellular automata, GIS},
abstract = {The modelling of land use change is a way to analyse future scenarios by modelling different pathways. Application of spatial data of different scales coupled with socio-economic data makes it possible to explore and test the understanding of land use change relations. In the EU-FP7 research project PASHMINA (Paradigm Shift modelling and innovative approaches), three storylines of future transportation paradigm shifts towards 2040 are created. These storylines are translated into spatial planning strategies and modelled using the cellular automata model LUCIA. For the modelling, an Eastern Danish case area was selected, comprising of the Copenhagen metropolitan area and its hinterland. The different scenarios are described using a range of different descriptive GIS datasets. These include mapping of accessibility based on public and private transportation, urban density and structure, and distribution of jobs and population. These indicators are then incorporated in the model calculations as factors determining urban development, related to the scenario outlines. The results calculated from the scenarios reveals the great difference in urban distribution that different spatial planning strategies can produce, changing the shape of the urban landscape. The scenarios visualized showed to outline different planning strategies that could be implemented, creating a more homogenous urban structure targeted at a reduction of transportation work and thus energy consumption. This will lead to less impact on climate from transportation based on a more optimal localization and transport infrastructure strategy.}
}
@article{WANG2019101386,
title = {Perceptions of built environment and health outcomes for older Chinese in Beijing: A big data approach with street view images and deep learning technique},
journal = {Computers, Environment and Urban Systems},
volume = {78},
pages = {101386},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101386},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519301644},
author = {Ruoyu Wang and Ye Liu and Yi Lu and Jinbao Zhang and Penghua Liu and Yao Yao and George Grekousis},
keywords = {Tencent street view (TSV), Perceived built environment attributes, Deep learning, Health outcomes, Older adults},
abstract = {Built environment attributes have been demonstrated to be associated with various health outcomes. However, most empirical studies have typically focused on objective built environmental measures. Still, perceptions of the built environment also play an important role in health and may complement studies with objective measures. Some built environment attributes, such as liveliness or beauty, are difficult to measure objectively. Traditional methods to assess perceptions of the built environment, such as questionnaires and focus groups, are time-consuming and prone to recall bias. The recent development in machine deep learning techniques and big data of street view images, makes it possible to assess perceptions of the built environment with street view images for a large-scale study area. By using online free Tencent Street View (TSV) images, this study assessed six perceptual attributes of the built environment: wealth, safety, liveliness, depression, bore and beauty. These attributes were associated with both the physical and the mental health outcomes of 1231 older adults in 48 neighborhoods in the Haidian District, Beijing, China. Results show that perceived safety was significantly associated with both the physical and mental health outcomes. Perceived depression and beauty were significant related to older adults' mental health, while perceived wealth, bore and liveliness were significantly related to their physical health. The findings carry important policy implications and hence contribute to the development of healthy cities. It is urgent to improve residents' positive perceptions and decrease their negative perceptions of the built environment, especially in neighborhoods that are highly populated by older adults.}
}
@article{LONG2022101710,
title = {Associations between mobility and socio-economic indicators vary across the timeline of the Covid-19 pandemic},
journal = {Computers, Environment and Urban Systems},
volume = {91},
pages = {101710},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101710},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001174},
author = {Jed A. Long and Chang Ren},
keywords = {COVID-19, Mobility, Socio-economic, Big data, Spatial model},
abstract = {Covid-19 interventions are greatly affecting patterns of human mobility. Changes in mobility during Covid-19 have differed across socio-economic gradients during the first wave. We use fine-scale network mobility data in Ontario, Canada to study the association between three different mobility measures and four socio-economic indicators throughout the first and second wave of Covid-19 (January to December 2020). We find strong associations between mobility and the socio-economic indicators and that relationships between mobility and other socio-economic indicators vary over time. We further demonstrate that understanding how mobility has changed in response to Covid-19 varies considerably depending on how mobility is measured. Our findings have important implications for understanding how mobility data should be used to study interventions across space and time. Our results support that Covid-19 non-pharmaceutical interventions have resulted in geographically disparate responses to mobility and quantifying mobility changes at fine geographical scales is crucial to understanding the impacts of Covid-19.}
}
@article{HSIAO2013659,
title = {Predictive models of safety based on audit findings: Part 2: Measurement of model validity},
journal = {Applied Ergonomics},
volume = {44},
number = {4},
pages = {659-666},
year = {2013},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S000368701300015X},
author = {Yu-Lin Hsiao and Colin Drury and Changxu Wu and Victor Paquet},
keywords = {Human error, HFACS-MA, Safety prediction, Neural network, Aviation maintenance},
abstract = {Part 1 of this study sequence developed a human factors/ergonomics (HF/E) based classification system (termed HFACS-MA) for safety audit findings and proved its measurement reliability. In Part 2, we used the human error categories of HFACS-MA as predictors of future safety performance. Audit records and monthly safety incident reports from two airlines submitted to their regulatory authority were available for analysis, covering over 6.5 years. Two participants derived consensus results of HF/E errors from the audit reports using HFACS-MA. We adopted Neural Network and Poisson regression methods to establish nonlinear and linear prediction models respectively. These models were tested for the validity of prediction of the safety data, and only Neural Network method resulted in substantially significant predictive ability for each airline. Alternative predictions from counting of audit findings and from time sequence of safety data produced some significant results, but of much smaller magnitude than HFACS-MA. The use of HF/E analysis of audit findings provided proactive predictors of future safety performance in the aviation maintenance field.}
}
@article{VANNOOIJEN2012167,
title = {A problem in hydrological model calibration in the case of averaged flux input and flux output},
journal = {Environmental Modelling & Software},
volume = {37},
pages = {167-178},
year = {2012},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212001090},
author = {R.R.P. van Nooijen and A.G. Kolechkina},
keywords = {Conceptual rainfall run-off modelling, Parameter calibration, Interval arithmetic, Linear reservoir model, Numerical methods, Hydrology, Calibration},
abstract = {This paper shows that the uncertainty due to derivation of fluxes from integral model input can be at least as large as that due to uncertainty in the measurements. The method used to illustrate the problem is interval arithmetic. During calibration hydrological models often seem to perform equally well for two or more sets of parameters that differ considerably. There are many ingredients that may contribute to this. One of those ingredients is a mismatch of information present in input and output data and information used by the model. In this paper we examine one particular case where this occurs. We assume that the input data sets are time series of fluxes integrated over fixed or variable time intervals, while outputs can be the result of continuous or discrete, direct or indirect flux measurements. We assume that for model calibration purposes each output value represents a value at a specific point in time. We show that even for a linear reservoir the process of averaging and sampling may make recovery of its sole parameter impossible. We also show that these problems can be diagnosed and for this simple model also solved through the use of interval arithmetic.}
}
@article{YAN2006364,
title = {Different experiences, different effects: a longitudinal study of learning a computer program in a network environment},
journal = {Computers in Human Behavior},
volume = {22},
number = {3},
pages = {364-380},
year = {2006},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2004.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0747563204001268},
author = {Zheng Yan},
keywords = {Computer experience, Computer performance, Computer network, Longitudinal research, Multilevel growth modeling},
abstract = {Students’ previous computer experience has been widely considered an important factor affecting subsequent computer performance. However, little research has been done to examine the contributions of different types of computer experience to computer performance at different time points. The present study compared the effects of four types of computer experience on 30 graduate students’ learning of a statistical program over one semester. Among the four types of computer experience, students’ earlier experience of using computer network systems was found to affect their initial performance of learning the statistics program, but the experience of using statistical programs, the experience of email programs, and the length of using computers did not. These findings suggest complex relationships between students’ computer experience and their computer performance and have implications for both learning and teaching computer programs and understanding the transfer of learning.}
}
@article{PAVLIS20041079,
title = {The generalized earthquake-location (GENLOC) package: an earthquake-location library},
journal = {Computers & Geosciences},
volume = {30},
number = {9},
pages = {1079-1091},
year = {2004},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2004.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300404001438},
author = {Gary L. Pavlis and Frank Vernon and Danny Harvey and Dan Quinlan},
keywords = {Seismic, Earthquake location, Animal times, Array showness, Nonlinear optimization, Array processing},
abstract = {We describe a library and associated set of applications for locating seismic events. The library is called the GENeralized LOCation (GENLOC) library because it is a general library that implements most methods commonly used for single event locations. The library has a flexible implementation of the standard Gauss–Newton method with many options for weighting schemes, inversion methods, and algorithms for choosing an initial location estimate. GENLOC also has a grid-search algorithm that makes no assumptions about the geometry of the grid it is searching returning only the point with a best fit solution for the specified residual norm. GENLOC supports both arrival time and array slowness vector measurements. A unique feature is the strong separation between the travel time/earth model problem and the location estimations. GENLOC can utilize data from any seismic phase for which the user can supply an earth model and method to compute theoretical travel times and/or slowness values. The GENLOC library has been used in five different working applications: (1) a simple command line program, (2) an interactive graphical user interface version used in an analyst information system, (3) a database-driven relocation program, (4) a recent implementation of the progressive multiple event location method, and (5) a real-time location program. We ran a validation test against LOCSAT and found reasonable consistency in estimated locations. We attribute observed differences in the solutions to roundoff errors in different calculators used by the two programs.}
}
@article{LI2013114,
title = {Three schemes for wireless coded broadcast to heterogeneous users},
journal = {Physical Communication},
volume = {6},
pages = {114-123},
year = {2013},
note = {Network Coding and its Applications to Wireless Communications},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2012.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1874490712000420},
author = {Yao Li and Emina Soljanin and Predrag Spasojević},
keywords = {LT codes, Chunked codes, Growth codes, Wireless broadcast, Network coding},
abstract = {We study and compare three coded schemes for the single-server wireless broadcast of multiple description coded content to heterogeneous users. The users (sink nodes) demand different numbers of descriptions over links with different packet loss rates. The three coded schemes are based on the LT codes, growth codes, and randomized chunked codes. The schemes are compared on the basis of the total number of transmissions required to deliver the demands of all users, which we refer to as the server (source) delivery time. We design the degree distributions of LT codes by solving suitably defined linear optimization problems, and numerically characterize the achievable delivery time for different coding schemes. We find that including a systematic phase (uncoded transmission) is significantly beneficial for scenarios with low demands, and that coding is necessary for efficiently delivering high demands. Different demand and error rate scenarios may require very different coding schemes. Growth codes and chunked codes do not perform as well as optimized LT codes in the heterogeneous communication scenario.}
}
@article{KANOULAS201913,
title = {Curved patch mapping and tracking for irregular terrain modeling: Application to bipedal robot foot placement},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {13-30},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308242},
author = {Dimitrios Kanoulas and Nikos G. Tsagarakis and Marsette Vona},
keywords = {Irregular surface modeling, Foothold contact modeling, Bounded curved patch modeling, Curved patch fitting and tracking, 3D perception for bipedal robots, Bipedal robot foot placement, Rough terrain stepping, Legged robot locomotion},
abstract = {Legged robots need to make contact with irregular surfaces, when operating in unstructured natural terrains. Representing and perceiving these areas to reason about potential contact between a robot and its surrounding environment, is still largely an open problem. This paper introduces a new framework to model and map local rough terrain surfaces, for tasks such as bipedal robot foot placement. The system operates in real-time, on data from an RGB-D and an IMU sensor. We introduce a set of parametrized patch models and an algorithm to fit them in the environment. Potential contacts are identified as bounded curved patches of approximately the same size as the robot’s foot sole. This includes sparse seed point sampling, point cloud neighborhood search, and patch fitting and validation. We also present a mapping and tracking system, where patches are maintained in a local spatial map around the robot as it moves. A bio-inspired sampling algorithm is introduced for finding salient contacts. We include a dense volumetric fusion layer for spatiotemporally tracking, using multiple depth data to reconstruct a local point cloud. We present experimental results on a mini-biped robot that performs foot placements on rocks, implementing a 3D foothold perception system, that uses the developed patch mapping and tracking framework.}
}
@article{MELCHIORRE2011410,
title = {Evaluation of prediction capability, robustness, and sensitivity in non-linear landslide susceptibility models, Guantánamo, Cuba},
journal = {Computers & Geosciences},
volume = {37},
number = {4},
pages = {410-425},
year = {2011},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2010.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0098300410003286},
author = {C. Melchiorre and E.A. {Castellanos Abella} and C.J. {van Westen} and M. Matteucci},
keywords = {Landslide susceptibility mapping, Artificial neural networks, Model evaluation},
abstract = {This paper describes a procedure for landslide susceptibility assessment based on artificial neural networks, and focuses on the estimation of the prediction capability, robustness, and sensitivity of susceptibility models. The study is carried out in the Guantanamo Province of Cuba, where 186 landslides were mapped using photo-interpretation. Twelve conditioning factors were mapped including geomorphology, geology, soils, landuse, slope angle, slope direction, internal relief, drainage density, distance from roads and faults, rainfall intensity, and ground peak acceleration. A methodology was used that subdivided the database in 3 subsets. A training set was used for updating the weights. A validation set was used to stop the training procedure when the network started losing generalization capability, and a test set was used to calculate the performance of the network. A 10-fold cross-validation was performed in order to show that the results are repeatable. The prediction capability, the robustness analysis, and the sensitivity analysis were tested on 10 mutually exclusive datasets. The results show that by means of artificial neural networks it is possible to obtain models with high prediction capability and high robustness, and that an exploration of the effect of the individual variables is possible, even if they are considered as a black-box model.}
}
@article{KHALID2020104748,
title = {Advancing real-time flood prediction in large estuaries: iFLOOD a fully coupled surge-wave automated web-based guidance system},
journal = {Environmental Modelling & Software},
volume = {131},
pages = {104748},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104748},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220300931},
author = {Arslaan Khalid and Celso M. Ferreira},
keywords = {Surge wave guidance system, Flood forecasting in estuaries, ADCIRC+SWAN model, Ensemble guidance},
abstract = {Real-time flood forecasting computational frameworks that can dynamically integrate oceanic, coastal and estuarine processes are becoming essential to provide accurate and timely information for emergency response and planning in largely populated estuaries during extreme events. This study presents a newly developed real-time total water flood guidance system that is fully automated based on the coupled surge-wave (ADCIRC + SWAN) model and provides water level forecasts in the Chesapeake Bay for a lead-time of 84 h twice a day displayed on a web-based public interface. This system improved the current total water level predictions in the Bay (RMSE < 0.12 m) when compared to the existing operational forecasting systems over the period of 6 months (Jan’19-Jun’19). Furthermore, we demonstrated that a bias correction scheme and a multi-member ensemble forecast improve the overall flood prediction. Results suggests that this framework can improve our current capacity to predict total water levels in large estuaries.}
}