@article{ZHANG2003115,
title = {Predicting information technology project escalation: A neural network approach},
journal = {European Journal of Operational Research},
volume = {146},
number = {1},
pages = {115-129},
year = {2003},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(02)00294-1},
url = {https://www.sciencedirect.com/science/article/pii/S0377221702002941},
author = {G.Peter Zhang and Mark Keil and Arun Rai and Joan Mann},
keywords = {Project management, IT project escalation, Neural networks, Logistic regression, Variable selection},
abstract = {Information system (IT) projects can often spiral out of control to become runaway systems that far exceed their original budget and scheduled due date. The majority of these escalated projects are eventually abandoned or significantly redirected without delivering intended business value. Because of the strategic importance of IT projects and the large amount of resources involved in the development of IT projects, the ability to predict project escalation tendency is critical. In this study, we compare neural network and logistic regression models in building an effective early warning system to predict project escalation. Variable selection approaches are employed to identify the most important predictor variables from those derived from the project management literature and four behavioral theories. Results show that neural networks are able to predict considerably better than the traditional statistical approach––logistic regression. In addition, project management factors are found to be more critical than behavioral factors in accounting for the success of an IT project.}
}
@article{NGUYEN2020106116,
title = {A long-term prediction approach based on long short-term memory neural networks with automatic parameter optimization by Tree-structured Parzen Estimator and applied to time-series data of NPP steam generators},
journal = {Applied Soft Computing},
volume = {89},
pages = {106116},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106116},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620300569},
author = {Hoang-Phuong Nguyen and Jie Liu and Enrico Zio},
keywords = {Prognostics and health management, Time-series forecasting, Multi-step ahead prediction, Long-short term memory, Nuclear power plant prognostics, Steam generator},
abstract = {Developing an accurate and reliable multi-step ahead prediction model is a key problem in many Prognostics and Health Management (PHM) applications. Inevitably, the further one attempts to predict into the future, the harder it is to achieve an accurate and stable prediction due to increasing uncertainty and error accumulation. In this paper, we address this problem by proposing a prediction model based on Long Short-Term Memory (LSTM), a deep neural network developed for dealing with the long-term dependencies in time-series data. Our proposed prediction model also tackles two additional issues. Firstly, the hyperparameters of the proposed model are automatically tuned by a Bayesian optimization algorithm, called Tree-structured Parzen Estimator (TPE). Secondly, the proposed model allows assessing the uncertainty on the prediction. To validate the performance of the proposed model, a case study considering steam generator data acquired from different French nuclear power plants (NPPs) is carried out. Alternative prediction models are also considered for comparison purposes.}
}
@article{RAJPATHAK2020103338,
title = {An integrated framework for automatic ontology learning from unstructured repair text data for effective fault detection and isolation in automotive domain},
journal = {Computers in Industry},
volume = {123},
pages = {103338},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103338},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305728},
author = {Dnyanesh Rajpathak and Yiming Xu and Ian Gibbs},
keywords = {Ontology learning, Automotive, Supervised machine learning, Decision support},
abstract = {A real-life hierarchical classification system is developed to automatically extract a domain ontology from repair data collected during the warranty period of an original equipment manufacturer (OEM), e.g. automotive. Given the complex nature of products, failures may occur impacting consumers and to limit their adverse effect it is critical to react through effective fault detection and isolation. The key fault signals, such as failed parts, their associated symptoms, and the repair actions are latent in repair data. The overwhelming size of real-life data makes it impossible to read and curate a domain ontology manually in a reasonable time. In our work, a hierarchical classification system firstly classifies key phrases into technical and non-technical classes. Secondly, the technical phrases are classified into part, symptom, or action classes. Our system is deployed as a prototype and its performance is validated using real-life data. The system achieved the average F1 score of 0.82 and the new ontology is used in fault detection and isolation in seven different fault models.}
}
@article{WOHLIN20072,
title = {An analysis of the most cited articles in software engineering journals - 2000},
journal = {Information and Software Technology},
volume = {49},
number = {1},
pages = {2-11},
year = {2007},
note = {Most Cited Journal Articles in Software Engineering - 2000},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001133},
author = {Claes Wohlin},
keywords = {Citations, Bibliometric research, Software engineering},
abstract = {Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2000. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2000 to contribute to a special issue of Information and Software Technology. Five authors have accepted the invitation and their articles appear in this special issue. Moreover, an analysis of the most cited software engineering journal articles in the last 20 years is presented. The presentation includes both the most cited articles in absolute numbers and the most cited articles when looking at the average number of citations per year. The article describing the SPIN model checker by G.J. Holzmann published in 1997 is first on both these lists.}
}
@article{TAHMOORESI2020110724,
title = {Studying the Relationship Between the Usage of APIs Discussed in the Crowd and Post-Release Defects},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110724},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110724},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301606},
author = {Hamed Tahmooresi and Abbas Heydarnoori and Reza Nadri},
keywords = {Defect prediction, Crowd knowledge, Stack overflow, API},
abstract = {Software development nowadays is heavily based on libraries, frameworks and their proposed Application Programming Interfaces (APIs). However, due to challenges such as the complexity and the lack of documentation, these APIs may introduce various obstacles for developers and common defects in software systems. To resolve these issues, developers usually utilize Question and Answer (Q&A) websites such as Stack Overflow by asking their questions and finding proper solutions for their problems on APIs. Therefore, these websites have become inevitable sources of knowledge for developers, which is also known as the crowd knowledge. However, the relation of this knowledge to the software quality has never been adequately explored before. In this paper, we study whether using APIs which are challenging according to the discussions of the Stack Overflow is related to code quality defined in terms of post-release defects. To this purpose, we define the concept of challenge of an API, which denotes how much the API is discussed in high-quality posts on Stack Overflow. Then, using this concept, we propose a set of products and process metrics. We empirically study the statistical correlation between our metrics and post-release defects as well as added explanatory and predictive power to traditional models through a case study on five open source projects including Spring, Elastic Search, Jenkins, K-8 Mail Android Client, and OwnCloud Android client. Our findings reveal that our metrics have a positive correlation with post-release defects which is comparable to known high-performance traditional process metrics, such as code churn and number of pre-release defects. Furthermore, our proposed metrics can provide additional explanatory and predictive power for software quality when added to the models based on existing products and process metrics. Our results suggest that software developers should consider allocating more resources on reviewing and improving external API usages to prevent further defects.}
}
@article{RANA201459,
title = {Selecting software reliability growth models and improving their predictive accuracy using historical projects data},
journal = {Journal of Systems and Software},
volume = {98},
pages = {59-78},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S016412121400185X},
author = {Rakesh Rana and Miroslaw Staron and Christian Berger and Jörgen Hansson and Martin Nilsson and Fredrik Törner and Wilhelm Meding and Christoffer Höglund},
keywords = {Embedded software, Defect inflow, Software reliability growth models},
abstract = {During software development two important decisions organizations have to make are: how to allocate testing resources optimally and when the software is ready for release. SRGMs (software reliability growth models) provide empirical basis for evaluating and predicting reliability of software systems. When using SRGMs for the purpose of optimizing testing resource allocation, the model's ability to accurately predict the expected defect inflow profile is useful. For assessing release readiness, the asymptote accuracy is the most important attribute. Although more than hundred models for software reliability have been proposed and evaluated over time, there exists no clear guide on which models should be used for a given software development process or for a given industrial domain. Using defect inflow profiles from large software projects from Ericsson, Volvo Car Corporation and Saab, we evaluate commonly used SRGMs for their ability to provide empirical basis for making these decisions. We also demonstrate that using defect intensity growth rate from earlier projects increases the accuracy of the predictions. Our results show that Logistic and Gompertz models are the most accurate models; we further observe that classifying a given project based on its expected shape of defect inflow help to select the most appropriate model.}
}
@article{VERNER2014115,
title = {Factors that motivate software engineering teams: A four country empirical study},
journal = {Journal of Systems and Software},
volume = {92},
pages = {115-127},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S016412121400020X},
author = {J.M. Verner and M.A. Babar and N. Cerpa and T. Hall and S. Beecham},
keywords = {Software development team motivation, Project success, Motivational factors},
abstract = {Motivation, although difficult to quantify, is considered to be the single largest factor in developer productivity; there are also suggestions that low motivation is an important factor in software development project failure. We investigate factors that motivate software engineering teams using survey data collected from software engineering practitioners based in Australia, Chile, USA and Vietnam. We also investigate the relationship between team motivation and project outcome, identifying whether the country in which software engineering practitioners are based affects this relationship. Analysis of 333 questionnaires indicates that failed projects are associated with low team motivation. We found a set of six common team motivational factors that appear to be culturally independent (project manager has good communication with project staff, project risks reassessed, controlled and managed during the project, customer has confidence in the project manager and the development team, the working environment is good, the team works well together, and the software engineer had a pleasant experience). We also found unique groupings of team motivational factors for each of the countries investigated. This indicates that there are cultural differences that project managers need to consider when working in a global environment.}
}
@article{COSTA2015182,
title = {Practical and representative faultloads for large-scale software systems},
journal = {Journal of Systems and Software},
volume = {103},
pages = {182-197},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215000357},
author = {Pedro Costa and João Gabriel Silva and Henrique Madeira},
keywords = {Experimental dependability evaluation, Dependability benchmarking, Injection of software faults},
abstract = {The faultload is one of the most critical elements of experimental dependability evaluation. It should embody a repeatable, portable, representative and generally accepted fault set. Concerning software faults, the definition of that kind of faultloads is particularly difficult, as it requires a much more complex emulation method than the traditional stuck-at or bit-flip used for hardware faults. Although faultloads based on software faults have already been proposed, the choice of adequate fault injection targets (i.e., actual software components where the faults are injected) is still an open and crucial issue. Furthermore, knowing that the number of possible software faults that can be injected in a given system is potentially very large, the problem of defining a faultload made of a small number of representative faults is of utmost importance. This paper presents a comprehensive fault injection study and proposes a strategy to guide the fault injection target selection to reduce the number of faults required for the faultload and exemplifies the proposed approach with a real web-server dependability benchmark and a large-scale integer vector sort application.}
}
@article{CASTORFILHO20061397,
title = {Specification of exception flow in software architectures},
journal = {Journal of Systems and Software},
volume = {79},
number = {10},
pages = {1397-1418},
year = {2006},
note = {Architecting Dependable Systems},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.02.060},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206001373},
author = {Fernando {Castor Filho} and Patrick Henrique da S. Brito and Cecília Mary F. Rubira},
keywords = {Exception handling, Software architecture, Architecture description languages, Verification, Alloy},
abstract = {In recent years, various approaches combining software architectures and exception handling have been proposed for increasing the dependability of software systems. This conforms with the idea supported by some authors that addressing exception handling-related issues since early phases of software development may improve the overall dependability of a system. By systematically designing the mechanism responsible for rendering a system reliable, developers increase the probability of the system being able to avoid catastrophic failures at runtime. This paper addresses the problem of describing how exceptions flow amongst architectural elements. In critical systems where rollback-based mechanisms might not be available, such as systems that interact with mechanical devices, exception handling is an important means for recovering from errors in a forward-based manner. We present a framework, named Aereal, that supports the description and analysis of exceptions that flow between architectural components. Since different architectural styles have different policies for exception flow, Aereal makes it possible to specify rules on how exceptions flow in a given style and to check for violations of these rules. We use a financial application and a control system as case studies to validate the proposed approach.}
}
@article{MUHAMMAD201883,
title = {Impact of earthquake source complexity and land elevation data resolution on tsunami hazard assessment and fatality estimation},
journal = {Computers & Geosciences},
volume = {112},
pages = {83-100},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417306052},
author = {Ario Muhammad and Katsuichiro Goda},
keywords = {DEM resolution, Earthquake source complexity, Tsunami hazard, Tsunami fatality, Big data},
abstract = {This study investigates the impact of model complexity in source characterization and digital elevation model (DEM) resolution on the accuracy of tsunami hazard assessment and fatality estimation through a case study in Padang, Indonesia. Two types of earthquake source models, i.e. complex and uniform slip models, are adopted by considering three resolutions of DEMs, i.e. 150 m, 50 m, and 10 m. For each of the three grid resolutions, 300 complex source models are generated using new statistical prediction models of earthquake source parameters developed from extensive finite-fault models of past subduction earthquakes, whilst 100 uniform slip models are constructed with variable fault geometry without slip heterogeneity. The results highlight that significant changes to tsunami hazard and fatality estimates are observed with regard to earthquake source complexity and grid resolution. Coarse resolution (i.e. 150 m) leads to inaccurate tsunami hazard prediction and fatality estimation, whilst 50-m and 10-m resolutions produce similar results. However, velocity and momentum flux are sensitive to the grid resolution and hence, at least 10-m grid resolution needs to be implemented when considering flow-based parameters for tsunami hazard and risk assessments. In addition, the results indicate that the tsunami hazard parameters and fatality number are more sensitive to the complexity of earthquake source characterization than the grid resolution. Thus, the uniform models are not recommended for probabilistic tsunami hazard and risk assessments. Finally, the findings confirm that uncertainties of tsunami hazard level and fatality in terms of depth, velocity and momentum flux can be captured and visualized through the complex source modeling approach. From tsunami risk management perspectives, this indeed creates big data, which are useful for making effective and robust decisions.}
}
@article{PIETRANTUONO2020110462,
title = {On the testing resource allocation problem: Research trends and perspectives},
journal = {Journal of Systems and Software},
volume = {161},
pages = {110462},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110462},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219302365},
author = {Roberto Pietrantuono},
keywords = {Testing, Resource allocation, Reliability allocation, Literature review, Test planning, Survey},
abstract = {In testing a software application, a primary concern is how to effectively plan the assignment of resources available for testing to the software components so as to achieve a target goal under given constraints. In the literature, this is known as testing resources allocation problem (TRAP). Researchers spent a lot of effort to propose models for supporting test engineers in this task, and a variety of solutions exist to assess the best trade-off between testing time, cost and quality of delivered products. This article presents a systematic mapping study aimed at systematically exploring the TRAP research area in order to provide an overview on the type of research performed and on results currently available. A sample of 68 selected studies has been classified and analyzed according to defined dimensions. Results give an overview of the state of the art, provide guidance to improve practicability and allow outlining a set of directions for future research and applications of TRAP solutions.}
}
@article{PAREEK201350,
title = {Study of effect of seismic displacements on landslide susceptibility zonation (LSZ) in Garhwal Himalayan region of India using GIS and remote sensing techniques},
journal = {Computers & Geosciences},
volume = {61},
pages = {50-63},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413002148},
author = {Naveen Pareek and Shilpa Pal and Mukat L. Sharma and Manoj K. Arora},
keywords = {Chamoli earthquake, Landslide, Seismic displacement},
abstract = {Landslides are the most damaging and threatening aftereffect of seismic events in Garhwal Himalayas. It is evident from past seismic events in Uttarakhand, India that no other phenomena can produce landslides of so great in size and number as a single seismic event can produce. Landslide inventories are produced for the study area before and after the occurrence of Chamoli Earthquake using Panchromatic (PAN) sharpened Linear Imaging Self Scanning-III (LISS-III) images. A sudden increase in number of landslides after the earthquake is observed. Further, two Landslide Susceptibility Zonation (LSZ) maps have been derived using pre- and post-Chamoli Earthquake landslide inventories. The difference of two LSZ indicates that landslides are very complex phenomenon and are affected by static factors in seismic conditions also. An attempt has been made to estimate the seismic displacements using Differential Synthetic Aperture Radar Interferometry (DIn SAR). European Remote Sensing Satellite-1/2 (ERS-1/ 2) SAR images have been used for preparing differential interferogram. Geometric and temporal decorrelation in SAR images is very high in the study area, which limits the use of DInSAR for displacement estimation. Theoretical displacement has been estimated using fault displacement modeling parameters for Chamoli earthquake. Post-Chamoli earthquake landslide inventory is overlaid over displacement map for understanding the impact of seismic displacement pattern with other static factors on the occurrence of landslides. It is observed that distribution and size of landslides is affected by displacement pattern controlled by other static factors also.}
}
@article{LIU2004333,
title = {Pre- and co-seismic ground deformations of the 1999 Chi-Chi, Taiwan earthquake, measured with SAR interferometry},
journal = {Computers & Geosciences},
volume = {30},
number = {4},
pages = {333-343},
year = {2004},
note = {Multidimensional geospatial technology for the geosciences},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2003.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0098300404000135},
author = {G.X. Liu and X.L. Ding and Z.L. Li and Z.W. Li and Y.Q. Chen and S.B. Yu},
keywords = {Chi-Chi earthquake, SAR interferometry, Displacement field, Fault model, GPS},
abstract = {The 1999 Mw=7.6 Chi-Chi earthquake was the strongest inland earthquake in Taiwan in the 20th century. Five radar images acquired with the C-band SARs onboard the ERS-1/2 satellites are combined to study the pre- and co-seismic surface deformations in the epicentral area of about 1500km2. The pre-seismic interferograms over 2–3 years show consistent fringe patterns that are equivalent to LOS displacement variations of up to about 32mm. The deformations are likely caused by the east-west tectonic compression in the region. The short-term co-seismic interferograms show clear arc-shaped fringe patterns of about 10 fringes, equivalent to displacement variations of about 28.3cm. The co-seismic deformation results fit well with both GPS measurements and a simulated interferogram computed based on a fault-dislocation model. This study demonstrates the capability of short-wavelength InSAR systems for monitoring ground deformations of flat terrain in tropical regions.}
}
@article{WOO201150,
title = {Modeling vulnerability discovery process in Apache and IIS HTTP servers},
journal = {Computers & Security},
volume = {30},
number = {1},
pages = {50-62},
year = {2011},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2010.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167404810000908},
author = {Sung-Whan Woo and HyunChul Joh and Omar H. Alhazmi and Yashwant K. Malaiya},
keywords = {Vulnerability discovery model (VDM), Risk evaluation, Web server, Quantitative modeling, Security},
abstract = {Vulnerability discovery models allow prediction of the number of vulnerabilities that are likely to be discovered in the future. Hence, they allow the vendors and the end users to manage risk by optimizing resource allocation. Most vulnerability discovery models proposed use the time as an independent variable. Effort-based modeling has also been proposed, which requires the use of market share data. Here, the feasibility of characterizing the vulnerability discovery process in the two major HTTP servers, Apache and IIS, is quantitatively examined using both time and effort-based vulnerability discovery models, using data spanning more than a decade. The data used incorporates the effect of software evolution for both servers. In addition to aggregate vulnerabilities, different groups of vulnerabilities classified using both the error types and severity levels are also examined. Results show that the selected vulnerability discovery models of both types can fit the data of the two HTTP servers very well. Results also suggest that separate modeling for an individual class of vulnerabilities can be done. In addition to the model fitting, predictive capabilities of the two models are also examined. The results demonstrate the applicability of quantitative methods to widely-used products, which have undergone evolution.}
}
@article{ABUGHARBIEH201626,
title = {Line-impedance matching and signal conditioning capabilities for high-speed feed-forward voltage-mode transmit drivers},
journal = {Microelectronics Journal},
volume = {55},
pages = {26-39},
year = {2016},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2016.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0026269216301914},
author = {Khaldoon Abugharbieh and Abraham Balabanyan and Armen Durgaryan and Vazgen Melikyan},
keywords = {28nm CMOS, Driver, Feed-forward, Impedance matching, PVT compensation, Signal conditioning, Voltage mode},
abstract = {This work presents the design and implementation of a power-efficient 2-tap feed-forward voltage mode driver which has impedance tuning and signal conditioning capabilities. The driver has a robust mechanism to match its impedance to the line impedance even when signal conditioning is enabled which minimizes reflection and improves signal quality. A mixed signal approach that detects and compensates for NMOS and PMOS transistor resistance variation is presented. An analog circuit detects the driver's resistance variation, and a digital circuit controls an analog compensation circuit to maintain line impedance matching for all signal conditioning configurations of the driver. When maximum signal conditioning is enabled, the driver can transmit a 40Gbits/sec PRBS7 signal through a 10 in. FR4 channel. It achieves a differential eye-opening amplitude of 100mVppd and an eye-opening width of 0.8UI consuming 9.7mW of at-speed power. Simulations demonstrate that worst case impedance deviation from 50Ω due to process, voltage and temperature variation is 2.7%. The driver is designed in 28nm CMOS process using a 0.85V nominal supply voltage. It is simulated using HSPICE circuit simulator and mixed mode simulations tools.}
}
@article{PALOMBA2018143,
title = {Crowdsourcing user reviews to support the evolution of mobile apps},
journal = {Journal of Systems and Software},
volume = {137},
pages = {143-162},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.043},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302807},
author = {Fabio Palomba and Mario Linares-Vásquez and Gabriele Bavota and Rocco Oliveto and Massimiliano Di Penta and Denys Poshyvanyk and Andrea De Lucia},
keywords = {Mobile app evolution, User reviews, Mining app stores, Empirical study},
abstract = {In recent software development and distribution scenarios, app stores are playing a major role, especially for mobile apps. On one hand, app stores allow continuous releases of app updates. On the other hand, they have become the premier point of interaction between app providers and users. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we empirically investigate—by performing a study on the evolution of 100 open source Android apps and by surveying 73 developers—to what extent app developers take user reviews into account, and whether addressing them contributes to apps’ success in terms of ratings. In order to perform the study, as well as to provide a monitoring mechanism for developers and project managers, we devised an approach, named CRISTAL, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results of our study indicate that (i) on average, half of the informative reviews are addressed, and over 75% of the interviewed developers claimed to take them into account often or very often, and that (ii) developers implementing user reviews are rewarded in terms of significantly increased user ratings.}
}
@incollection{RICO20081,
title = {History of Computers, Electronic Commerce and Agile Methods},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {73},
pages = {1-55},
year = {2008},
booktitle = {Advances in COMPUTERS},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)00401-4},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808004014},
author = {David F. Rico and Hasan H. Sayani and Ralph F. Field},
abstract = {Abstract
The purpose of this chapter is to present a literature review relevant to a study of using agile methods to manage the development of Internet websites and their subsequent quality. This chapter places website quality within the context of the $2.4 trillion U.S. electronic commerce industry. Thus, this chapter provides a history of electronic computers, electronic commerce, software methods, software quality metrics, agile methods and studies on agile methods. None of these histories are without controversy. For instance, some scholars begin the study of the electronic computer by mentioning the emergence of the Sumerian text, Hammurabi code or the abacus. We, however, will align our history with the emergence of the modern electronic computer at the beginning of World War II. The history of electronic commerce also has poorly defined beginnings. Some studies of electronic commerce begin with the widespread use of the Internet in the early 1990s. However, electronic commerce cannot be appreciated without establishing a deeper context. Few scholarly studies, if any, have been performed on agile methods, which is the basic purpose of this literature review. That is, to establish the context to conduct scholarly research within the fields of agile methods and electronic commerce.}
}
@article{SHEPHERD2021102471,
title = {Physical fault injection and side-channel attacks on mobile devices: A comprehensive analysis},
journal = {Computers & Security},
volume = {111},
pages = {102471},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002959},
author = {Carlton Shepherd and Konstantinos Markantonakis and Nico {van Heijningen} and Driss Aboulkassimi and Clément Gaine and Thibaut Heckmann and David Naccache},
keywords = {Fault injection attacks, Side channel attacks, System-on-Chips (socs), Mobile device security, Embedded systems security},
abstract = {Today’s mobile devices contain densely packaged system-on-chips (SoCs) with multi-core, high-frequency CPUs and complex pipelines. In parallel, sophisticated SoC-assisted security mechanisms have become commonplace for protecting device data, such as trusted execution environments, full-disk and file-based encryption. Both advancements have dramatically complicated the use of conventional physical attacks, requiring the development of specialised attacks. In this survey, we consolidate recent developments in physical fault injections and side-channel attacks on modern mobile devices. In total, we comprehensively survey over 50 fault injection and side-channel attack papers published between 2009–2021. We evaluate the prevailing methods, compare existing attacks using a common set of criteria, identify several challenges and shortcomings, and suggest future directions of research.}
}
@article{ARORA2006221,
title = {Resettable vector clocks},
journal = {Journal of Parallel and Distributed Computing},
volume = {66},
number = {2},
pages = {221-237},
year = {2006},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2005.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731505001759},
author = {Anish Arora and Sandeep S. Kulkarni and Murat Demirbas},
keywords = {Vector clocks, Reset, Component substitutability, Bounded-space, Fault-tolerance, Self-stabilization},
abstract = {Vector clocks (VC) are an inherent component of a rich class of distributed applications. In this paper, we first consider the problem of realistic implementation—more specifically, bounded-space and fault-tolerant—of applications that use vector clocks (VC). To this end, we generalize the notion of VC to resettable vector clocks (RVC), and provide a realistic implementation of RVC. Further, we identify an interface contract under which our RVC implementation can be substituted for VC in client applications, without affecting the client's correctness. This interface contract is designed for phase-based applications that provide certain communication guarantees. Based on such substitution and the use of the interface contract, we show how to transform the client so that it is itself realistically implemented. We illustrate our method in the context of Ricart–Agrawala's mutual exclusion program and Garg–Chase's predicate detection program.}
}
@article{PRECUP201575,
title = {An overview on fault diagnosis and nature-inspired optimal control of industrial process applications},
journal = {Computers in Industry},
volume = {74},
pages = {75-94},
year = {2015},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2015.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361515000469},
author = {Radu-Emil Precup and Plamen Angelov and Bruno Sielly Jales Costa and Moamar Sayed-Mouchaweh},
keywords = {Data-driven control, Data mining, Evolving soft computing techniques, Fault diagnosis, Nature-inspired optimization algorithms, Wind turbines},
abstract = {Fault detection, isolation and optimal control have long been applied to industry. These techniques have proven various successful theoretical results and industrial applications. Fault diagnosis is considered as the merge of fault detection (that indicates if there is a fault) and fault isolation (that determines where the fault is), and it has important effects on the operation of complex dynamical systems specific to modern industry applications such as industrial electronics, business management systems, energy, and public sectors. Since the resources are always limited in real-world industrial applications, the solutions to optimally use them under various constraints are of high actuality. In this context, the optimal tuning of linear and nonlinear controllers is a systematic way to meet the performance specifications expressed as optimization problems that target the minimization of integral- or sum-type objective functions, where the tuning parameters of the controllers are the vector variables of the objective functions. The nature-inspired optimization algorithms give efficient solutions to such optimization problems. This paper presents an overview on recent developments in machine learning, data mining and evolving soft computing techniques for fault diagnosis and on nature-inspired optimal control. The generic theory is discussed along with illustrative industrial process applications that include a real liquid level control application, wind turbines and a nonlinear servo system. New research challenges with strong industrial impact are highlighted.}
}
@article{HUANG2017226,
title = {Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study},
journal = {Journal of Systems and Software},
volume = {132},
pages = {226-252},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301516},
author = {Jianglin Huang and Jacky Wai Keung and Federica Sarro and Yan-Fu Li and Y.T. Yu and W.K. Chan and Hongyi Sun},
keywords = {Empirical software engineering estimation, , Imputation, Cross-validation, Missing data},
abstract = {Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation.}
}
@article{JAVEDANISADAEI2016782,
title = {Combining ARFIMA models and fuzzy time series for the forecast of long memory time series},
journal = {Neurocomputing},
volume = {175},
pages = {782-796},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.10.079},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215015386},
author = {Hossein {Javedani Sadaei} and Rasul Enayatifar and Frederico Gadelha Guimarães and Maqsood Mahmud and Zakarya A. Alzamil},
keywords = {ARFIMA, Fuzzy time series, Long memory time series, Forecasting, Stock market, Exchange rate},
abstract = {Long memory time series are stationary processes in which there is a statistical long range dependency between the current value and values in different times of the series. Therefore, in this class of series, there is a slow decay of the autocorrelation function as the time difference increases. Many practical forecasting problems fall in this class, for instance, in financial time series, hydrology and earth sciences applications. This research introduces a hybrid method combining Auto Regressive Fractional Integrated Moving Average (ARFIMA) models and Fuzzy Time Series (FTS) for the forecast of long memory (long-range) time series. The proposed method is developed as one algorithm consisting of two phases. The first phase is related to the autoregressive part of the model, while the second phase is related to the Moving Average part. Based on these ideas, the combined ARFIMA and FTS model is introduced. For the parameter estimation of the model, Particle Swarm Optimization (PSO) method is selected, based on its performance on similar optimization problems. In order to illustrate the benefit and potential of the proposed ARFIMA–FTS method, it has been applied to the two stock index databases, namely Taiwan Capitalization Weighted Stock Index (TAIEX)and Dow Jones Industrial Average (DJIA), together with exchange rate data of nine main currencies versus USD. Based on the reported results, it is possible to conclude the superiority of the proposed hybrid method, compared with classical ARFIMA models and other methods in the literature.}
}
@article{BRINGS20201010,
title = {A systematic map on verification and validation of emergent behavior in software engineering research},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {1010-1037},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.049},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19333606},
author = {Jennifer Brings and Marian Daun and Kevin Keller and Patricia {Aluko Obe} and Thorsten Weyer},
keywords = {Emergent behavior, Verification and validation, Systematic mapping study},
abstract = {Context:
Emergent behavior cannot be attributed to one individual system alone but arises in the interplay of various systems, components etc. Ensuring the correctness of emergent behavior is a well-known challenge that has been addressed by research in various subfields of software engineering.
Objective:
This paper aims at providing a unified view on the research activities conducted and research contributions made on verification and validation of emergent behavior.
Methods:
We have conducted a systematic mapping study on the topic of verification and validation of emergent behavior. We applied a combined search strategy using manual, database, and snowball search. In total we investigated 7211 papers, from these 168 relevant papers have been included and classified.
Results:
Results show an increasing interest in the topic of verification and validation of emergent behavior. As only little validation and evaluation research has been conducted, the field can be considered still immature. There exist different verification and validation techniques used in the various solution approaches such as model checking, simulation, or runtime monitoring. It stands out that even though research is published in different software engineering fields and subfields no verification or validation technique can be attributed solely to a single field.}
}
@article{DURELLI2013934,
title = {A scoping study on the 25 years of research into software testing in Brazil and an outlook on the future of the area},
journal = {Journal of Systems and Software},
volume = {86},
number = {4},
pages = {934-950},
year = {2013},
note = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S016412121200283X},
author = {Vinicius Humberto Serapilha Durelli and Rodrigo Fraxino Araujo and Marco Aurelio Graciotto Silva and Rafael Alves Paes de Oliveira and Jose Carlos Maldonado and Marcio Eduardo Delamaro},
keywords = {Software testing, Systematic mapping, Brazilian research},
abstract = {Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we have synthesized its 25-year history of research on software testing. Using information drawn from this overview we highlighted which software testing topics have been the most extensively surveyed in SBES literature. We have also devised a co-authorship network to depict the most prolific research groups and researchers. Moreover, by performing a citation analysis of the selected studies we have tried to ascertain the importance of SBES in a wider scenario. Finally, using the information extracted from the studies, we have shed light on the state-of-the-art of software testing in Brazil and provided an outlook on its foreseeable future.}
}
@article{BOUFFARD201533,
title = {The ultimate control flow transfer in a Java based smart card},
journal = {Computers & Security},
volume = {50},
pages = {33-46},
year = {2015},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S016740481500005X},
author = {Guillaume Bouffard and Jean-Louis Lanet},
keywords = {Java Card security, Control flow transfer, Countermeasures, Evaluation, Fault tree analysis, Smart card, Logical attack},
abstract = {Recently, researchers published several attacks on smart cards. Among these, software attacks are the most affordable, they do not require specific hardware (laser, EM probe, etc.). Such attacks succeed to modify a sensitive system element which offers access to the smart card assets. To prevent that, smart card manufacturers embed dedicated countermeasures that aim to protect the sensitive system elements. We present a generic approach based on a Control Flow Transfer (CFT) attack to modify the Java Card program counter. This attack is built on a type confusion using the couple of instructions jsr/ret. Evaluated on different Java Cards, this new attack is a generic CFT exploitation that succeeds on each attacked cards. We present several countermeasures proposed by the literature or implemented by smart card designers and for all of them we explain how to bypass them. Then, we propose to use Attack Countermeasure Tree to develop an effective and affordable countermeasure for this attack.}
}
@article{DING20211,
title = {Flexible scheme for reconfiguring 2D mesh-connected VLSI subarrays under row and column rerouting},
journal = {Journal of Parallel and Distributed Computing},
volume = {151},
pages = {1-12},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521000034},
author = {Hao Ding and Junyan Qian and Bisheng Huang and Lingzhong Zhao and Zhongyi Zhai},
keywords = {Fault tolerance, Reconfiguration, Degradable VLSI array, Algorithm, Rerouting scheme},
abstract = {In the mesh-connected processors, some processor elements (PEs) become ineffective due to high temperature, overload and other factors, which can affect the stability of the system. This paper deals with the problem of reconfiguring the largest possible subarray from the processor with faults under the row and column rerouting constraint. Firstly, a flexible routing scheme, based on dynamic programming, is proposed to construct the local optimal logical columns. Secondly, we discuss and revise the PEs that cannot be connected between every two logical columns under this scheme. Finally, an efficient algorithm is presented to construct the maximum subarray in polynomial time. The experimental results show that, both on the random and clustered fault scenarios, the proposed algorithm under flexible rerouting scheme is capable of constructing the larger scale logical arrays. On a 48 × 48 host array with 15% fault density, the improvement on the use of fault-free PEs is up to 6.22% for random faults. On a 256 × 256 host array, the improvement can be up to 85.60% for clustered faults. Moreover, the proposed algorithm runs faster than previous algorithms under different size arrays and fault densities, the average improvement in running time is up to 99% compared with state-of-the-art.}
}
@article{KULESZA2013905,
title = {The crosscutting impact of the AOSD Brazilian research community},
journal = {Journal of Systems and Software},
volume = {86},
number = {4},
pages = {905-933},
year = {2013},
note = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212002427},
author = {Uirá Kulesza and Sérgio Soares and Christina Chavez and Fernando Castor and Paulo Borba and Carlos Lucena and Paulo Masiero and Claudio Sant’Anna and Fabiano Ferrari and Vander Alves and Roberta Coelho and Eduardo Figueiredo and Paulo F. Pires and Flávia Delicato and Eduardo Piveta and Carla Silva and Valter Camargo and Rosana Braga and Julio Leite and Otávio Lemos and Nabor Mendonça and Thais Batista and Rodrigo Bonifácio and Nélio Cacho and Lyrene Silva and Arndt {von Staa} and Fábio Silveira and Marco Túlio Valente and Fernanda Alencar and Jaelson Castro and Ricardo Ramos and Rosangela Penteado and Cecília Rubira},
keywords = {Aspect-Oriented Software Development, Modularity, Research impact},
abstract = {Background
Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil.
Aims
In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community.
Method
Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations.
Results
Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.}
}
@article{GAROUSI2016108,
title = {Highly-cited papers in software engineering: The top-100},
journal = {Information and Software Technology},
volume = {71},
pages = {108-128},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001871},
author = {Vahid Garousi and João M. Fernandes},
keywords = {Software engineering, Highly-cited papers, Top cited, Most cited, Most frequently cited, Bibliometrics},
abstract = {Context
According to the search reported in this paper, as of this writing (May 2015), a very large number of papers (more than 70,000) have been published in the area of Software Engineering (SE) since its inception in 1968. Citations are crucial in any research area to position the work and to build on the work of others. Identification and characterization of highly-cited papers are common and are regularly reported in various disciplines.
Objective
The objective of this study is to identify the papers in the area of SE that have influenced others the most as measured by citation count. Studying highly-cited SE papers helps researchers to see the type of approaches and research methods presented and applied in such papers, so as to be able to learn from them to write higher quality papers which will likely receive high citations.
Method
To achieve the above objective, we conducted a study, comprised of five research questions, to identify and classify the top-100 highly-cited SE papers in terms of two metrics: total number of citations and average annual number of citations.
Results
By total number of citations, the top paper is "A metrics suite for object-oriented design", cited 1817 times and published in 1994. By average annual number of citations, the top paper is "QoS-aware middleware for Web services composition", cited 154.2 times on average annually and published in 2004.
Conclusion
It is concluded that it is important to identify the highly-cited SE papers and also to characterize the overall citation landscape in the SE field. We hope that this paper will encourage further discussions in the SE community towards further analysis and formal characterization of the highly-cited SE papers.}
}
@article{NUNEZVARELA2017164,
title = {Source code metrics: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {128},
pages = {164-197},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.044},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300663},
author = {Alberto S. Nuñez-Varela and Héctor G. Pérez-Gonzalez and Francisco E. Martínez-Perez and Carlos Soubervielle-Montalvo},
keywords = {Source code metrics, Software metrics, Object-oriented metrics, Aspect-oriented metrics, Feature-oriented metrics, Systematic mapping study},
abstract = {Context
Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics.
Objectives
This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends.
Method
A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed.
Results
Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines.
Conclusions
Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.}
}
@article{LEMOS2013951,
title = {Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences},
journal = {Journal of Systems and Software},
volume = {86},
number = {4},
pages = {951-969},
year = {2013},
note = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212003238},
author = {Otávio Augusto Lazzarini Lemos and Fabiano Cutigi Ferrari and Marcelo Medeiros Eler and José Carlos Maldonado and Paulo Cesar Masiero},
keywords = {Software testing, Evaluation studies, Software testing research in Brazil},
abstract = {This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in SBES has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20% performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40% of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation.}
}
@article{DEMAGALHAES201576,
title = {Investigations about replication of empirical studies in software engineering: A systematic mapping study},
journal = {Information and Software Technology},
volume = {64},
pages = {76-101},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000300},
author = {Cleyton V.C. {de Magalhães} and Fabio Q.B. {da Silva} and Ronnie E.S. Santos and Marcos Suassuna},
keywords = {Replications, Experiments, Empirical studies, Mapping study, Systematic literature review, Software engineering},
abstract = {Context
Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
Objective
In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.
Method
We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013.
Results
We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.
Conclusions
Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.}
}
@article{HOLL2012828,
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
journal = {Information and Software Technology},
volume = {54},
number = {8},
pages = {828-852},
year = {2012},
note = {Special Issue: Voice of the Editorial Board},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S095058491200033X},
author = {Gerald Holl and Paul Grünbacher and Rick Rabiser},
keywords = {Product line engineering, Large-scale systems, Multi product lines, Systematic literature review},
abstract = {Context
Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented.
Objective
The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs.
Method
Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry.
Results
The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized.
Conclusions
We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.}
}
@article{ILLESSEIFERT2010539,
title = {Exploring the relationship of a file’s history and its fault-proneness: An empirical method and its application to open source programs},
journal = {Information and Software Technology},
volume = {52},
number = {5},
pages = {539-558},
year = {2010},
note = {TAIC-PART 2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909002109},
author = {Timea Illes-Seifert and Barbara Paech},
keywords = {Empirical study, Software testing, Software history/evolution},
abstract = {Context
The knowledge about particular characteristics of software that are indicators for defects is very valuable for testers because it helps them to focus the testing effort and to allocate their limited resources appropriately.
Objective
In this paper, we explore the relationship between several historical characteristics of files and their defect count.
Method
For this purpose, we propose an empirical approach that uses statistical procedures and visual representations of the data in order to determine indicators for a file’s defect count. We apply this approach to nine open source Java projects across different versions.
Results
Only 4 of 9 programs show moderate correlations between a file’s defects in previous and in current releases in more than half of the analysed releases. In contrast to our expectations, the oldest files represent the most fault-prone files. Additionally, late changes correlate with a file’s defect count only partly. The number of changes, the number of distinct authors performing changes to a file as well as the file’s age are good indicators for a file’s defect count in all projects.
Conclusion
Our results show that a software’s history is a good indicator for ist quality. We did not find one indicator that persists across all projects in an equal manner. Nevertheless, there are several indicators that show significant strong correlations in nearly all projects: DA (number of distinct authors) and FC (frequency of change). In practice, for each software, statistical analyses have to be performed in order to evaluate the best indicator(s) for a file’s defect count.}
}
@article{PAUN2016146,
title = {A failure index for HPC applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {93-94},
pages = {146-153},
year = {2016},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2016.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0743731516300156},
author = {Andrei Păun and Clayton Chandler and Chokchai Box Leangsuksun and Mihaela Păun},
keywords = {Failure Index (FI), Inequality measures, High Performance Computing, Resilience, System volatility, Adequate level of performance},
abstract = {This paper conducts an examination of log files originating from High Performance Computing (HPC) applications with known reliability problems. The results of this study further the maturation and adoption of meaningful metrics representing HPC system and application failure characteristics. Quantifiable metrics representing the reliability of HPC applications are foundational for building an application resilience methodology critical in the realization of exascale supercomputing. In this examination, statistical inequality methods originating from the study of economics are applied to health and status information contained in HPC application log files. The main result is the derivation of a new failure index metric for HPC—a normalized representation of parallel application volatility and/or resiliency to complement existing reliability metrics such as mean time between failure (MTBF), which aims for a better presentation of HPC application resilience. This paper provides an introduction to a Failure Index (FI) for HPC reliability and takes the reader through a use-case wherein the FI is used to expose various run-time fluctuations in the failure rate of applications running on a collection of HPC platforms.}
}
@article{FENTON1999149,
title = {Software metrics: successes, failures and new directions},
journal = {Journal of Systems and Software},
volume = {47},
number = {2},
pages = {149-157},
year = {1999},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(99)00035-7},
url = {https://www.sciencedirect.com/science/article/pii/S0164121299000357},
author = {Norman E Fenton and Martin Neil},
abstract = {The history of software metrics is almost as old as the history of software engineering. Yet, the extensive research and literature on the subject has had little impact on industrial practice. This is worrying given that the major rationale for using metrics is to improve the software engineering decision making process from a managerial and technical perspective. Industrial metrics activity is invariably based around metrics that have been around for nearly 30 years (notably Lines of Code or similar size counts, and defects counts). While such metrics can be considered as massively successful given their popularity, their limitations are well known, and mis-applications are still common. The major problem is in using such metrics in isolation. We argue that it is possible to provide genuinely improved management decision support systems based on such simplistic metrics, but only by adopting a less isolationist approach. Specifically, we feel it is important to explicitly model: (a) cause and effect relationships and (b) uncertainty and combination of evidence. Our approach uses Bayesian Belief nets, which are increasingly seen as the best means of handling decision-making under uncertainty. The approach is already having an impact in Europe.}
}
@article{TOMASZEWSKI20071227,
title = {Statistical models vs. expert estimation for fault prediction in modified code – an industrial case study},
journal = {Journal of Systems and Software},
volume = {80},
number = {8},
pages = {1227-1238},
year = {2007},
note = {The Impact of Barry Boehm’s Work on Software Engineering Education and Training},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.12.548},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206003566},
author = {Piotr Tomaszewski and Jim Håkansson and Håkan Grahn and Lars Lundberg},
keywords = {Fault prediction, Expert estimation, Evaluation},
abstract = {Statistical fault prediction models and expert estimations are two popular methods for deciding where to focus the fault detection efforts when the fault detection budget is limited. In this paper, we present a study in which we empirically compare the accuracy of fault prediction offered by statistical prediction models with the accuracy of expert estimations. The study is performed in an industrial setting. We invited eleven experts that are involved in the development of two large telecommunication systems. Our statistical prediction models are built on historical data describing one release of one of those systems. We compare the performance of these statistical fault prediction models with the performance of our experts when predicting faults in the latest releases of both systems. We show that the statistical methods clearly outperform the expert estimations. As the main reason for the superiority of the statistical models we see their ability to cope with large datasets. This makes it possible for statistical models to perform reliable predictions for all components in the system. This also enables prediction at a more fine-grain level, e.g., at the class instead of at the component level. We show that such a prediction is better both from the theoretical and from the practical perspective.}
}
@article{XU201959,
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
journal = {Journal of Systems and Software},
volume = {154},
pages = {59-78},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300627},
author = {Zhou Xu and Shuai Li and Xiapu Luo and Jin Liu and Tao Zhang and Yutian Tang and Jun Xu and Peipei Yuan and Jacky Keung},
keywords = {Cross version defect prediction, Spare modeling, Training subset selection, Weighted extreme learning machine},
abstract = {Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules in the current version. Unfortunately, the differences of data distribution across versions may hinder the effectiveness of the trained CVDP model. Thus, it is not trivial to select a suitable training subset from the prior version to promote the CVDP performance. In this paper, we propose a novel method, called Two-Stage Training Subset Selection (TSTSS), to address this challenging issue. In the first stage, TSTSS utilizes a sparse modeling representative selection method to select an initial module subset from the prior version which can well reconstruct the data of the prior version. In the second stage, TSTSS leverages a dissimilarity-based sparse subset selection method to further refine the selected module subset, which enables the selected modules to well represent the modules of the current version. Finally, we use a novel weighted extreme learning machine classifier to construct the CVDP model. We evaluate the CVDP performance of TSTSS on 50 cross-version pairs using 6 indicators. The experiments show that TSTSS can efficiently improve the CVDP performance compared with 11 baseline methods.}
}
@article{SCHAAF2004253,
title = {Intelligent IP retrieval driven by application requirements},
journal = {Integration},
volume = {37},
number = {4},
pages = {253-287},
year = {2004},
note = {IP and Design Reuse},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2004.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167926004000021},
author = {Martin Schaaf and Andrea Freßmann and Rainer Maximini and Ralph Bergmann and Alexander Tartakovski and Martin Radetzki},
keywords = {IP retrieval, Case-based reasoning, Virtual marketplaces, Parametrized IPs},
abstract = {For the design of Systems on Chip it is essential to reuse previously developed and verified components, so-called Intellectual Properties (IPs), in order to meet nowadays requirements in reliability, correctness, and time-to-market. On the downside, deciding about reusing a third-party component in a design situation can consume substantial time and resources. This is especially true in situations where many potential candidates exist due to the large amount of functional, non-functional, and quality related aspects of each component. In order to facilitate the search for IPs in large IP assets, e.g. virtual marketplaces, we have developed the IP Broker tool suite that facilitates the development of IP retrieval solutions by making use of Case-based Reasoning technology. It enables integrators to search for IPs driven by their specific requirements. Applications based on IP Broker can be easily integrated into already existing company sites or portal sites.}
}
@article{CAI200147,
title = {On the neural network approach in software reliability modeling},
journal = {Journal of Systems and Software},
volume = {58},
number = {1},
pages = {47-62},
year = {2001},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(01)00027-9},
url = {https://www.sciencedirect.com/science/article/pii/S0164121201000279},
author = {Kai-Yuan Cai and Lin Cai and Wei-Dong Wang and Zhou-Yi Yu and David Zhang},
keywords = {Software reliability modeling, Neural network, Network architecture, Scaling function, Filtering, Empirical probability density distribution, Software operational profile},
abstract = {Previous studies have shown that the neural network approach can be applied to identify defect-prone modules and predict the cumulative number of observed software failures. In this study we examine the effectiveness of the neural network approach in handling dynamic software reliability data overall and present several new findings. Specifically, we find 1.The neural network approach is more appropriate for handling datasets with `smooth' trends than for handling datasets with large fluctuations.2.The training results are much better than the prediction results in general.3.The empirical probability density distribution of predicting data resembles that of training data. A neural network can qualitatively predict what it has learned.4.Due to the essential problems associated with the neural network approach and software reliability data, more often than not, the neural network approach fails to generate satisfactory quantitative results.}
}
@article{VALINATAJ2011630,
title = {A reconfigurable and adaptive routing method for fault-tolerant mesh-based networks-on-chip},
journal = {AEU - International Journal of Electronics and Communications},
volume = {65},
number = {7},
pages = {630-640},
year = {2011},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1434841110002141},
author = {Mojtaba Valinataj and Siamak Mohammadi and Juha Plosila and Pasi Liljeberg and Hannu Tenhunen},
keywords = {Network-on-Chip, Fault tolerance, Routing algorithm, Reconfiguration, Congestion},
abstract = {High reliability against undesirable effects is one of the key objectives in the design of on-chip networks. This paper presents a very low cost fault-tolerant routing method to tolerate faulty links and routers in mesh-based Networks-on-Chip. This new algorithm can be dynamically reconfigured to support irregular topologies caused by faulty components in a mesh network. In addition, it is a distributed, adaptive and congestion-aware routing algorithm where only two virtual channels are used for both adaptiveness and fault-tolerance. The proposed routing method has a multi-level fault-tolerance capability and therefore it is capable to tolerate more faulty components in more complicated faulty situations with additional hardware costs. The network performance, fault-tolerance capability and hardware overhead are evaluated through appropriate simulations and syntheses. The experimental results show that the overall reliability of a Network-on-Chip is significantly enhanced against multiple component failures with only a small hardware overhead.}
}
@article{XU2019110402,
title = {LDFR: Learning deep feature representation for software defect prediction},
journal = {Journal of Systems and Software},
volume = {158},
pages = {110402},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110402},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301761},
author = {Zhou Xu and Shuai Li and Jun Xu and Jin Liu and Xiapu Luo and Yifeng Zhang and Tao Zhang and Jacky Keung and Yutian Tang},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network},
abstract = {Software Defect Prediction (SDP) aims to detect defective modules to enable the reasonable allocation of testing resources, which is an economically critical activity in software quality assurance. Learning effective feature representation and addressing class imbalance are two main challenges in SDP. Ideally, the more discriminative the features learned from the modules and the better the rescue performed on the imbalance issue, the more effective it should be in detecting defective modules. In this study, to solve these two challenges, we propose a novel framework named LDFR by Learning Deep Feature Representation from the defect data for SDP. Specifically, we use a deep neural network with a new hybrid loss function that consists of a triplet loss to learn a more discriminative feature representation of the defect data and a weighted cross-entropy loss to remedy the imbalance issue. To evaluate the effectiveness of the proposed LDFR framework, we conduct extensive experiments on a benchmark dataset with 27 defect data (each with three types of features), using three traditional and three effort-aware indicators. Overall, the experimental results demonstrate the superiority of our LDFR framework in detecting defective modules when compared with 27 baseline methods, except in terms of the indicator of Precision.}
}
@article{CHATZIPOULIDIS201528,
title = {Information infrastructure risk prediction through platform vulnerability analysis},
journal = {Journal of Systems and Software},
volume = {106},
pages = {28-41},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215000837},
author = {Aristeidis Chatzipoulidis and Dimitrios Michalopoulos and Ioannis Mavridis},
keywords = {Zero-day risk, Kolmogorov-Smirnov test, Bayesian belief network (BBN)},
abstract = {The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov–-Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks.}
}
@article{FU2010384,
title = {Failure-aware resource management for high-availability computing clusters with distributed virtual machines},
journal = {Journal of Parallel and Distributed Computing},
volume = {70},
number = {4},
pages = {384-393},
year = {2010},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2010.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731510000031},
author = {Song Fu},
keywords = {System availability, Resource management, Component failures, Cluster computing, Distributed virtual machines, System reconfiguration},
abstract = {In large-scale networked computing systems, component failures become norms instead of exceptions. Failure-aware resource management is crucial for enhancing system availability and achieving high performance. In this paper, we study how to efficiently utilize system resources for high-availability computing with the support of virtual machine (VM) technology. We design a reconfigurable distributed virtual machine (RDVM) infrastructure for networked computing systems. We propose failure-aware node selection strategies for the construction and reconfiguration of RDVMs. We leverage the proactive failure management techniques in calculating nodes’ reliability states. We consider both the performance and reliability status of compute nodes in making selection decisions. We define a capacity–reliability metric to combine the effects of both factors in node selection, and propose Best-fit algorithms with optimistic and pessimistic selection strategies to find the best qualified nodes on which to instantiate VMs to run user jobs. We have conducted experiments using failure traces from production systems and the NAS Parallel Benchmark programs on a real-world cluster system. The results show the enhancement of system productivity by using the proposed strategies with practically achievable accuracy of failure prediction. With the Best-fit strategies, the job completion rate is increased by 17.6% compared with that achieved in the current LANL HPC cluster. The task completion rate reaches 91.7% with 83.6% utilization of relatively unreliable nodes.}
}
@article{ARVANITOU201752,
title = {A mapping study on design-time quality attributes and metrics},
journal = {Journal of Systems and Software},
volume = {127},
pages = {52-77},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S016412121730016X},
author = {Elvira Maria Arvanitou and Apostolos Ampatzoglou and Alexander Chatzigeorgiou and Matthias Galster and Paris Avgeriou},
keywords = {Software quality, Measurement, Design-time quality attributes, Mapping study},
abstract = {Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.}
}
@article{JULKA2011613,
title = {Making use of prognostics health management information for aerospace spare components logistics network optimisation},
journal = {Computers in Industry},
volume = {62},
number = {6},
pages = {613-622},
year = {2011},
note = {Special Issue: Grand Challenges for Discrete Event Logistics Systems},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2011.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166361511000649},
author = {Nirupam Julka and Annamalai Thirunavukkarasu and Peter Lendermann and Boon Ping Gan and Arnd Schirrmann and Helge Fromm and Elaine Wong},
keywords = {Discrete Event Logistics Systems, Aerospace spare component logistics, Model-based approach, Prognostics and Health Management, D-SIMSPAIR},
abstract = {Although research has evolved significantly over the last decade, there are still a large number of Grand Challenges confronting modelling, model deployment, and model-based decision making of large-scale complex Discrete Event Logistics Systems (DELS) to be tackled, as identified and reviewed during a Dagstuhl workshop in March 2010. This paper illustrates how several of these challenges are already being addressed, based on a series of case studies from the Aerospace Spare Components Logistics domain, where consolidated operational Prognostics and Health Management (PHM) information can be used for tactical planning and optimisation of spare components logistics networks. In this setting, the growing potential of PHM technology to facilitate the maintenance and support of commercial and military aircraft emphasises the need for tools to determine the impacts and benefits of a PHM system. To achieve this, the prognostics parameters and related logistics policies were identified, modelled, and subsequently incorporated into a simulation-based decision support framework.}
}
@article{LEE2001119,
title = {Self-repairable GALs},
journal = {Journal of Systems Architecture},
volume = {47},
number = {2},
pages = {119-135},
year = {2001},
issn = {1383-7621},
doi = {https://doi.org/10.1016/S1383-7621(00)00061-8},
url = {https://www.sciencedirect.com/science/article/pii/S1383762100000618},
author = {Chong H. Lee and Douglas V. Hall and Marek A. Perkowski and David S. Jun},
keywords = {Generic Array Logic (GAL) devices, Design methodology, High security, Safety},
abstract = {This paper describes the concept of self-testable and self-repairable Generic Array Logic (GAL) devices for high security and safety applications. A design methodology is proposed for self-repairing of a GAL which is a kind of Electrically Programmable Logic Devices (EPLDs). The fault-locating and fault-repairing architecture with electrically re-configurable GALs is presented. It uses universal test sets, fault-detecting logic, and self-repairing circuits with spare devices. The design method allows to detect, diagnose, and repair of all multiple stuck-at faults which might occur on E2CMOS cells in programmable AND plane. A self-repairing methodology is presented, based on our design architecture. A “column replacement” method with extra columns is introduced that discards each faulty column entirely and replaces it with an extra column. The evaluation methodology proves that a self-repairable GAL will last longer in the field. It gives also information on how many extra columns a GAL needs to reach a lifetime goal, in terms of simulation looping time, until the GAL is not useful any more. Therefore, an ideal point could be estimated, where the maximum reliability can be reached with the minimum cost.}
}
@article{WEI2018580,
title = {Architecture-level hazard analysis using AADL},
journal = {Journal of Systems and Software},
volume = {137},
pages = {580-604},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301188},
author = {Xiaomin Wei and Yunwei Dong and Xuelin Li and W. Eric Wong},
keywords = {Hazard analysis, Model transformation, Semantic preservation, AADL, Hazard model annex},
abstract = {Software systems are becoming increasingly important in safety-critical areas. Designing safe software requires a significant emphasis on hazards in the early design phase of software development. In this paper, we propose a hazard analysis approach based on Architecture Analysis and Design Language (AADL). First, to make up the deficiencies of Error Model Annex (EMV2), we create Hazard Model Annex (HMA) to specify the hazard sources, hazards, hazard trigger mechanisms, and mishaps. By using HMA, a safety model can be built by annotating an architecture model with the error model and hazard model. Then, an architecture-level hazard analysis approach is proposed to automatically generate the hazard analysis table. The approach contains the model transformation from a safety model to a Deterministic Stochastic Petri Nets (DSPNs) model for calculating the occurrence probability of hazards and mishaps. In addition, we present the formal semantics for each constituent part of the safety model, define the model mapping rules, and verify the semantic preservation of the transformation. Finally, HMA is implemented to build safety models and two Eclipse plug-ins of our methodology are also implemented. A case study on a flight control software system has been employed to demonstrate the feasibility of our proposed technique.}
}
@article{MUSCETTOLA19985,
title = {Remote Agent: to boldly go where no AI system has gone before},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {5-47},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00068-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800068X},
author = {Nicola Muscettola and P.Pandurang Nayak and Barney Pell and Brian C. Williams},
keywords = {Autonomous agents, Architectures, Constraint-based planning, Scheduling, Execution, Reactive systems, Diagnosis, Recovery, Model-based reasoning},
abstract = {Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous fleets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraintbased temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's first New Millennium mission, is scheduled for a period of a week in mid 1999. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper.}
}
@article{QUSEF2014147,
title = {Recovering test-to-code traceability using slicing and textual analysis},
journal = {Journal of Systems and Software},
volume = {88},
pages = {147-168},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213002574},
author = {Abdallah Qusef and Gabriele Bavota and Rocco Oliveto and Andrea {De Lucia} and Dave Binkley},
keywords = {Test-to-code traceability, Dynamic slicing, Information retrieval},
abstract = {Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature.}
}
@article{ROBINSON2012188,
title = {SimLean: Utilising simulation in the implementation of lean in healthcare},
journal = {European Journal of Operational Research},
volume = {219},
number = {1},
pages = {188-197},
year = {2012},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2011.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0377221711011234},
author = {Stewart Robinson and Zoe J. Radnor and Nicola Burgess and Claire Worthington},
keywords = {OR in health services, Lean, Discrete-event simulation},
abstract = {Discrete-event simulation (DES) and lean are approaches that have a similar motivation: improvement of processes and service delivery. Both are being used to help improve the delivery of healthcare, but rarely are they used together. This paper explores from a theoretical and an empirical perspective the potential complementary roles of DES and lean in healthcare. The aim is to increase the impact of both approaches in the improvement of healthcare systems. Out of this exploration, the ‘SimLean’ approach is developed in which three roles for DES with lean are identified: education, facilitation and evaluation. These roles are demonstrated through three examples of DES in action with lean. The work demonstrates how the fusion of DES with lean can improve both stakeholder engagement with DES and the impact of lean.}
}
@article{GEIST2018244,
title = {Determining on-fault earthquake magnitude distributions from integer programming},
journal = {Computers & Geosciences},
volume = {111},
pages = {244-259},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417306556},
author = {Eric L. Geist and Tom Parsons},
keywords = {Earthquake magnitude distribution, Integer programming, Earthquake forecast, Optimization},
abstract = {Earthquake magnitude distributions among faults within a fault system are determined from regional seismicity and fault slip rates using binary integer programming. A synthetic earthquake catalog (i.e., list of randomly sampled magnitudes) that spans millennia is first formed, assuming that regional seismicity follows a Gutenberg-Richter relation. Each earthquake in the synthetic catalog can occur on any fault and at any location. The objective is to minimize misfits in the target slip rate for each fault, where slip for each earthquake is scaled from its magnitude. The decision vector consists of binary variables indicating which locations are optimal among all possibilities. Uncertainty estimates in fault slip rates provide explicit upper and lower bounding constraints to the problem. An implicit constraint is that an earthquake can only be located on a fault if it is long enough to contain that earthquake. A general mixed-integer programming solver, consisting of a number of different algorithms, is used to determine the optimal decision vector. A case study is presented for the State of California, where a 4 kyr synthetic earthquake catalog is created and faults with slip ≥3 mm/yr are considered, resulting in >106 variables. The optimal magnitude distributions for each of the faults in the system span a rich diversity of shapes, ranging from characteristic to power-law distributions.}
}
@article{JAFRI2013811,
title = {Energy-aware fault-tolerant network-on-chips for addressing multiple traffic classes},
journal = {Microprocessors and Microsystems},
volume = {37},
number = {8, Part A},
pages = {811-822},
year = {2013},
note = {Special Issue DSD 2012 on Reliability and Dependability in MPSoC Technologies},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2013.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0141933113000537},
author = {Syed. M.A.H. Jafri and Liang Guang and Ahmed Hemani and Kolin Paul and Juha Plosila and Hannu Tenhunen},
keywords = {Network on chips, Fault tolerant network on chips, Adaptive network on chips, Energy aware systems},
abstract = {This paper presents an energy efficient architecture to provide on-demand fault tolerance to multiple traffic classes, running simultaneously on single network on chip (NoC) platform. Today, NoCs host multiple traffic classes with potentially different reliability needs. Providing platform-wide worst-case (maximum) protection to all the classes is neither optimal nor desirable. To reduce the overheads incurred by fault tolerance, various adaptive strategies have been proposed. The proposed techniques rely on individual packet fields and operating conditions to adjust the intensity and hence the overhead of fault tolerance. Presence of multiple traffic classes undermines the effectiveness of these methods. To complement the existing adaptive strategies, we propose on-demand fault tolerance, capable of providing required reliability, while significantly reducing the energy overhead. Our solution relies on a hierarchical agent based control layer and a reconfigurable fault tolerance data path. The control layer identifies the traffic class and directs the packet to the path providing the needed reliability. Simulation results using representative applications (matrix multiplication, FFT, wavefront, and HiperLAN) showed up to 95% decrease in energy consumption compared to traditional worst case methods. Synthesis results have confirmed a negligible additional overhead, for providing on-demand protection (up to 5.3% area), compared to the overall fault tolerance circuitry.}
}
@article{FUCHS202185,
title = {Defect detection in CT scans of cast aluminum parts: A machine vision perspective},
journal = {Neurocomputing},
volume = {453},
pages = {85-96},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221006524},
author = {Patrick Fuchs and Thorben Kröger and Christoph S. Garbe},
keywords = {Semantic segmentation, Computed tomography, Deep learning, Defect detection, Self-supervision, Simulated training data},
abstract = {One of the many applications of X-ray computed tomography (CT) in industry is the detection of pores, cavities and other flaws in cast metal parts. Because of its improvement on part safety and saving of expenses, CT inspection is moving from a random sample inspection towards a full in-line inspection. With the increasing amount of produced data, however, comes the need for an automated processing. Due to tight time constraints the resulting CT scans are very artifact afflicted, which impedes automated inspection. In recent years, deep learning methods—convolutional neural networks in particular—have been used with great success to tackle even complex segmentation tasks in cluttered scenes. As we show, these methods are also applicable to the domain of industrial CT data: they are able to cope with noise, beam hardening, scatter and other artifacts which we encounter here. However, these methods need a vast amount of precisely labeled training data to work properly. Gathering the necessary data is not only cumbersome due to the need of annotating three-dimensional data but also expensive as it requires the knowledge of domain experts. Therefore, we present a new approach: We train our models on realistically simulated CT data only. Here, a precise per-voxel ground truth can simply be computed. In order to show that the simulated data is sufficient to train a segmentation network, we turn to its prediction performance on real CT data. We compare the prediction performance of traditional algorithms as well as the trained segmentation network on simulated and real validation data and demonstrate that they behave similarly. The ground truth for the real validation data is hand-labeled using high-quality CT scans, while the actual validation set consists of CT scans of lower quality of the exact same parts. For a comprehensive evaluation, we evaluate the probability of detection as well as the intersection over union. The first tells us how likely a flaw of given size can be found with a given confidence, which is of special interest to domain experts. The latter gives us a per-voxel information of how precise the overall segmentation is. Moreover, our synthetic data enables us to examine the influence of different artifact types on the detection rate. Besides these quantitative analyses we show some qualitative results of real-world applications. To the best of our knowledge, we describe the first approach for defect detection in three-dimensional CT data, which is solely trained with simulated data.}
}
@article{LIN201261,
title = {An integrated 3D log processing optimization system for hardwood sawmills in central Appalachia, USA},
journal = {Computers and Electronics in Agriculture},
volume = {82},
pages = {61-74},
year = {2012},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2011.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0168169911003267},
author = {Wenshu Lin and Jingxin Wang},
keywords = {Log breakdown, Optimization, Exhaustive, Dynamic programming, Hardwood sawmills, Appalachian hardwood},
abstract = {An integrated 3D log processing optimization system was developed to perform 3D log generation, opening face determination, headrig log sawing simulation, flitch edging and trimming simulation, cant resawing, and lumber grading. A circular cross-section model together with 3D modeling techniques were used to reconstruct 3D virtual logs. Internal log defects (knots) were depicted using a cone model with apex at the central axis of the log. Heuristic and dynamic programming (DP) algorithms were developed to determine the best opening face, primary log sawing, edging and trimming, and cant resawing optimization. The National Hardwood Lumber Association (NHLA) grading rules were computerized and incorporated into the system for lumber grading. Sawing methods considered in the system include live sawing, cant sawing, grade sawing, and multiple-thickness sawing. The system was tested using field data collected at two central Appalachian hardwood sawmills. Results showed that using the optimization system can significantly improve lumber value recovery. The optimization system can assist mill managers and operators in efficiently utilizing raw materials and increasing their overall competitiveness in the ever-changing forest products market.}
}
@article{SANTOS2018450,
title = {A systematic review on the code smell effect},
journal = {Journal of Systems and Software},
volume = {144},
pages = {450-477},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301444},
author = {José Amancio M. Santos and João B. Rocha-Junior and Luciana Carla Lins Prates and Rogeres Santos do Nascimento and Mydiã Falcão Freitas and Manoel Gomes de Mendonça},
keywords = {Code smell, Systematic review, Thematic synthesis},
abstract = {Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics.}
}
@article{KARANATSIOU2019246,
title = {A bibliometric assessment of software engineering scholars and institutions (2010–2017)},
journal = {Journal of Systems and Software},
volume = {147},
pages = {246-261},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302334},
author = {Dimitra Karanatsiou and Yihao Li and Elvira-Maria Arvanitou and Nikolaos Misirlis and W. Eric Wong},
keywords = {Top institutions, Top scholars, Software engineering, Publications},
abstract = {This paper presents the findings of a bibliometric study, targeting an eight-year period (2010–2017), with the aim of identifying: (a) emerging research directions, (b) the top-20 institutions, and (c) top-20 early stage, consolidated, and experienced scholars in the field of software engineering. To perform this goal, we performed a bibliometric study, by applying the mapping study technique on top-quality software engineering venues, and developed a dataset of 14,456 primary studies. As the ranking metric for institutions, we used the count of papers in which authors affiliated with this institute have been identified in the obtained dataset, whereas regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. Finally, we identified the top-20 rising scholars in the SE research community, based on their recent publication record (between 2015 and 2017) and their research age.}
}
@article{CANDEA2004213,
title = {Improving availability with recursive microreboots: a soft-state system case study},
journal = {Performance Evaluation},
volume = {56},
number = {1},
pages = {213-248},
year = {2004},
note = {Dependable Systems and Networks - Performance and Dependability Symposium (DSN-PDS) 2002: Selected Papers},
issn = {0166-5316},
doi = {https://doi.org/10.1016/j.peva.2003.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166531603001366},
author = {George Candea and James Cutler and Armando Fox},
keywords = {Microreboots, High availability, Recovery-oriented computing},
abstract = {Even after decades of software engineering research, complex computer systems still fail. This paper makes the case for increasing research emphasis on dependability and, specifically, on improving availability by reducing time-to-recover. All software fails at some point, so systems must be able to recover from failures. Recovery itself can fail too, so systems must know how to intelligently retry their recovery. We present here a recursive approach, in which a minimal subset of components is recovered first; if that does not work, progressively larger subsets are recovered. Our domain of interest is Internet services; these systems experience primarily transient or intermittent failures, that can typically be resolved by rebooting. Conceding that failure-free software will continue eluding us for years to come, we undertake a systematic investigation of fine grain component-level restarts, microreboots, as high availability medicine. Building and maintaining an accurate model of large Internet systems is nearly impossible, due to their scale and constantly evolving nature, so we take an application-generic approach, that relies on empirical observations to manage recovery. We apply recursive microreboots to Mercury, a commercial off-the-shelf (COTS)-based satellite ground station that is based on an Internet service platform. Mercury has been in successful operation for over 3 years. From our experience with Mercury, we draw design guidelines and lessons for the application of recursive microreboots to other software systems. We also present a set of guidelines for building systems amenable to recursive reboots, known as “crash-only software systems.”}
}
@article{TUAN20051151,
title = {Application of micro CT and computation modeling in bone tissue engineering},
journal = {Computer-Aided Design},
volume = {37},
number = {11},
pages = {1151-1161},
year = {2005},
note = {Bio-CAD},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2005.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448505000369},
author = {Ho Saey Tuan and Dietmar W. Hutmacher},
keywords = {Computer aided design, Medical imaging, Scaffolds, Bone engineering, Micro CT},
abstract = {Computer aided technologies, medical imaging, and rapid prototyping has created new possibilities in biomedical engineering. The systematic variation of scaffold architecture as well as the mineralization inside a scaffold/bone construct can be studied using computer imaging technology and CAD/CAM and micro computed tomography (CT). In this paper, the potential of combining these technologies has been exploited in the study of scaffolds and osteochondral repair. Porosity, surface area per unit volume and the degree of interconnectivity were evaluated through imaging and computer aided manipulation of the scaffold scan data. For the osteochondral model, the spatial distribution and the degree of bone regeneration were evaluated. In this study the versatility of two softwares Mimics (Materialize), CTan and 3D realistic visualization (Skyscan) were assessed, too.}
}
@article{CERDEIRAL201956,
title = {Software project management in high maturity: A systematic literature mapping},
journal = {Journal of Systems and Software},
volume = {148},
pages = {56-87},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302218},
author = {Cristina T. Cerdeiral and Gleison Santos},
keywords = {Quantitative project management, High maturity project management, Maturity models},
abstract = {High maturity in software development involves statistically controlling the performance of critical subprocesses and using the predictability thus gained to manage projects with better planning precision and monitoring control. Maturity models such as CMMI mention statistical and other quantitative methods, techniques, and tools supporting high-maturity project management, but do not provide details about them, their use or their available types. Thus, knowledge is lacking on how to support software process improvement initiatives to select and apply statistical and other quantitative methods, techniques and tools in this context. The goal of this study is to identify various methods, techniques, and tools which can assist in high-maturity software project management. By conducting a systematic literature mapping, we identified 108 papers describing 153 contributions. We describe the contributions identified, classifying them by their type, their software technology maturation phase, the method by which they were evaluated, the development methods and characteristics which they support, and the process/indicator areas to which they were applied. We hope this work can help fill the knowledge gap on the statistical and other quantitative methods, techniques and tools actually being proposed, evaluated, experimented with and adopted by organizations to support quantitative high-maturity software project management.}
}
@article{LIN201855,
title = {A fault-tolerant ONVIF protocol extension for seamless surveillance video stream recording},
journal = {Computer Standards & Interfaces},
volume = {55},
pages = {55-72},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916302008},
author = {Chia-Feng Lin and Hsin-Ta Chiao and Ruey-Kai Sheu and Yue-Shan Chang and Shyan-Ming Yuan},
keywords = {ONVIF, Fault tolerance, Surveillance},
abstract = {ONVIF (Open Network Video Interface Forum) is an importance industrial standard for the video surveillance field. There are over 8000 ONVIF-compliant IP cameras (i.e., ONVIF NVT) and network video recorders (i.e., ONVIF NVS) from various vendors listed on the ONVIF official website. However, because the current ONVIF specifications do not support any fault-tolerant functions to handle NVS failure, in this paper we propose a fault-tolerant solution for ONVIF NVS and try to minimize the modifications to the current ONVIF specifications in the proposed design. Besides, we also propose an extension to the ONVIF NVT for preventing the loss of captured NVT video data during the failover procedure triggered by NVS failure. Finally, we evaluate the following four performance metrics of the proposed fault-tolerant ONVIF surveillance system: the maximal video local cache time supported by an NVT, the response time for accessing an ONVIF web service, the interrupt duration of video recording due to NVS failure, and the live video black out time observed by a video management system. Since the experimental results are all within the acceptable range for a real fault-tolerant video surveillance system in the real world, the experiences provided by this paper is a good reference for improving the fault-tolerant capabilities of the ONVIF video surveillance systems.}
}
@article{SCHILLER2001519,
title = {Diagnosis of transient faults in quantised systems},
journal = {Engineering Applications of Artificial Intelligence},
volume = {14},
number = {4},
pages = {519-536},
year = {2001},
issn = {0952-1976},
doi = {https://doi.org/10.1016/S0952-1976(01)00020-3},
url = {https://www.sciencedirect.com/science/article/pii/S0952197601000203},
author = {F. Schiller and J. Schröder and J. Lunze},
keywords = {Fault diagnosis, Dynamical systems, Qualitative modelling, Stochastic automata, Signal quantisation},
abstract = {This paper concerns the diagnosis and identification of faults that occur in systems where signals can only be measured through a quantiser. A qualitative model is used that represents the discrete-event behaviour of the quantised system. Three different diagnostic algorithms are presented for determining the fault probabilities, the first concerning faults currently affecting the system and the second determining the probabilities that faults occurred at any time in the past. The third algorithm is based on the assumption that the faults occurred only during a time interval in the past. Due to this assumption, the algorithm is applicable for continuously running processes and for the online identification of transient faults. The approach is illustrated by an example of a manufacturing cell.}
}
@article{CAGLAYAN2016288,
title = {Effect of developer collaboration activity on software quality in two large scale projects},
journal = {Journal of Systems and Software},
volume = {118},
pages = {288-296},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300097},
author = {Bora Çaglayan and Ayşe Başar Bener},
keywords = {Collaboration networks, Developer collaboration, Software quality, Human factor in software engineering},
abstract = {Developers work together during software development and maintenance to resolve issues and implement features in large software projects. The structure of their development collaboration activity may have impact on the quality of the final product in terms of higher number of defects. In this paper, we aim to understand the effect of collaboration on the defect proneness software. We model the collaboration of developers as an undirected network. We extract the centrality of the developers from the collaboration network using different measures that quantifies the importance of the nodes. We analyze the defect inducing and fixing data of the developers in two large software projects. Our findings in this study can be summarized as follows: (a) Centrality and source code change activity of developers in the collaboration network may change their defect induction rates i.e. the defect proneness of their change sets, (b) Contrary to the common perception, more experienced people have relatively higher defect induction rates.}
}
@article{YAZDANBAKHSH20161256,
title = {On deterministic chaos in software reliability growth models},
journal = {Applied Soft Computing},
volume = {49},
pages = {1256-1269},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616304008},
author = {O. Yazdanbakhsh and S. Dick and I. Reay and E. Mace},
keywords = {Software reliability, Chaos theory, Time series analysis, Machine learning, Forecasting},
abstract = {Software reliability growth models attempt to forecast the future reliability of a software system, based on observations of the historical occurrences of failures. This allows management to estimate the failure rate of the system in field use, and to set release criteria based on these forecasts. However, the current software reliability growth models have never proven to be accurate enough for widespread industry use. One possible reason is that the model forms themselves may not accurately capture the underlying process of fault injection in software; it has been suggested that fault injection is better modeled as a chaotic process rather than a random one. This possibility, while intriguing, has not yet been evaluated in large-scale, modern software reliability growth datasets. We report on an analysis of four software reliability growth datasets, including ones drawn from the Android and Mozilla open-source software communities. These are the four largest software reliability growth datasets we are aware of in the public domain, ranging from 1200 to over 86,000 observations. We employ the methods of nonlinear time series analysis to test for chaotic behavior in these time series; we find that three of the four do show evidence of such behavior (specifically, a multifractal attractor). Finally, we compare a deterministic time series forecasting algorithm against a statistical one on both datasets, to evaluate whether exploiting the apparent chaotic behavior might lead to more accurate reliability forecasts.}
}
@article{XU2021110862,
title = {A comprehensive comparative study of clustering-based unsupervised defect prediction models},
journal = {Journal of Systems and Software},
volume = {172},
pages = {110862},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110862},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302521},
author = {Zhou Xu and Li Li and Meng Yan and Jin Liu and Xiapu Luo and John Grundy and Yifeng Zhang and Xiaohong Zhang},
keywords = {Clustering-based unsupervised models, Empirical study, Data analytics for defect prediction},
abstract = {Software defect prediction recommends the most defect-prone software modules for optimization of the test resource allocation. The limitation of the extensively-studied supervised defect prediction methods is that they require labeled software modules which are not always available. An alternative solution is to apply clustering-based unsupervised models to the unlabeled defect data, called Clustering-based Unsupervised Defect Prediction (CUDP). However, there are few studies to explore the impacts of clustering-based models on defect prediction performance. In this work, we performed a large-scale empirical study on 40 unsupervised models to fill this gap. We chose an open-source dataset including 27 project versions with 3 types of features. The experimental results show that (1) different clustering-based models have significant performance differences and the performance of models in the instance-violation-score-based clustering family is obviously superior to that of models in hierarchy-based, density-based, grid-based, sequence-based, and hybrid-based clustering families; (2) the models in the instance-violation-score-based clustering family achieves competitive performance compared with typical supervised models; (3) the impacts of feature types on the performance of the models are related to the indicators used; and (4)the clustering-based unsupervised models do not always achieve better performance on defect data with the combination of the 3 types of features.}
}
@article{WOUNGANG20121682,
title = {Coding-error based defects in enterprise resource planning software: Prevention, discovery, elimination and mitigation},
journal = {Journal of Systems and Software},
volume = {85},
number = {7},
pages = {1682-1698},
year = {2012},
note = {Software Ecosystems},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212000544},
author = {Isaac Woungang and Felix O. Akinladejo and David W. White and Mohammad S. Obaidat},
keywords = {Defect density, Coding defects, ERP, Software development, Defect reduction, Code auditing, Static code analysis, Software testing},
abstract = {Software defects due to coding errors continue to plague the industry with disastrous impact, especially in the enterprise application software category. Identifying how much of these defects are specifically due to coding errors is a challenging problem. In this paper, we investigate the best methods for preventing new coding defects in enterprise resource planning (ERP) software, and discovering and fixing existing coding defects. A large-scale survey-based ex-post-facto study coupled with experiments involving static code analysis tools on both sample code and real-life million lines of code open-source ERP software were conducted for such purpose. The survey-based methodology consisted of respondents who had experience developing ERP software. This research sought to determine if software defects could be merely mitigated or totally eliminated, and what supporting policies, procedures and infrastructure were needed to remedy the problem. In this paper, we introduce a hypothetical framework developed to address our research questions, the hypotheses we have conjectured, the research methodology we have used, and the data analysis methods used to validate the stated hypotheses. Our study revealed that: (a) the best way for ERP developers to discover coding-error based defects in existing programs is to choose an appropriate programming language; perform a combination of manual and automated code auditing, static code analysis, and formal test case design, execution and analysis, (b) the most effective ways to mitigate defects in an ERP system is to track the defect densities in the ERP software, fix the defects found, perform regression testing, and update the resulting defect density statistics, and (c) the impact of epistemological and legal commitments on the defect densities of ERP systems is inconclusive. We feel that our proposed model has the potential to vastly improve the quality of ERP and other similar software by reducing the coding-error defects, and recommend that future research aimed at testing the model in actual production environments.}
}
@article{KITCHENHAM201037,
title = {What’s up with software metrics? – A preliminary mapping study},
journal = {Journal of Systems and Software},
volume = {83},
number = {1},
pages = {37-51},
year = {2010},
note = {SI: Top Scholars},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0164121209001599},
author = {Barbara Kitchenham},
keywords = {Software metrics, Secondary study, Literature survey, Mapping study, Influential papers, Empirical evaluation problems},
abstract = {Background
Many papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research.
Aim
This paper aims to identify trends in influential software metrics papers and assess the possibility of using secondary studies to integrate research results.
Method
Search facilities in the SCOPUS tool were used to identify the most cited papers in the years 2000–2005 inclusive. Less cited papers were also selected from 2005. The selected papers were classified according factors such as to main topic, goal and type (empirical or theoretical or mixed). Papers classified as “Evaluation studies” were assessed to investigate the extent to which results could be synthesized.
Results
Compared with less cited papers, the most cited papers were more frequently journal papers, and empirical validation or data analysis studies. However, there were problems with some empirical validation studies. For example, they sometimes attempted to evaluate theoretically invalid metrics and fail to appreciate the importance of the context in which data are collected.
Conclusions
This paper, together with other similar papers, confirms that there is a large body of research related to software metrics. However, software metrics researchers may need to refine their empirical methodology before they can answer useful empirical questions.}
}
@article{CAPILUPPI2020110593,
title = {The effect of multiple developers on structural attributes: A Study based on java software},
journal = {Journal of Systems and Software},
volume = {167},
pages = {110593},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110593},
url = {https://www.sciencedirect.com/science/article/pii/S016412122030073X},
author = {Andrea Capiluppi and Nemitari Ajienka and Steve Counsell},
keywords = {Object oriented, Metrics, Collaborative development, Open source, Software structure},
abstract = {Context
Long-term software projects employ different software developers who collaborate on shared artifacts. The accumulation of changes pushed by different developers leave traces on the underlying code, that have an effect on its future maintainability, and even reuse.
Objective
This study focuses on the how the changes by different developers might have an impact on the code: we investigate whether the work of multiple developers, and their experience, have a visible effect on the structural metrics of the underlying code.
Method
We consider nine object-oriented (OO) attributes and we measure them in a GitHub sample containing the top 200 ‘forked’ projects. For each of their classes, we evaluated the number of distinct developers contributing to its source code, and their experience in the project.
Results
We show that the presence of multiple developers working on the same class has a visible effect on the chosen OO metrics, and often in the opposite direction to what the guidelines for each attribute suggest. We also show how the relative experience of developers in a project plays an important role in the distribution of those metrics, and the future maintenance of the Java classes.
Conclusions
Our results show how distributed development has an effect on the structural attributes of a software system and how the experience of developers plays a fundamental role in that effect. We also discover workarounds and best practices in 4 applied case studies.}
}
@article{PANDA2015170,
title = {Distributed self fault diagnosis algorithm for large scale wireless sensor networks using modified three sigma edit test},
journal = {Ad Hoc Networks},
volume = {25},
pages = {170-184},
year = {2015},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570870514002224},
author = {Meenakshi Panda and P.M. Khilar},
keywords = {Wireless sensor networks, Distributed self fault diagnosis algorithm, Soft fault, Hard fault, Three sigma edit test, Normal Gaussian distribution},
abstract = {Distributed self diagnosis is an important problem in wireless sensor networks (WSNs) where each sensor node needs to learn its own fault status. The classical methods for fault finding using mean, median, majority voting and hypothetical test based approaches are not suitable for large scale WSNs due to large deviation in inaccurate data transmission by different faulty sensor nodes. In this paper, a modified three sigma edit test based self fault diagnosis algorithm is proposed which diagnose both hard and soft faulty sensor nodes. The proposed distribute self fault diagnosis (DSFD) algorithm is simulated in NS3 and the performances are compared with the existing distributed fault detection algorithms. The simulation results show that the detection accuracy, false alarm rate and false positive rate performance of the DSFD algorithm is much better in adverse environment where the traditional methods fails to detect the fault. The other parameters such as detection latency, energy consumption and the network life time are also determined.}
}
@article{HAGHIGHATKHAH201725,
title = {Automotive software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {128},
pages = {25-55},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300560},
author = {Alireza Haghighatkhah and Ahmad Banijamali and Olli-Pekka Pakanen and Markku Oivo and Pasi Kuvaja},
keywords = {Literature survey, Systematic mapping study, Automotive software engineering, Automotive systems, Embedded systems, Software-intensive systems},
abstract = {The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.}
}
@article{SHEPPERD2018120,
title = {The role and value of replication in empirical software engineering results},
journal = {Information and Software Technology},
volume = {99},
pages = {120-132},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304305},
author = {Martin Shepperd and Nemitari Ajienka and Steve Counsell},
keywords = {Software engineering, Experiment, Reliability, Replication, Meta-analysis},
abstract = {Context
Concerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence.
Objective
We aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship.
Method
We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals.
Results
Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘confirmatory’ result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of effect sizes. Where we are able to compute replication prediction intervals these were surprisingly large.
Conclusion
We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is confirmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis.}
}
@article{ZHAO201964,
title = {Software defect prediction via cost-sensitive Siamese parallel fully-connected neural networks},
journal = {Neurocomputing},
volume = {352},
pages = {64-74},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219305004},
author = {Linchang Zhao and Zhaowei Shang and Ling Zhao and Taiping Zhang and Yuan Yan Tang},
keywords = {Siamese parallel fully-connected networks, Cost-sensitive learning, Deep learning, Few-shot learning, Software defect prediction},
abstract = {Software defect prediction (SDP) has caused widespread concern among software engineering researchers, which aims to erect a software defect predictor according to historical data. However, it is still difficult to develop an effective SDP model on high-dimensional and limited data. In this study, a novel SDP model for this problem is proposed, called Siamese parallel fully-connected networks (SPFCNN), which combines the advantages of Siamese networks and deep learning into a unified method. And training this model is administered by AdamW algorithm for finding the best weights. The minimum value of a singular formula is the target of training for SPFCNN model. Significantly, we extensively compared SPFCNN method with the state-of-the-art SDP approaches using six openly available datasets from the NASA repository. Six indexes are used to evaluate the performance of the proposed method. Experimental results showed that the SPFCNN method contributes to significantly higher performance compared with benchmarked SDP approaches, indicating that a cost-sensitive neural network could be developed successfully for SDP.}
}
@article{BOURHFIR2001693,
title = {Test cases selection from SDL specifications},
journal = {Computer Networks},
volume = {35},
number = {6},
pages = {693-708},
year = {2001},
issn = {1389-1286},
doi = {https://doi.org/10.1016/S1389-1286(00)00203-6},
url = {https://www.sciencedirect.com/science/article/pii/S1389128600002036},
author = {C. Bourhfir and E. Aboulhamid and F. Khendek and R. Dssouli},
keywords = {Test selection, SDL, EFSM, Data-flow testing, Control-flow testing, Test criteria},
abstract = {Selecting appropriate test cases is a crucial activity in software testing. In this paper, we give an overview and discuss existing methods and tools for test case selection for communication protocols. More precisely, we are interested in techniques for test case generation from specification and description language (SDL) specifications and its underlying behavioral model, extended finite state machines (EFSM) and its variants.}
}
@article{KUMAR2016170,
title = {Hybrid functional link artificial neural network approach for predicting maintainability of object-oriented software},
journal = {Journal of Systems and Software},
volume = {121},
pages = {170-190},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216000054},
author = {Lov Kumar and Santanu Ku. Rath},
keywords = {Artificial neural network, CK metrics suite, Maintainability},
abstract = {In present day, software development methodology is mostly based on object-oriented paradigm. With the increase in the number of these software system, their effective maintenance aspects becomes a crucial factor. Most of the maintainability prediction models in literature are based on techniques such as regression analysis and simple neural network. In this paper, three artificial intelligence techniques (AI) such as hybrid approach of functional link artificial neural network (FLANN) with genetic algorithm (GA), particle swarm optimization (PSO) and clonal selection algorithm (CSA), i.e., FLANN-Genetic (FGA and AFGA), FLANN-PSO (FPSO and MFPSO), FLANN-CSA (FCSA) are applied to design a model for predicting maintainability. These three AI techniques are applied to predict maintainability on two case studies such as Quality Evaluation System (QUES) and User Interface System (UIMS). This paper also focuses on the effectiveness of feature reduction techniques such as rough set analysis (RSA) and principal component analysis (PCA) when they are applied for predicting maintainability. The results show that feature reduction techniques are very effective in obtaining better results while using FLANN-Genetic.}
}
@article{EJLALI2004317,
title = {FPGA-based fault injection into switch-level models},
journal = {Microprocessors and Microsystems},
volume = {28},
number = {5},
pages = {317-327},
year = {2004},
note = {Special Issue on FPGAs: Applications and Designs},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2004.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0141933104000195},
author = {Alireza Ejlali and Seyed {Ghassem Miremadi}},
keywords = {FPGA, Speed-up, Switch-level modeling, Gate-level modeling},
abstract = {This article presents a method for fast fault injection into switch-level circuits using FPGA chips. In this method, gates model switch-level circuits and we can emulate mixed gate-switch-level models. By the use of this method, FPGA chips can be used to accelerate the fault-injection campaigns into switch-level models. The approach has been evaluated experimentally by injecting a set of faults into a pipelined RISC processor. The experimental results show that significant speed-ups with respect to fully simulation-based fault-injection methods can be achieved.}
}
@article{SAAGI2017282,
title = {A model library for simulation and benchmarking of integrated urban wastewater systems},
journal = {Environmental Modelling & Software},
volume = {93},
pages = {282-295},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216306703},
author = {R. Saagi and X. Flores-Alsina and S. Kroll and K.V. Gernaey and U. Jeppsson},
keywords = {Benchmark simulation models, Urban wastewater system modelling, Integrated control strategies},
abstract = {This paper presents a freely distributed, open-source toolbox to predict the behaviour of urban wastewater systems (UWS). The proposed library is used to develop a system-wide Benchmark Simulation Model (BSM-UWS) for evaluating (local/global) control strategies in urban wastewater systems (UWS). The set of models describe the dynamics of flow rates and major pollutants (COD, TSS, N and P) within the catchment (CT), sewer network (SN), wastewater treatment plant (WWTP) and river water system (RW) for a hypothetical, though realistic, UWS. Evaluation criteria are developed to allow for direct assessment of the river water quality instead of the traditional emission based metrics (for sewer overflows and WWTP discharge). Three case studies are included to illustrate the applicability of the proposed toolbox and also demonstrate the potential benefits of implementing integrated control in the BSM-UWS platform. Simulation results show that the integrated control strategy developed to maximize the utilization of the WWTP's capacity represents a balanced choice in comparison to other options. It also improves the river water quality criteria for unionized ammonia and dissolved oxygen by 62% and 6%, respectively.}
}
@article{KUMAR2018686,
title = {Effective fault prediction model developed using Least Square Support Vector Machine (LSSVM)},
journal = {Journal of Systems and Software},
volume = {137},
pages = {686-712},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300717},
author = {Lov Kumar and Sai Krishna Sripada and Ashish Sureka and Santanu Ku. Rath},
keywords = {CK metrics, Cost analysis, Fault, Feature selection techniques, Least Squares Support Vector Machine (LSSVM), Object-oriented software},
abstract = {Software developers and project teams spend considerable amount of time in identifying and fixing faults reported by testers and users. Predicting defects and identifying regions in the source code containing faults before it is discovered or invoked by users can be valuable in terms of saving maintenance resources, user satisfaction and preventing major system failures post deployment. Fault prediction can also improve the effectiveness of software quality assurance activities by guiding the test team to focus efforts on fault prone components. The work presented in this paper involves building an effective fault prediction tool by identifying and investigating the predictive power of several well-known and widely used software metrics for fault prediction. We apply ten different feature selection techniques to choose the best set of metrics from a set of twenty source code metrics. We build the fault prediction model using Least Squares Support Vector Machine (LSSVM) learning method associated with linear, polynomial and radial basis function kernel functions. We perform experiments on 30 Open Source Java projects. Experimental results reveals that our prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low- 52.139%, median- 46.206%, and high- 32.080%).}
}
@article{MOEYERSOMS201580,
title = {Comprehensible software fault and effort prediction: A data mining approach},
journal = {Journal of Systems and Software},
volume = {100},
pages = {80-90},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002295},
author = {Julie Moeyersoms and Enric {Junqué de Fortuny} and Karel Dejaeger and Bart Baesens and David Martens},
keywords = {Rule extraction, Software fault and effort prediction, Comprehensibility},
abstract = {Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data.}
}
@article{VANDECRUYS2008823,
title = {Mining software repositories for comprehensible software fault prediction models},
journal = {Journal of Systems and Software},
volume = {81},
number = {5},
pages = {823-839},
year = {2008},
note = {Software Process and Product Measurement},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2007.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S0164121207001902},
author = {Olivier Vandecruys and David Martens and Bart Baesens and Christophe Mues and Manu {De Backer} and Raf Haesen},
keywords = {Classification, Software mining, Fault prediction, Comprehensibility, Ant Colony Optimization},
abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.}
}
@article{ELHADEF2008321,
title = {A distributed fault identification protocol for wireless and mobile ad hoc networks},
journal = {Journal of Parallel and Distributed Computing},
volume = {68},
number = {3},
pages = {321-335},
year = {2008},
note = {Wireless Mesh Networks: Behavior, Artifacts and Solutions},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2007.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0743731507000731},
author = {Mourad Elhadef and Azzedine Boukerche and Hisham Elkadiki},
keywords = {Wireless networks, Mobile ad hoc networks, Fault tolerance, Distributed algorithms, System-level fault self-diagnosis, Comparison-based approach},
abstract = {This paper considers the problem of self-diagnosis of wireless and mobile ad hoc networks (MANETs) using the comparison approach. In this approach, a network (MANET) consists of a collection of n independent heterogeneous mobile or stationary hosts interconnected via wireless links, and it is assumed that at most σ of these hosts are faulty. In order to diagnose the state of the MANET, tasks are assigned to pairs of hosts and the outcomes of these tasks are compared. The agreements and disagreements between the hosts are the basis for identifying the faulty ones. The comparison approach is believed to be one of the most practical fault identification approaches for diagnosing hard and soft faults. We develop a new distributed self-diagnosis protocol, called Dynamic-DSDP, for MANETs that identifies both hard and soft faults in a finite amount of time. The protocol is constructed on top of a reliable multi-hop architecture. Correctness and complexity proofs are provided and they show that our Dynamic-DSDP performs better, from a communication complexity viewpoint, than the existing protocols. We have also developed a simulator, that is scalable to a large number of nodes. Using the simulator, we carried out a simulation study to analyze the effectiveness of the self-diagnosis protocol and its performance with regards to the number of faulty hosts. The simulation results show that the proposed approach is an attractive and viable alternative or addition to present fault diagnosis techniques in MANET environments.}
}
@article{JIAN20151251,
title = {LS-SVM based substation circuit breakers maintenance scheduling optimization},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {64},
pages = {1251-1258},
year = {2015},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2014.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S014206151400564X},
author = {Liu Jian and Tan Tianyuan},
keywords = {LS-SVM, Bi-level optimization algorithm, Circuit breakers, Maintenance schedule},
abstract = {An optimized maintenance schedule for circuit breakers (CBs) can enhance substation reliability and lower maintenance cost. In this paper, a Least Squares Support Vector Machines (LS-SVM) based CB maintenance scheduling optimization approach that considers constraint of cost effect is proposed. Historical operation data are utilized to build a defects tree. A bi-level optimization algorithm is used to choose LS-SVM parameters. The LS-SVM algorithm is used to predict the distribution of defects before and after scheme optimization using aggregated defect data, outage duration, maintenance operation defect detection rate, etc. After the defect loss is quantified based on an expert scoring method and the Gross Domestic Product to power consumption ratio, a cost effect measurement is used to determine the best scheme. The effect of the proposed approach is verified using a numerical simulation of an electric power corporation.}
}
@article{MA2007795,
title = {Web error classification and analysis for reliability improvement},
journal = {Journal of Systems and Software},
volume = {80},
number = {6},
pages = {795-804},
year = {2007},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206002809},
author = {Li Ma and Jeff Tian},
keywords = {Orthogonal defect classification (ODC), Web errors, Web reliability, Web server logs, Web error classification and analysis},
abstract = {In this paper, we adapt an existing defect classification and analysis framework, orthogonal defect classification (ODC), to analyze web errors and identify problematic areas for focused reliability improvement. Based on information extracted from existing web server logs, web errors are classified according to their response code, file type, referrer type, agent type, and observation time. We also introduce an analysis procedure to identify high-risk/high-leverage sub-classes of problems and consolidate analysis results to recommend appropriate followup actions. Results applying our approach to the www.seas.smu.edu and www.kde.org web sites are included to demonstrate its applicability and effectiveness.}
}