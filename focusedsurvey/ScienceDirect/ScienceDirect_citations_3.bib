@article{GALLAGHER20071000,
title = {Parameter estimation and uncertainty analysis for a watershed model},
journal = {Environmental Modelling & Software},
volume = {22},
number = {7},
pages = {1000-1020},
year = {2007},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2006.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815206001629},
author = {Mark Gallagher and John Doherty},
keywords = {Uncertainty analysis, Parameter estimation, Mathematical modeling, Markov Chain Monte Carlo, Model calibration},
abstract = {Where numerical models are employed as an aid to environmental management, the uncertainty associated with predictions made by such models must be assessed. A number of different methods are available to make such an assessment. This paper explores the use of three such methods, and compares their performance when used in conjunction with a lumped parameter model for surface water flow (HSPF) in a large watershed. Linear (or first-order) uncertainty analysis has the advantage that it can be implemented with virtually no computational burden. While the results of such an analysis can be extremely useful for assessing parameter uncertainty in a relative sense, and ascertaining the degree of correlation between model parameters, its use in analyzing predictive uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods are far more robust, and can produce reliable estimates of parameter and predictive uncertainty. As well as this, they can provide the modeler with valuable qualitative information on the shape of parameter and predictive probability distributions; these shapes can be quite complex, especially where local objective function optima lie within those parts of parameter space that are considered probable after calibration has been undertaken. Nonlinear calibration-constrained optimization can also provide good estimates of parameter and predictive uncertainty, even in situations where the objective function surface is complex. Furthermore, they can achieve these estimates using far fewer model runs than MCMC methods. However, they do not provide the same amount of qualitative information on the probability structure of parameter space as do MCMC methods, a situation that can be partially rectified by combining their use with an efficient gradient-based search method that is specifically designed to locate different local optima. All methods of parameter and predictive uncertainty analysis discussed herein are implemented using freely-available software. Hence similar studies, or extensions of the present study, can be easily undertaken in other modeling contexts by other modelers.}
}
@article{STUMPF20141,
title = {Surface reconstruction and landslide displacement measurements with Pléiades satellite images},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {95},
pages = {1-12},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0924271614001269},
author = {A. Stumpf and J.-P. Malet and P. Allemand and P. Ulrich},
keywords = {Image correlation, VHR satellite images, Landslides, Stereo-photogrammetry, No ground control, Surface deformation},
abstract = {Recent advances in image-matching techniques and VHR satellite imaging at submeter resolution theoretically offer the possibility to measure Earth surface displacements with decimetric precision. However, this possibility has yet not been explored and requirements of ground control and external topographic datasets are considered as important bottlenecks that hinder a more common application of optical image correlation for displacement measurements. This article describes an approach combining spaceborne stereo-photogrammetry, orthorectification and sub-pixel image correlation to measure the horizontal surface displacement of landslides from Pléiades satellite images. The influence of the number of ground-control points on the accuracy of the image orientation, the extracted surface models and the estimated displacement rates is quantified through comparisons with airborne laser scan and in situ global navigation satellite measurements at permanent stations. The comparison shows a maximum error of 0.13m which is one order of magnitude more accurate than what has been previously reported with spaceborne optical images from other sensors. The obtained results indicate that the approach can be applied without significant loss in accuracy when no ground control points are available. It could, therefore, greatly facilitate displacement measurements for a broad range of applications.}
}
@incollection{PACK2003711,
title = {Traffic capacity analysis for shared access networks with QoS objectives and uncertain demands},
editor = {J. Charzinski and R. Lehnert and P. Tran-Gia},
series = {Teletraffic Science and Engineering},
publisher = {Elsevier},
volume = {5},
pages = {711-720},
year = {2003},
booktitle = {Providing Quality of Service in Heterogeneous Environments},
issn = {1388-3437},
doi = {https://doi.org/10.1016/S1388-3437(03)80220-2},
url = {https://www.sciencedirect.com/science/article/pii/S1388343703802202},
author = {Charles D. Pack},
abstract = {In the USA, the Telecommunication Act of 1996 specifies that the incumbent local exchange carriers (ILECs) must petition the FCC (and state PUCs) in order to be able to offer “long distance” services within their territory. One of the major criteria for gaining approval of the petition is that competitive local exchange carriers (CLECs), using the ILEC network for access, have access quality that is essentially comparable to that of the ILEC in similar parts of the network. In this paper, we develop analytical models that can help quantify the effects of forecast (“planning data”) quality on the capacity required to ensure that high-quality access is available to both ILECs and CLECs. In fact, some of the results are rather dramatic. A goal of increasing call completions to nearly 100%, may require 70% (or more) of the trunk groups have significant “reserve capacity” when planning data is poor.}
}
@article{GLASS2020767,
title = {Uncovering spatial productivity centers using asymmetric bidirectional spillovers},
journal = {European Journal of Operational Research},
volume = {285},
number = {2},
pages = {767-788},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720301259},
author = {Anthony J. Glass and Karligash Kenjegalieva and Mustapha Douch},
keywords = {Productivity and competitiveness, Spatial stochastic frontier analysis, Revenue function, Multiple spatial networks, U.S. banks},
abstract = {The principal contribution of this paper is to present the first method to sift through a large number of firms in an industry to uncover which firms act as large spatial total factor productivity (TFP) growth centers. We define a large spatial TFP growth center as a firm that is a large net generator of spatial TFP growth spillovers, i.e., it is a source of large TFP growth spill-outs to other firms vis-à-vis the size of the TFP growth spill-ins that permeate to the firm from other firms. We use this definition because, other things being equal, firms would want to locate near a firm that is a net generator of TFP growth spillovers. In the process of presenting the above method we make three further contributions, two of which are methodological and the other relates to our application. First, rather than follow the literature on spatial frontier modeling by considering spatial interaction between firms in a single network, we introduce a more sophisticated model that is able to account for spatial interaction in multiple networks. Second, we obtain bidirectional spatial TFP growth decompositions by complementing a unidirectional decomposition in the literature, where the spillover components are spill-ins to a firm, with a decomposition that includes spill-out components. Third, from a spatial revenue frontier for U.S. banks (1998–2015), we find a number of cases where banks that represent large spatial TFP growth centers have branches that cluster together, while in several states we find no such clusters.}
}
@article{KIM20103011,
title = {Collision-Aware Rate Adaptation in multi-rate WLANs: Design and implementation},
journal = {Computer Networks},
volume = {54},
number = {17},
pages = {3011-3030},
year = {2010},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2010.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389128610001593},
author = {Seongkwan Kim and Lochan Verma and Sunghyun Choi and Daji Qiao},
keywords = {IEEE 802.11 WLAN, Rate adaptation, Collision awareness, ns-2 simulation, MadWifi-based implementation},
abstract = {Many rate adaptation algorithms have been proposed for IEEE 802.11 Wireless LAN devices and most of them operate in an open-loop manner, i.e., the transmitter adapts its transmission rate without using the feedback from the receiver. A key problem with such transmitter-based rate adaptation schemes is that they do not consider the collision effect. Accordingly, they often result in severe throughput degradation when many transmission failures are due to frame collisions. In this paper, we present a transmitter-based rate adaptation scheme, called CARA (Collision-Aware Rate Adaptation), and its MadWifi-based implementation. The key idea of CARA is that the transmitter combines adaptively the RTS/CTS (Request-to-Send/Clear-to-Send) exchange with the CCA (Clear Channel Assessment) functionality in order to differentiate frame collisions from transmission failures due to channel errors. The effectiveness of CARA schemes is evaluated via extensive ns-2 simulations and testbed experimentations.}
}
@article{LI2013981,
title = {The prediction model for electrical power system using an improved hybrid optimization model},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {44},
number = {1},
pages = {981-987},
year = {2013},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2012.08.047},
url = {https://www.sciencedirect.com/science/article/pii/S0142061512004954},
author = {Guo-Dong Li and Shiro Masuda and Masatake Nagai},
keywords = {Thermal electric power generation prediction, Grey model, Regression model, Markov chain model, Taylor approximation method},
abstract = {In this paper, an improved hybrid optimization model based on grey GM (1,1) model is proposed to develop the prediction model in power systems. To realize more accurate prediction, the regression model is firstly integrated into GM (1,1) through compensation for the residual error series. The improved model is defined as RGM (1,1). Furthermore, Markov chain model is applied to RGM (1,1) to enhance the prediction performance. We call the proposed model as MC-RGM (1,1). Finally, Taylor approximation method is presented MC-RGM (1,1) for achieving the high prediction accuracy. The improved model is defined as T-MC-RGM (1,1). A real case of thermal electric power generation in Japan is used to validate the effectiveness of proposed model.}
}
@article{OU201233,
title = {Forecasting agricultural output with an improved grey forecasting model based on the genetic algorithm},
journal = {Computers and Electronics in Agriculture},
volume = {85},
pages = {33-39},
year = {2012},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2012.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0168169912000816},
author = {Shang-Ling Ou},
keywords = {Agricultural output, Forecasting, GM(1,1), Background value, Genetic algorithm},
abstract = {Agriculture is the foundation of the national economy. Thus, an appropriate tool for forecasting agricultural output is very important for policy making. In this study, both modified background value calculation and use of a genetic algorithm to find the optimal parameters were adopted simultaneously to construct an improved GM(1,1) model (GAIGM(1,1)). The sample period of the forecasting models includes the annual values for the data of Taiwan’s agricultural output from 1998 to 2010. The mean absolute percentage error and the root mean square percentage error are two criteria with which to compare the various forecasting models results. Both in-sample and out-of-sample forecast performance results show that the GAIGM(1,1) model has highly accurate forecasting. Therefore, the GAIGM(1,1) model can raise the forecast accuracy of the GM(1,1) model, and it is suitable for use in modeling and forecasting of agricultural output.}
}
@article{XU2005219,
title = {Improving TCP performance in integrated wireless communications networks},
journal = {Computer Networks},
volume = {47},
number = {2},
pages = {219-237},
year = {2005},
note = {Wireless Internet},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2004.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128604001963},
author = {Kai Xu and Ye Tian and Nirwan Ansari},
keywords = {Wireless TCP, Bandwidth estimation, Explicit congestion notification, Congestion control, Loss differentiation},
abstract = {Many analytical and simulation-based studies of TCP performance in wireless environments assume an error-free and congestion-free reverse channel that has the same capacity as the forward channel. Such an assumption does not hold in many real-world scenarios, particularly in the hybrid networks consisting of various wireless LAN (WLAN) and cellular technologies. In this paper, we first study, through extensive simulations, the performance characteristics of four representative TCP schemes, namely TCP New Reno, SACK, Veno, and Westwood, under the network conditions of asymmetric end-to-end link capacities, correlated wireless errors, and link congestion in both forward and reverse directions. We then propose a new TCP scheme, called TCP New Jersey, which is capable of distinguishing wireless packet losses from congestion packet losses, and reacting accordingly. TCP New Jersey consists of two key components, the timestamp-based available bandwidth estimation (TABE) algorithm and the congestion warning (CW) router configuration. TABE is a TCP-sender-side algorithm that continuously estimates the bandwidth available to the connection and guides the sender to adjust its transmission rate when the network becomes congested. TABE is immune to the ACK drops as well as ACK compression. CW is a configuration of network routers such that routers alert end stations by marking all packets when there is a sign of an incipient congestion. The marking of packets by the CW-configured routers helps the sender of the TCP connection to effectively differentiate packet losses caused by network congestion from those caused by wireless link errors. Our simulation results show that TCP New Jersey is able to accurately estimate the available bandwidth of the bottleneck link of an end-to-end path; and the TABE estimator is immune to link asymmetry, bi-directional congestion, and the relative position of the bottleneck link in the multi-hop end-to-end path. The proactive congestion avoidance control mechanism proposed in our scheme minimizes the network congestion, reduces the network volatility, and stabilizes the queue lengths while achieving more throughput than other TCP schemes.}
}
@article{NALLATHAMBI2020103202,
title = {Fault diagnosis architecture for SKINNY family of block ciphers},
journal = {Microprocessors and Microsystems},
volume = {77},
pages = {103202},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103202},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120303690},
author = {Bharathiraja Nallathambi and Karthigaikumar Palanivel},
keywords = {Fault detection, Field-programmable gate array (FPGA), SKINNY, CED-based scheme},
abstract = {Cryptographic hardware and software applications are prone to various attacks either from the environments or from the attacker to gain the secret key. Resource-constrained devices use lightweight cryptographic algorithms to achieve a high level of security. It's always a trade-off between efficient resource utilization and level of security. Out of different attacks, in recent years, fault injection attacks is well matured. It becomes imperative to choose the best and efficient fault diagnosis schemes for lightweight cryptography. In this paper, we propose novel Concurrent error detection (CED), i.e., recomputing with inverted operands (REIO) method for SKINNY Family of Block Ciphers to increase the reliability. The proposed fault detection technique for SKINNY round-based pipelined architecture is adapted. The result shows that the throughput overhead of the SKINNY remains within 2.5% variance for the proposed novel fault detection with a pipelined technique, a maximum of 10% area overhead. We have implemented the proposed fault detection scheme using Xilinx FPGA. Best to our knowledge, there is no CED based fault detection technique proposed in the literature for the SKINNY family of block ciphers. The implementation results show that the proposed scheme is more effective and well suited for resource-constrained environments.}
}
@article{GLASS20191165,
title = {A spatial productivity index in the presence of efficiency spillovers: Evidence for U.S. banks, 1992–2015},
journal = {European Journal of Operational Research},
volume = {273},
number = {3},
pages = {1165-1179},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718307574},
author = {Anthony J. Glass and Karligash Kenjegalieva},
keywords = {(D) Productivity and competitiveness, Spatial total factor productivity, Productivity decomposition, Spatial cost and allocative efficiency growth, Large U.S. banks},
abstract = {We present the methodology for a new spatial decomposition of total factor productivity (TFP) growth. The relevant literature is underdeveloped as there is just one short study which proposes a partial spatial TFP growth decomposition. We develop this literature in four respects. The first two developments are methodological to go from a partial decomposition to a complete one. First, we augment the partial decomposition with a cost efficiency spillover growth component. Second, we introduce own and spillover allocative efficiency growth components. Third, we provide a more detailed coverage of the spatial decomposition of TFP growth. Fourth, in contrast to the traditional application to geographical areas (e.g., countries) in the relevant literature, we apply our decomposition using firm level data, which suggests that there can be an important role for spatial productivity analysis in OR. Our application is to large U.S. banks over the period 1992–2015. Among other things, we find for the average large U.S. bank that TFP growth since the financial crisis has become much more dependent on the bank itself and less so on spatial spillovers.}
}
@article{LU201058,
title = {Bubble-sensing: Binding sensing tasks to the physical world},
journal = {Pervasive and Mobile Computing},
volume = {6},
number = {1},
pages = {58-71},
year = {2010},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2009.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S157411920900100X},
author = {Hong Lu and Nicholas D. Lane and Shane B. Eisenman and Andrew T. Campbell},
keywords = {Opportunistic sensing, Mobile phones, Location-based service},
abstract = {We propose bubble-sensing, a new sensor network abstraction that allows mobile phone users to create a binding between sensing tasks and the physical world at locations of interest, that remains active for a duration set by the user. We envision mobile phones being able to affix sensing task bubbles at places of interest and then receive sensed data as it becomes available in a delay-tolerant fashion, in essence, creating a living documentary of places of interest in the physical world. The system relies on other mobile phones that opportunistically pass through bubble-sensing locations to acquire tasks and do the sensing on behalf of the initiator, and deliver the data to the bubble-sensing server for retrieval by the user who initiated the task. We describe an implementation of the bubble-sensing system using sensor-enabled mobile phones, specifically, Nokia’s N80 and N95 (with GPS, accelerometers, microphone, camera). Task bubbles are maintained at locations through the interaction of “bubble carriers”, which carry the sensing task into the area of interest, and “bubble anchors”, which maintain the task bubble in the area when the bubble carrier is no longer present. In our implementation, bubble carriers and bubble anchors implement a number of simple mobile phone based protocols that refresh the task bubble state as new mobile phones move through the area. Phones communicate using the local Ad-Hoc 802.11g radio to transfer task state and maintain the task in the region of interest. This task bubble state is ephemeral and times out when no bubble carriers or bubble anchors are in the area. Our design is resilient to periods when no mobiles pass through the bubble area and is capable of “reloading” the task into the bubble region. In this paper, we describe the bubble-sensing system and a simple proof-of-concept experiment.}
}
@article{BHASKAR20061104,
title = {Machine learning in bioinformatics: A brief survey and recommendations for practitioners},
journal = {Computers in Biology and Medicine},
volume = {36},
number = {10},
pages = {1104-1125},
year = {2006},
note = {Intelligent Technologies in Medicine and Bioinformatics},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2005.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S001048250500096X},
author = {Harish Bhaskar and David C. Hoyle and Sameer Singh},
abstract = {Machine learning is used in a large number of bioinformatics applications and studies. The application of machine learning techniques in other areas such as pattern recognition has resulted in accumulated experience as to correct and principled approaches for their use. The aim of this paper is to give an account of issues affecting the application of machine learning tools, focusing primarily on general aspects of feature and model parameter selection, rather than any single specific algorithm. These aspects are discussed in the context of published bioinformatics studies in leading journals over the last 5 years. We assess to what degree the experience gained by the pattern recognition research community pervades these bioinformatics studies. We finally discuss various critical issues relating to bioinformatic data sets and make a number of recommendations on the proper use of machine learning techniques for bioinformatics research based upon previously published research on machine learning.}
}
@article{MALINGS201753,
title = {Surface heat assessment for developed environments: Probabilistic urban temperature modeling},
journal = {Computers, Environment and Urban Systems},
volume = {66},
pages = {53-64},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517300613},
author = {Carl Malings and Matteo Pozzi and Kelly Klima and Mario Bergés and Elie Bou-Zeid and Prathap Ramamurthy},
abstract = {Extreme heat waves, exacerbated by the urban heat island effect, have major impacts on the lives and health of city residents. Projected future temperature increases for many urban areas of the United States will further exacerbate these impacts. Accurate predictions of the spatial and temporal distribution of risk associated with such heat waves can support the optimal implementation of strategies to mitigate these risks, such as the issuance of heat advisories and the activation of cooling centers. In this paper, we describe how fine resolution simulations of historic extreme heat events are generated and used to train a probabilistic spatio-temporal model of the temperature distribution in an urban area. We further demonstrate how this model can be used to combine temperature data from various sources and downscale regional predictions in order to provide accurate fine resolution temperature forecasts. Applications of this model are presented for two urban areas: New York City, NY and Pittsburgh, PA, USA. Based on simulated temperature data from fine resolution forecasting models, we find that this probabilistic method can improve the prediction accuracies of urban temperatures, locally and especially in the short-term, with respect to other temperature forecasting and interpolation methods, such as the use of average city-wide temperature predictions and estimates at discrete weather stations.}
}
@article{VILIZZI20131,
title = {Model development of a Bayesian Belief Network for managing inundation events for wetland fish},
journal = {Environmental Modelling & Software},
volume = {41},
pages = {1-14},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212002757},
author = {L. Vilizzi and A. Price and L. Beesley and B. Gawne and A.J. King and J.D. Koehn and S.N. Meredith and D.L. Nielsen},
keywords = {Decision Support Tool, Environmental water, Fish populations, Murray-Darling Basin},
abstract = {Wetlands are essential components of floodplain–river ecosystems that often suffer degradation due to river regulation. To this end, the application of environmental water is increasingly being seen as an important amelioration strategy. However, decisions regarding the delivery of water to maximise environmental benefits, including native fish population health, are complex and difficult. This paper describes the development of a Bayesian Belief Network (BBN) model as part of a Decision Support Tool for assessing inundation strategies to benefit native wetland fish. Separate, albeit closely related, BBNs were developed for three native (golden perch Macquaria ambigua, carp gudgeon Hypseleotris spp., Australian smelt Retropinna semoni) and one alien fish species (common carp Cyprinus carpio carpio). The model structure was based on a conceptualisation of the relationships between wetland habitats, hydrology and fish responses, with emphasis on the types of inundation activities undertaken by managers. Conditional probability tables for fish responses were constructed from expert opinion and the model was validated against field data. The predictive ability and sensitivity of the model reflected the inherent high variability in relationships between wetland characteristics, hydrology and fish responses, but was nonetheless able to address satisfactorily such complexities within a holistic framework. As the model was designed in conjunction with managers and evaluated by them, its application will be enhanced by on-going engagement between managers and scientists.}
}
@article{SHAO2016260,
title = {Efficient Leave-One-Out Cross-Validation-based Regularized Extreme Learning Machine},
journal = {Neurocomputing},
volume = {194},
pages = {260-270},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216003052},
author = {Zhifei Shao and Meng Joo Er},
keywords = {Extreme Learning Machine (ELM), Regularized ELM (RELM), Ridge regression, LOO-CV, Leave-One-Out Cross-Validation},
abstract = {It is well known that the Leave-One-Out Cross-Validation (LOO-CV) is a highly reliable procedure in terms of model selection. Unfortunately, it is an extremely tedious method and has rarely been deployed in practical applications. In this paper, a highly efficient Leave-One-Out Cross-Validation (LOO-CV) formula has been developed and integrated with the popular Regularized Extreme Learning Machine (RELM). The main contribution of this paper is the proposed algorithm, termed as Efficient LOO-CV-based RELM (ELOO-RELM), that can effectively and efficiently update the LOO-CV error with every regularization parameter and automatically select the optimal model with limited user intervention. Rigorous analysis of computational complexity shows that the ELOO-RELM, including the tuning process, can achieve similar efficiency as the original RELM with pre-defined parameter, in which both scale linearly with the size of the training data. An early termination criterion is also introduced to further speed up the learning process. Experimentation studies on benchmark datasets show that the ELOO-RELM can achieve comparable generalization performance as the Support Vector Machines (SVM) with significantly higher learning efficiency. More importantly, comparing to the trial and error tuning procedure employed by the original RELM, the ELOO-RELM can provide more reliable results by the virtue of incorporating the LOO-CV procedure.}
}
@article{RIDLER201476,
title = {Data assimilation framework: Linking an open data assimilation library (OpenDA) to a widely adopted model interface (OpenMI)},
journal = {Environmental Modelling & Software},
volume = {57},
pages = {76-89},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000590},
author = {Marc E. Ridler and Nils {van Velzen} and Stef Hummel and Inge Sandholt and Anne Katrine Falk and Arnold Heemink and Henrik Madsen},
keywords = {OpenDA, OpenMI, Data assimilation, Hydrological modeling, Kalman filter, Uncertainty},
abstract = {Data assimilation optimally merges model forecasts with observations taking into account both model and observational uncertainty. This paper presents a new data assimilation framework that enables the many Open Model Interface (OpenMI) 2.0 .NET compliant hydrological models already available, access to a robust data assimilation library. OpenMI is an open standard that allows models to exchange data during runtime, thus transforming a complex numerical model to a ‘plug and play’ like component. OpenDA is an open interface standard for a set of tools, filters, and numerical techniques to quickly implement data assimilation. The OpenDA–OpenMI framework is presented and tested on a synthetic case that highlights the potential of this new framework. MIKE SHE, a distributed and integrated hydrological model is used to assimilate hydraulic head in a catchment in Denmark. The simulated head over the entire domain were significantly improved by using an ensemble based Kalman filter.}
}
@article{SHEPHERD2021102471,
title = {Physical fault injection and side-channel attacks on mobile devices: A comprehensive analysis},
journal = {Computers & Security},
volume = {111},
pages = {102471},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002959},
author = {Carlton Shepherd and Konstantinos Markantonakis and Nico {van Heijningen} and Driss Aboulkassimi and Clément Gaine and Thibaut Heckmann and David Naccache},
keywords = {Fault injection attacks, Side channel attacks, System-on-Chips (socs), Mobile device security, Embedded systems security},
abstract = {Today’s mobile devices contain densely packaged system-on-chips (SoCs) with multi-core, high-frequency CPUs and complex pipelines. In parallel, sophisticated SoC-assisted security mechanisms have become commonplace for protecting device data, such as trusted execution environments, full-disk and file-based encryption. Both advancements have dramatically complicated the use of conventional physical attacks, requiring the development of specialised attacks. In this survey, we consolidate recent developments in physical fault injections and side-channel attacks on modern mobile devices. In total, we comprehensively survey over 50 fault injection and side-channel attack papers published between 2009–2021. We evaluate the prevailing methods, compare existing attacks using a common set of criteria, identify several challenges and shortcomings, and suggest future directions of research.}
}
@article{ONEILL2017283,
title = {Toward a taxonomy of the unintentional discharge of firearms in law enforcement},
journal = {Applied Ergonomics},
volume = {59},
pages = {283-292},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2016.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003687016301661},
author = {John O'Neill and Dawn A. O'Neill and William J. Lewinski},
keywords = {Firearm, Gun, Law enforcement, Police, Unintentional discharge},
abstract = {An unintentional discharge (UD) is an activation of the trigger mechanism that results in an unplanned discharge that is outside of the firearm's prescribed use. UDs can result in injury or death, yet have been understudied in scientific literature. Pre-existing (1974–2015) UD reports (N = 137) from seven law enforcement agencies in the United States of America were analyzed by context, officer behavior, type of firearm, and injuries. Over 50% of UDs occurred in contexts with low threat potential while engaged in routine firearm tasks. The remaining UDs occurred in contexts with elevated to high threat potential during muscle co-activation, unfamiliar firearm tasks, contact with inanimate objects, and a medical condition. An antecedent-behavior-consequence (A-B-C) taxonomy as well as a standardized reporting form, based on the current findings and the existing literature, are offered as tools for identifying the conditions under which UDs may be likely to occur.}
}
@article{MARTINELLI200750,
title = {A relative map approach to SLAM based on shift and rotation invariants},
journal = {Robotics and Autonomous Systems},
volume = {55},
number = {1},
pages = {50-61},
year = {2007},
note = {Simultaneous Localisation and Map Building},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2006.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889006001473},
author = {Agostino Martinelli and Viet Nguyen and Nicola Tomatis and Roland Siegwart},
keywords = {Localization, Mapping, SLAM, Mobile robot navigation, EKF},
abstract = {This paper presents a solution to the Simultaneous Localization and Mapping (SLAM) problem in the stochastic map framework based on the concept of the relative map. The idea consists in introducing a map state, which only contains relative quantities among the features invariant under shift and rotation. The estimation of this relative state is carried out through an Extended Kalman Filter. The shift and rotation invariance of the state allows us to significantly reduce the computational burden. In particular, the computational requirement is independent of the number of features. Furthermore, since the estimation process is local, it is not affected by the linearization introduced by the EKF. The cases of point features and corner features are considered. Furthermore, in the case of corners, it is considered a realistic case of an indoor environment containing structures consisting of several corners. Finally, since a relative map contains dependent elements, the information coming from all the constraints which express the elements dependency, is exploited. For this, an approximated solution with low computational requirement is proposed. Its limitation arises at the loop closure since it cannot exploit the information in this case. This is discussed in depth for the case of point features. Experimental results carried out on a real platform in our laboratory and by using the Victoria park dataset show the performance of the approach.}
}
@article{BOXWALA2004468,
title = {Organization and representation of patient safety data: Current status and issues around generalizability and scalability},
journal = {Journal of the American Medical Informatics Association},
volume = {11},
number = {6},
pages = {468-478},
year = {2004},
issn = {1067-5027},
doi = {https://doi.org/10.1197/jamia.M1317},
url = {https://www.sciencedirect.com/science/article/pii/S106750270400115X},
author = {Aziz A. Boxwala and Meghan Dierks and Maura Keenan and Susan Jackson and Robert Hanscom and David W. Bates and Luke Sato},
abstract = {Recent reports have identified medical errors as a significant cause of morbidity and mortality among patients. A variety of approaches have been implemented to identify errors and their causes. These approaches include retrospective reporting and investigation of errors and adverse events and prospective analyses for identifying hazardous situations. The above approaches, along with other sources, contribute to data that are used to analyze patient safety risks. A variety of data structures and terminologies have been created to represent the information contained in these sources of patient safety data. Whereas many representations may be well suited to the particular safety application for which they were developed, such application-specific and often organization-specific representations limit the sharability of patient safety data. The result is that aggregation and comparison of safety data across organizations, practice domains, and applications is difficult at best. A common reference data model and a broadly applicable terminology for patient safety data are needed to aggregate safety data at the regional and national level and conduct large-scale studies of patient safety risks and interventions.}
}
@article{SHANTIA2021103731,
title = {Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103731},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103731},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000166},
author = {Amirhossein Shantia and Rik Timmers and Yiebo Chong and Cornel Kuiper and Francesco Bidoia and Lambert Schomaker and Marco Wiering},
keywords = {Robotic navigation, Reinforcement learning, Deep neural networks, Localization and mapping, Robot simulation},
abstract = {In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning. The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.}
}
@article{ANNAERT2003617,
title = {Determinants of mutual fund underperformance: A Bayesian stochastic frontier approach},
journal = {European Journal of Operational Research},
volume = {151},
number = {3},
pages = {617-632},
year = {2003},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(02)00603-3},
url = {https://www.sciencedirect.com/science/article/pii/S0377221702006033},
author = {Jan Annaert and Julien {van den Broeck} and Rudi {Vander Vennet}},
keywords = {Finance, Investment analysis, Mutual funds, Stochastic frontier},
abstract = {The purpose of this paper is to identify ex ante fund statistics that can be related to future performance of European equity funds. In an efficient market setting, actively managed portfolios cannot outperform a passive benchmark strategy. However, purely by chance, some funds outperform their benchmark ex post, making the identification of performance determinants a difficult task. To alleviate this problem, we decompose the return deviation from its expected return into a noise component and an efficiency term, which is 100% if the fund exhibits no underperformance. The decomposition is based on the Bayesian frontier approach. We find evidence that fund efficiency is positively related to fund size and historical performance, the latter being solely due to the poorly performing funds. We fail to find a link between fund age and performance.}
}
@article{BOUFFARD201533,
title = {The ultimate control flow transfer in a Java based smart card},
journal = {Computers & Security},
volume = {50},
pages = {33-46},
year = {2015},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S016740481500005X},
author = {Guillaume Bouffard and Jean-Louis Lanet},
keywords = {Java Card security, Control flow transfer, Countermeasures, Evaluation, Fault tree analysis, Smart card, Logical attack},
abstract = {Recently, researchers published several attacks on smart cards. Among these, software attacks are the most affordable, they do not require specific hardware (laser, EM probe, etc.). Such attacks succeed to modify a sensitive system element which offers access to the smart card assets. To prevent that, smart card manufacturers embed dedicated countermeasures that aim to protect the sensitive system elements. We present a generic approach based on a Control Flow Transfer (CFT) attack to modify the Java Card program counter. This attack is built on a type confusion using the couple of instructions jsr/ret. Evaluated on different Java Cards, this new attack is a generic CFT exploitation that succeeds on each attacked cards. We present several countermeasures proposed by the literature or implemented by smart card designers and for all of them we explain how to bypass them. Then, we propose to use Attack Countermeasure Tree to develop an effective and affordable countermeasure for this attack.}
}
@article{KULESZA2013905,
title = {The crosscutting impact of the AOSD Brazilian research community},
journal = {Journal of Systems and Software},
volume = {86},
number = {4},
pages = {905-933},
year = {2013},
note = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212002427},
author = {Uirá Kulesza and Sérgio Soares and Christina Chavez and Fernando Castor and Paulo Borba and Carlos Lucena and Paulo Masiero and Claudio Sant’Anna and Fabiano Ferrari and Vander Alves and Roberta Coelho and Eduardo Figueiredo and Paulo F. Pires and Flávia Delicato and Eduardo Piveta and Carla Silva and Valter Camargo and Rosana Braga and Julio Leite and Otávio Lemos and Nabor Mendonça and Thais Batista and Rodrigo Bonifácio and Nélio Cacho and Lyrene Silva and Arndt {von Staa} and Fábio Silveira and Marco Túlio Valente and Fernanda Alencar and Jaelson Castro and Ricardo Ramos and Rosangela Penteado and Cecília Rubira},
keywords = {Aspect-Oriented Software Development, Modularity, Research impact},
abstract = {Background
Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil.
Aims
In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community.
Method
Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations.
Results
Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.}
}
@article{KABIR2016144,
title = {Predicting water main failures: A Bayesian model updating approach},
journal = {Knowledge-Based Systems},
volume = {110},
pages = {144-156},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116302386},
author = {Golam Kabir and Solomon Tesfamariam and Jason Loeppky and Rehan Sadiq},
keywords = {Water main failure, Bayesian updating, Bayesian model averaging (BMA), Survival analysis, Weibull proportional hazard model (PHM), Reliability, Uncertainty},
abstract = {Water utilities often rely on water main failure prediction models to develop an effective maintenance, rehabilitation and replacement (M/R/R) action plan. However, the understanding of water main failure becomes difficult due to various uncertainties. In this study, a Bayesian updating based water main failure prediction framework is developed to update the performance of the Bayesian Weibull proportional hazard (BWPHM) model. Applicability of the proposed framework is illustrated with modeling failure prediction of cast iron and ductile iron pipes of the water distribution network of the City of Calgary, Alberta, Canada. The Bayesian updating models have effectively improved the water main failure prediction whenever new data or information becomes available. The proposed framework can assess the model performance in the light of uncertain and evolving information and will help the water utility authorities to attain an acceptable level of service considering financial constraints.}
}
@article{SE200259,
title = {Ground plane estimation, error analysis and applications},
journal = {Robotics and Autonomous Systems},
volume = {39},
number = {2},
pages = {59-71},
year = {2002},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(02)00175-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889002001756},
author = {Stephen Se and Michael Brady},
keywords = {Ground plane, Stereo, Mobility aids, Error analysis, Pose estimation, Obstacle detection, Curb detection},
abstract = {Ground plane perception is of vital importance to human mobility. In order to develop a stereo-based mobility aid for the partially sighted, we model the ground plane based on disparity and analyze its uncertainty. Because the mobility aid is to be mounted on a person, the cameras will be moving around while the person is walking. By calibrating the ground plane at each frame, we show that a partial pose estimate can be recovered. Moreover, by keeping track of how the ground plane changes and analyzing the ground plane, we show that obstacles and curbs are detected. Detailed error analysis has been carried out as reliability is of utmost importance for human applications.}
}
@article{BOS2009251,
title = {Effects of heterogeneity on bank efficiency scores},
journal = {European Journal of Operational Research},
volume = {195},
number = {1},
pages = {251-261},
year = {2009},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2008.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0377221708001495},
author = {J.W.B. Bos and M. Koetter and J.W. Kolari and C.J.M. Kool},
keywords = {Bank production, Heterogeneity, -Efficiency, Benchmarking, Distress},
abstract = {Bank efficiency estimates often serve as a proxy of managerial skill since they quantify sub-optimal production choices. But such deviations can also be due to omitted systematic differences among banks. In this study, we examine the effects of heterogeneity on bank efficiency scores. We compare different specifications of a stochastic cost and alternative profit frontier model with a baseline specification. After conducting a specification test, we discuss heterogeneity effects on efficiency levels, ranks and the tails of the efficiency distribution. We find that heterogeneity controls influence both banks’ optimal costs and profits and their ability to be efficient. Differences in efficiency scores are important for more than only methodological reasons. First, different ways of accounting for heterogeneity result in estimates of foregone profits and additional costs that are significantly different from what we infer from our general specification. Second, banks are significantly re-ranked when their efficiency is estimated with a specification other than the preferred, general specification. Third, the general specification gives the most reliable estimates of the probability of distress, although differences to the other specifications are low.}
}
@article{PARK2021101655,
title = {Impacts of tree and building shades on the urban heat island: Combining remote sensing, 3D digital city and spatial regression approaches},
journal = {Computers, Environment and Urban Systems},
volume = {88},
pages = {101655},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101655},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521000624},
author = {Yujin Park and Jean-Michel Guldmann and Desheng Liu},
keywords = {3D city model, Tree shade, Shade location, Urban heat mitigation, Greening scenario, Spatial regression},
abstract = {The continued increase in average and extreme temperatures around the globe is expected to strike urban communities more harshly because of the urban heat island (UHI). Devising natural and design-based solutions to stem the rising heat has become an important urban planning issue. Recent studies have examined the impacts of 2D/3D urban land-use structures on land surface temperature (LST), but with little attention to the shades cast by 3D objects, such as buildings and trees. It is, however, known that shades are particularly relevant for controlling summertime temperatures. This study examines the role of urban shades created by trees and buildings, focusing on the effects of shade extent and location on LST mitigation. A realistic 3D digital representation of urban and suburban landscapes, combined with detailed 2D land cover information, is developed. Shadows projected on horizontal and vertical surfaces are obtained through GIS analysis, and then quantified as independent variables explaining LST variations over grids of varying sizes with spatial regression models. The estimation results show that the shades on different 3D surfaces, including building rooftops, sun-facing façades, not-sun-facing façades, and on 2D surfaces including roadways, other paved covers, and grass, have cooling effects of varying impact, showing that shades clearly modify the thermal effects of urban built-up surfaces. Tree canopy volume has distinct effects on LST via evapotranspiration. One of the estimated models is used, after validation, to simulate the LST impacts of neighborhood scenarios involving additional greening. The findings illustrate how urban planners can use the proposed methodology to design 3D land-use solutions for effective heat mitigation.}
}
@article{CHAPI2017229,
title = {A novel hybrid artificial intelligence approach for flood susceptibility assessment},
journal = {Environmental Modelling & Software},
volume = {95},
pages = {229-245},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217301573},
author = {Kamran Chapi and Vijay P. Singh and Ataollah Shirzadi and Himan Shahabi and Dieu Tien Bui and Binh Thai Pham and Khabat Khosravi},
keywords = {Flood susceptibility, Bagging-LMT, Bayesian logistic regression, Logistic model tree, Iran},
abstract = {A new artificial intelligence (AI) model, called Bagging-LMT - a combination of bagging ensemble and Logistic Model Tree (LMT) - is introduced for mapping flood susceptibility. A spatial database was generated for the Haraz watershed, northern Iran, that included a flood inventory map and eleven flood conditioning factors based on the Information Gain Ratio (IGR). The model was evaluated using precision, sensitivity, specificity, accuracy, Root Mean Square Error, Mean Absolute Error, Kappa and area under the receiver operating characteristic curve criteria. The model was also compared with four state-of-the-art benchmark soft computing models, including LMT, logistic regression, Bayesian logistic regression, and random forest. Results revealed that the proposed model outperformed all these models and indicate that the proposed model can be used for sustainable management of flood-prone areas.}
}
@article{KOUADIO2018324,
title = {Artificial intelligence approach for the prediction of Robusta coffee yield using soil fertility properties},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {324-338},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918304733},
author = {Louis Kouadio and Ravinesh C. Deo and Vivekananda Byrareddy and Jan F. Adamowski and Shahbaz Mushtaq and Van {Phuong Nguyen}},
keywords = {Smallholder farms, Robusta coffee, Soil fertility, Extreme learning machine, Machine learning in agriculture},
abstract = {As a commodity for daily consumption, coffee plays a crucial role in the economy of several African, American and Asian countries; yet, the accurate prediction of coffee yield based on environmental, climatic and soil fertility conditions remains a challenge for agricultural system modellers. The ability of an Extreme Learning Machine (ELM) model to analyse soil fertility properties and to generate an accurate estimation of Robusta coffee yield was assessed in this study. The performance of 18 different ELM-based models with single and multiple combinations of the predictor variables based on the soil organic matter (SOM), available potassium, boron, sulphur, zinc, phosphorus, nitrogen, exchangeable calcium, magnesium, and pH, was evaluated. The ELM model’s performance was compared to that of existing predictive tools: Multiple Linear Regression (MLR) and Random Forest (RF). Individual model performance and inter-model performance comparisons were based on the root mean square error (RMSE), mean absolute error (MAE), Willmott’s Index (WI), Nash-Sutcliffe efficiency coefficient (ENS), and the Legates and McCabe’s Index (ELM) in the independent testing dataset. In the independent testing phase, an ELM model constructed with SOM, available potassium and available sulphur as predictor variables generated the most accurate coffee yield estimate (i.e., RMSE = 496.35 kg ha−1 or ±13.6%, and MAE = 326.40 kg ha−1 or ±7.9%). This contrasted with the less accurate MLR (RMSE = 1072.09 kg ha−1 and MAE = 797.60 kg ha−1) and RF (RMSE = 1087.35 kg ha−1 and MAE = 769.57 kg ha−1) model. Normalized metrics showed the ELM model’s ability to yield highly accurate results: WI = 0.9952, ENS = 0.406 and ELM = 0.431. In comparison to the MLR and RF models, the adoption of the ELM model as an improved class of artificial intelligence models for coffee yield prediction in smallholder farms in this study constitutes an original contribution to the agronomic sector, particularly with respect to the appropriate selection of most optimal soil properties that can be used in the prediction of optimal coffee yield. The potential utility of coupling artificial intelligence algorithms with biophysical-crop models (i.e., as a data-intelligent automation tool) in decision-support systems that implement precision agriculture, in an effort to improve yield in smallholder farms based on carefully screened soil fertility dataset was confirmed.}
}
@article{FIALIK200917,
title = {Noise and the Mermin-GHZ Game},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {251},
pages = {17-26},
year = {2009},
note = {Proceedings of the International Doctoral Workshop on Mathematical and Engineering Methods in Computer Science (MEMICS 2008)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2009.08.025},
url = {https://www.sciencedirect.com/science/article/pii/S1571066109003508},
author = {Ivan Fialík},
keywords = {Pseudo-telepathy games, Mermin-GHZ game, noisy quantum channels, quantum winning strategy},
abstract = {A pseudo-telepathy game is a game for two or more players for which there is no classical winning strategy, but there is a winning strategy based on sharing quantum entanglement by the players. Since it is generally very hard to perfectly implement a quantum winning strategy for a pseudo-telepathy game, quantum players are almost certain to make errors even though they use a winning strategy. After introducing a model for pseudo-telepathy games, we investigate the impact of several basic noisy quantum channels on the quantum winning strategy for the Mermin-GHZ game. The question of how strong the noise can be so that quantum players would still be better than classical ones is also dealt with.}
}
@article{CUI201782,
title = {Towards trustworthy storage using SSDs with proprietary FTL},
journal = {Microprocessors and Microsystems},
volume = {55},
pages = {82-90},
year = {2017},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141933117303198},
author = {Xiaotong Cui and Liang Shi and Kaijie Wu},
keywords = {SSD, RAID, Malicious, Storage, Design for trust},
abstract = {In recent years we have seen an increasing deployment of flash-based storage, such as SSD, in mission-critical applications due to its fast read/write speed, small form factor, strong shock resistance, etc.. SSDs use a middle layer called flash translation layer (FTL) to maintain the compatibility with the traditional magnetic-based HDDs. Unlike the traditional HDD where the host OS has the knowledge on where and how to access data, SSD uses FTL to translate and implement all operations. Even worse, FTL, which is considered as one of most important intellectual properties of flash-based storage, is often proprietary. This brings up a serious security concern on design trustworthiness when the manufacturer either accidentally or intentionally implements those operations incorrectly or maliciously. We analyze the possible threats that are brought up by the design trust issues, and propose simple yet effective schemes as countermeasures with overhead evaluation.}
}
@article{JONKER20031145,
title = {Philosophies and technologies for ambient aware devices in wearable computing grids},
journal = {Computer Communications},
volume = {26},
number = {11},
pages = {1145-1158},
year = {2003},
note = {Ubiquitous Computing},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(02)00249-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140366402002499},
author = {Pieter Jonker and Stelian Persa and Jurjen Caarls and Frank de Jong and Inald Lagendijk},
keywords = {Ambient aware devices, Personal digital assistants, Differential global positioning system, UMTS, Ad-hex networking},
abstract = {In this paper we treat design philosophies and enabling technologies for ambient awareness within grids of future mobile computing/communication devices. We extensively describe the possible context sensors, their required accuracies, their use in mobile services—possibly leading to background interactions of user devices—as well as a draft of their integration into an ambient aware device. We elaborate on position sensing as one of the main aspects of context aware systems. We first describe a maximum accuracy setup for a mobile user that has the ability of Augmented Reality for indoor and outdoor applications. We then focus on a set-up for pose sensing of a mobile user, based on the fusion of several inertia sensors and DGPS. We describe the anchoring of the position of the user by using visual tracking, using a camera and image processing. We describe our experimental set-up with a background process that, once initiated by the DGPS system, continuously looks in the image for visual clues and—when found—tries to track them, to continuously adjust the inertial sensor system. We present some results of our combined inertia tracking and visual tracking system; we are able to track device rotation and position with an update rate of 10 ms with an accuracy for the rotation of about two degrees, whereas head position accuracy is in the order of a few cm at a visual clue distance of less than 3 m.}
}
@article{NGUYEN20071572,
title = {Systematic testing of an integrated systems model for coastal zone management using sensitivity and uncertainty analyses},
journal = {Environmental Modelling & Software},
volume = {22},
number = {11},
pages = {1572-1587},
year = {2007},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2006.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815206002064},
author = {T.G. Nguyen and J.L. {de Kok}},
keywords = {Integrated systems model, Coastal zone management, Decision support system, Sensitivity and uncertainty analyses, Expert elicitation, Validation, Testing, Sulawesi},
abstract = {Systematic testing of integrated systems models is extremely important but its difficulty is widely underestimated. The inherent complexity of the integrated systems models, the philosophical debate about the model validity and validation, the uncertainty in model inputs, parameters and future context and the scarcity of field data complicate model validation. This calls for a validation framework and procedures which can identify the strengths and weaknesses of the model with the available data from observations, the literature and experts’ opinions. This paper presents such a framework and the respective procedure. Three tests, namely, Parameter-Verification, Behaviour-Anomaly and Policy-Sensitivity are selected to test a Rapid assessment Model for Coastal-zone Management (RaMCo). The Morris sensitivity analysis, a simple expert elicitation technique and Monte Carlo uncertainty analysis are used to facilitate these three tests. The usefulness of the procedure is demonstrated for two examples.}
}
@article{KNOWLING2020104653,
title = {Disentangling environmental and economic contributions to hydro-economic model output uncertainty: An example in the context of land-use change impact assessment},
journal = {Environmental Modelling & Software},
volume = {127},
pages = {104653},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104653},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219305031},
author = {Matthew J. Knowling and Jeremy T. White and Garry W. McDonald and Joon-Hwan Kim and Catherine R. Moore and Brioch Hemmings},
keywords = {Hydro-economic model, Uncertainty quantification, Decision making, Surface-water/groundwater model, Computable general equilibrium model, Optimization under uncertainty},
abstract = {This paper presents a framework to systematically compare the contributions to uncertainty in hydro-economic simulated outputs from the uncertainty surrounding input parameters employed by the hydrologic and economic models independently. We consider an illustrative case study example. An integrated modeling framework is adopted, involving a surface-water/groundwater nitrate-transport model, and a multi-regional Computable General Equilibrium model. Environmental uncertainty contributions are determined by optimizing nitrate-loading under ecologically-relevant constraint uncertainty at varying risk stances—the results of which are mapped to economic outputs. Economic uncertainty contributions are quantified through Monte-Carlo sampling of variables associated with social-accounting matrices and substitution and transformation elasticities. Results indicate that, at the study-area scale, the environmental contribution to Gross-Regional Product uncertainty is generally larger compared to that of economic uncertainty. Nevertheless, the reliability of hydro-economic outputs is shown to be highly dependent on environmental and economic sources of uncertainty. On the basis of our case study findings, we recommend that commensurate effort be focused toward enhanced assimilation of observation data in both types of models to reduce their respective uncertainties.}
}
@article{JIANG2017274,
title = {Stitching images of dual-cameras onboard satellite},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {128},
pages = {274-286},
year = {2017},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2017.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271616302544},
author = {Yonghua Jiang and Kai Xu and Ruishan Zhao and Guo Zhang and Kan Cheng and Ping Zhou},
keywords = {Image stitching, Geometric calibration, Virtual camera, RPC, Exterior orientation},
abstract = {The way of installing dual-cameras on one satellite is adopted to further enlarge the imaging swath, thereby improving the efficiency of data capturing. In this case, stitching images of dual-cameras with high precision is a key step in the practical application. Due to the inadequate overlapping area of dual-cameras, stitching their images by classic methods may cause internal accuracy loss of the mosaic image. The reason is that classic methods estimate the geometric transformation of dual-cameras merely by a few unevenly distributed precise tie points in overlapping area of dual-cameras, which is similar to the case of using unevenly distributed ground control points (GCPs) in block adjustment. This paper proposed a new method to precisely stitch images of dual cameras without losing internal accuracy. First, a model was built to recover the relative geometric relation of dual-cameras and eliminate Charge-Coupled Device (CCD) distortions of each camera, then a virtual camera model depending on the calibrated geometric relation was adopted to achieve a seamless mosaic image. The panchromatic images of camera A and camera B onboard Yaogan-24 were collected as the experimental data. Experiment results show that the calibration accuracies of dual-cameras are better than 0.3 pixels, and the stitching accuracies can reach the sub-pixel level, ranging from 0.3 to 0.5 pixels. On the other hand, the positioning accuracies with GCPs of the mosaic image and of individual camera are better than 0.6 pixels and 0.5 pixels respectively, so the internal accuracy loss of the mosaic image only reaches 0.1 pixels, which can be neglected. This demonstrates that the proposed method can achieve seamless mosaic images without losing internal accuracy.}
}
@article{LETA2015129,
title = {Assessment of the different sources of uncertainty in a SWAT model of the River Senne (Belgium)},
journal = {Environmental Modelling & Software},
volume = {68},
pages = {129-146},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000596},
author = {Olkeba Tolessa Leta and Jiri Nossent and Carlos Velez and Narayan Kumar Shrestha and Ann {van Griensven} and Willy Bauwens},
keywords = {Uncertainty assessment, Rainfall uncertainty, Stream flow, SWAT, DREAM, River Senne},
abstract = {Although rainfall input uncertainties are widely identified as being a key factor in hydrological models, the rainfall uncertainty is typically not included in the parameter identification and model output uncertainty analysis of complex distributed models such as SWAT and in maritime climate zones. This paper presents a methodology to assess the uncertainty of semi-distributed hydrological models by including, in addition to a list of model parameters, additional unknown factors in the calibration algorithm to account for the rainfall uncertainty (using multiplication factors for each separately identified rainfall event) and for the heteroscedastic nature of the errors of the stream flow. We used the Differential Evolution Adaptive Metropolis algorithm (DREAM(zs)) to infer the parameter posterior distributions and the output uncertainties of a SWAT model of the River Senne (Belgium). Explicitly considering heteroscedasticity and rainfall uncertainty leads to more realistic parameter values, better representation of water balance components and prediction uncertainty intervals.}
}
@article{LUCAS201181,
title = {Updating the Phase 1 habitat map of Wales, UK, using satellite sensor data},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {66},
number = {1},
pages = {81-102},
year = {2011},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2010.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271610000870},
author = {Richard Lucas and Katie Medcalf and Alan Brown and Peter Bunting and Johanna Breyer and Dan Clewley and Steve Keyworth and Philippa Blackmore},
keywords = {Land cover, Habitat classification, Satellite, Wales, Object-oriented eCognition},
abstract = {The Phase 1 Survey is the most comprehensive and widely used national level map of semi-natural habitats in Wales. However, the survey was based largely on field survey and was conducted over several decades, before being completed in 1997. Given that resources for a repeat survey were limited, this study has used an object-orientated rule-based classification implemented within eCognition of multi-temporal satellite sensor data acquired between 2003 and 2006 to map semi-natural habitats and agricultural land across Wales, thereby allowing a progressive update of the Phase 1 Survey. The classification of objects to Phase 1 habitat classes was undertaken in two steps; firstly the landscape of Wales was divided into objects using orthorectified SPOT-5 High Resolution Geometric (HRG) reflectance data (10 m spatial resolution) and Land Parcel Information System (LPIS) boundaries. A rule-base was then developed to progressively discriminate and map the distribution of 105 sub-habitats across Wales based on time-series of SPOT HRG, Terra-1 Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) and Indian Remote Sensing Satellite (IRS) LISS-3 data, derived datasets (e.g., vegetation indices, fractional images) and ancillary information (e.g., topography). The rules coupled knowledge of ecology and the information content of these remote sensing data using a combination of thresholds, Boolean operations and fuzzy membership functions. A second rule-base was then developed to translate the more detailed sub-habitat classification to Phase 1 habitat classes. Indicative accuracies of the revised Phase 1 mapping, based on comparisons with the later Phase 2 survey (for selected habitats), were >80% overall and typically between 70% and 90% for many classes. Through this exercise, Wales has become the first country in Europe to produce a national map of habitats (as opposed to land cover) through object-orientated classification of satellite sensor data. Furthermore, the approach can be adapted to allow continual monitoring of the extent and condition of habitats and agricultural land.}
}
@article{VILLOSLADA201591,
title = {High-displacement flexible Shape Memory Alloy actuator for soft wearable robots},
journal = {Robotics and Autonomous Systems},
volume = {73},
pages = {91-101},
year = {2015},
note = {Wearable Robotics},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2014.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0921889014002115},
author = {A. Villoslada and A. Flores and D. Copaci and D. Blanco and L. Moreno},
keywords = {SMA actuator, Flexible actuator, Rapid control prototyping, Hammerstein–Wiener model, Position and velocity control},
abstract = {This paper describes a flexible Shape Memory Alloy (SMA) actuator designed to increase the limited displacement that these alloys can induce. The SMA actuator has been designed so that it can be bent up to about 180°, providing more freedom of movements and a better integration in wearable robots, specially in soft wearable robots, than standard rigid solutions. Although the actuator length is relatively short, this original design allows a great linear displacement, because it can have one or more loops of the same SMA wire inside the actuator. This implies that the length of the SMA wire is at least two times greater than the length of the actuator. The adopted strategy for both position and speed control that overcomes the hysteresis and prevents overheating the actuator is also described. The control algorithm has been implemented in a rapid control prototyping (RCP) system based on a low cost hardware platform. Finally, the application of this novel actuator in a wrist exoskeleton prototype is shown to demonstrate the feasibility of using the flexible SMA actuator in a real soft wearable robot.}
}
@article{VRUGT2016273,
title = {Markov chain Monte Carlo simulation using the DREAM software package: Theory, concepts, and MATLAB implementation},
journal = {Environmental Modelling & Software},
volume = {75},
pages = {273-316},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215300396},
author = {Jasper A. Vrugt},
keywords = {Bayesian inference, Markov chain Monte Carlo (MCMC) simulation, Random walk metropolis (RWM), Adaptive metropolis (AM), Differential evolution Markov chain (DE-MC), Prior distribution, Likelihood function, Posterior distribution, Approximate Bayesian computation (ABC), Diagnostic model evaluation, Residual analysis, Environmental modeling, Bayesian model averaging (BMA), Generalized likelihood uncertainty estimation (GLUE), Multi-processor computing, Extended metropolis algorithm (EMA)},
abstract = {Bayesian inference has found widespread application and use in science and engineering to reconcile Earth system models with data, including prediction in space (interpolation), prediction in time (forecasting), assimilation of observations and deterministic/stochastic model output, and inference of the model parameters. Bayes theorem states that the posterior probability, p(H|Y˜) of a hypothesis, H is proportional to the product of the prior probability, p(H) of this hypothesis and the likelihood, L(H|Y˜) of the same hypothesis given the new observations, Y˜, or p(H|Y˜)∝p(H)L(H|Y˜). In science and engineering, H often constitutes some numerical model, ℱ(x) which summarizes, in algebraic and differential equations, state variables and fluxes, all knowledge of the system of interest, and the unknown parameter values, x are subject to inference using the data Y˜. Unfortunately, for complex system models the posterior distribution is often high dimensional and analytically intractable, and sampling methods are required to approximate the target. In this paper I review the basic theory of Markov chain Monte Carlo (MCMC) simulation and introduce a MATLAB toolbox of the DiffeRential Evolution Adaptive Metropolis (DREAM) algorithm developed by Vrugt et al. (2008a, 2009a) and used for Bayesian inference in fields ranging from physics, chemistry and engineering, to ecology, hydrology, and geophysics. This MATLAB toolbox provides scientists and engineers with an arsenal of options and utilities to solve posterior sampling problems involving (among others) bimodality, high-dimensionality, summary statistics, bounded parameter spaces, dynamic simulation models, formal/informal likelihood functions (GLUE), diagnostic model evaluation, data assimilation, Bayesian model averaging, distributed computation, and informative/noninformative prior distributions. The DREAM toolbox supports parallel computing and includes tools for convergence analysis of the sampled chain trajectories and post-processing of the results. Seven different case studies illustrate the main capabilities and functionalities of the MATLAB toolbox.}
}
@article{JIANG2010206,
title = {A programming system for sensor-based scientific applications},
journal = {Journal of Computational Science},
volume = {1},
number = {4},
pages = {206-220},
year = {2010},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2010.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877750310000487},
author = {Nanyan Jiang and Manish Parashar},
keywords = {Programming model, Middleware, Sensor-based scientific applications, In-network processing, End-to-end system},
abstract = {Technical advances are leading to a pervasive computational ecosystem that integrates computing infrastructures with embedded sensors and actuators, and are giving rise to a new paradigm for monitoring, understanding, and managing natural and engineered systems – one that is information/data-driven. In this paper, we present a programming system that can support such end-to-end sensor-based dynamic data-driven applications. Specifically, the programming system enables these applications at two levels. First, it provides programming abstractions for integrating sensor systems with computational models for scientific and engineering processes and with other application components in an end-to-end experiment. Second, it provides programming abstractions and system software support for developing in-network data processing mechanisms. The former supports complex querying of the sensor system, while the latter enables development of in-network data processing mechanisms such as aggregation, adaptive interpolation and assimilation. Furthermore, for the latter, we also explore the use of temporal and spatial correlations of sensor measurements in the targeted application domains to tradeoff between the complexity of coordination among sensor clusters and the savings that result from having fewer sensors for in-network processing, while maintaining an acceptable error threshold. The research is evaluated using two application scenarios: the management and optimization of an instrumented oil field and the management and optimization of an instrumented data center. Experimental results show that the provided programming system reduces overheads while achieving near optimal and timely management and control in both application scenarios.}
}
@article{BASU2011329,
title = {Comparing simulation models for market risk stress testing},
journal = {European Journal of Operational Research},
volume = {213},
number = {1},
pages = {329-339},
year = {2011},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2011.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221711001780},
author = {Sanjay Basu},
keywords = {Risk management, Volatility updation, Tail diversification, Simulation models, Fat-tailed distributions},
abstract = {The subprime crisis has reminded us that effective stress tests should not only combine subjective scenarios with historical data, but also be probabilistic. In this paper, we combine three hypothetical shocks, of varying degrees, with more than six years of daily data on USD-INR and Euro-INR. Our objective is to compare six simulation-based stress models for foreign exchange positions. We find that while volatility-weighted historical simulation is the best model for volatility persistence, jump diffusion based Monte Carlo simulation is better at capturing correlation breakdown. Loss estimates from very fat-tailed distributions are not sensitive to the severity of stress scenarios.}
}
@article{BALOG20091,
title = {A language modeling framework for expert finding},
journal = {Information Processing & Management},
volume = {45},
number = {1},
pages = {1-19},
year = {2009},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2008.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306457308000678},
author = {Krisztian Balog and Leif Azzopardi and Maarten {de Rijke}},
keywords = {Expert finding, Language modeling, Expertise search, Intranet search},
abstract = {Statistical language models have been successfully applied to many information retrieval tasks, including expert finding: the process of identifying experts given a particular topic. In this paper, we introduce and detail language modeling approaches that integrate the representation, association and search of experts using various textual data sources into a generative probabilistic framework. This provides a simple, intuitive, and extensible theoretical framework to underpin research into expertise search. To demonstrate the flexibility of the framework, two search strategies to find experts are modeled that incorporate different types of evidence extracted from the data, before being extended to also incorporate co-occurrence information. The models proposed are evaluated in the context of enterprise search systems within an intranet environment, where it is reasonable to assume that the list of experts is known, and that data to be mined is publicly accessible. Our experiments show that excellent performance can be achieved by using these models in such environments, and that this theoretical and empirical work paves the way for future principled extensions.}
}
@article{RAMOS201892,
title = {A data mining framework based on boundary-points for gene selection from DNA-microarrays: Pancreatic Ductal Adenocarcinoma as a case study},
journal = {Engineering Applications of Artificial Intelligence},
volume = {70},
pages = {92-108},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618300150},
author = {Juan Ramos and José A. Castellanos-Garzón and Juan F. de Paz and Juan M. Corchado},
keywords = {Feature selection, Gene selection, Data mining, Cluster analysis, Evolutionary computation, Boundary point, DNA-microarray, Visual analytics, Filter method, Boundary gene},
abstract = {Gene selection (or feature selection) from DNA-microarray data can be focused on different techniques, which generally involve statistical tests, data mining and machine learning. In recent years there has been an increasing interest in using hybrid-technique sets to face the problem of meaningful gene selection; nevertheless, this issue remains a challenge. In an effort to address the situation, this paper proposes a novel hybrid framework based on data mining techniques and tuned to select gene subsets, which are meaningfully related to the target disease conducted in DNA-microarray experiments. For this purpose, the framework above deals with approaches such as statistical significance tests, cluster analysis, evolutionary computation, visual analytics and boundary points. The latter is the core technique of our proposal, allowing the framework to define two methods of gene selection. Another novelty of this work is the inclusion of the age of patients as an additional factor in our analysis, which can leading to gaining more insight into the disease. In fact, the results reached in this research have been very promising and have shown their biological validity. Hence, our proposal has resulted in a methodology that can be followed in the gene selection process from DNA-microarray data.}
}
@article{KOMINAKIS200235,
title = {A preliminary study of the application of artificial neural networks to prediction of milk yield in dairy sheep},
journal = {Computers and Electronics in Agriculture},
volume = {35},
number = {1},
pages = {35-48},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(02)00051-0},
url = {https://www.sciencedirect.com/science/article/pii/S0168169902000510},
author = {A.P. Kominakis and Z. Abas and I. Maltaris and E. Rogdakis},
keywords = {Artificial neural networks, Dairy sheep, Test-day records, Lactation milk yield, Prediction},
abstract = {The aim of this study was to test the usefulness of artificial neural networks (ANNs) for predicting lactation as well as test-day milk yield(s) in Chios dairy sheep on the basis of a few (2–4) available test-day records at the beginning of a lactation period. The ANN employed was a neural network-like system with some advantages over other ANNs. No selection of learning coefficients, of the number of hidden layers or of the number of neurons in the layers was required. The effect on the network's predictive ability of the number of records used in the training phase, the number of input variables (i.e. test-day records) and data preprocessing was investigated. Input variables were the county, herd, lactation, lambing month, litter size, milk yield recorder, test day and days in milk (after lambing) when the first milk sample was obtained. Various criteria of goodness of prediction of lactation as well as of test-day yields were used, including Pearson and rank correlations between observed and predicted yields; the average difference between observed and predicted yields; the difference between their standard deviations; the standard deviation of differences between observed and predicted yields, and the ratio between it and the observed mean value. The average difference between observed and predicted yields was generally statistically non-significant (P<0.05) while predicted standard deviations were underestimated. Values of Pearson and rank correlations between observed and predicted lactation yields ranged from 0.87 to 0.97. In prediction of test-day yields, correlation estimates were generally lower than those obtained in lactation yields and declined as the interval between yields increased. Better predictions were obtained as the number of records used for training increased from 500 to 1000, the number of test-day records increased from 2 to 4, and data preprocessing (i.e. encoding of data) was employed. Training the network for low prediction error of a specific parameter did not improve its overall performance. In contrast, network specialization (i.e. using training data for specific parameters prediction) improved the predictive ability of the parameter in question. Results illustrated the potential effectiveness of ANNs in predicting milk yield in dairy sheep and appeared to justify further pursuit of this research.}
}
@article{TRAN20091386,
title = {Lightweight detection of node presence in MANETs},
journal = {Ad Hoc Networks},
volume = {7},
number = {7},
pages = {1386-1399},
year = {2009},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2009.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570870509000171},
author = {Thi Minh Chau Tran and Björn Scheuermann and Martin Mauve},
keywords = {Presence detection, Mobile ad hoc networks, MANETs, Soft state Bloom filter},
abstract = {While mobility in the sense of node movement has been an intensively studied aspect of mobile ad hoc networks (MANETs), another aspect of mobility has not yet been subjected to systematic research: nodes may not only move around but also enter and leave the network. In fact, many proposed protocols for MANETs exhibit worst case behavior when an intended communication partner is currently not present. Therefore, knowing whether a given node is currently present in the network can often help to avoid unnecessary overhead. In this paper, we present a solution to the presence detection problem. It uses a Bloom filter-based beaconing mechanism to aggregate and distribute information about the presence of network nodes. We describe the algorithm and discuss design alternatives. We assess the algorithm’s properties both analytically and through simulation, and thereby underline the effectiveness and applicability of our approach.}
}
@article{JAFRI2014124,
title = {Design of the coarse-grained reconfigurable architecture DART with on-line error detection},
journal = {Microprocessors and Microsystems},
volume = {38},
number = {2},
pages = {124-136},
year = {2014},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2013.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0141933113002032},
author = {S.M.A.H. Jafri and S.J. Piestrak and O. Sentieys and S. Pillement},
keywords = {Coarse-grained reconfigurable architecture (CGRA), Fault-tolerant system, Reconfigurable system, On-line error detection, Self-checking circuit, Residue code, Arithmetic code, Temporary faults},
abstract = {This paper presents the implementation of the coarse-grained reconfigurable architecture (CGRA) DART with on-line error detection intended for increasing fault-tolerance. Most parts of the data paths and of the local memory of DART are protected using residue code modulo 3, whereas only the logic unit is protected using duplication with comparison. These low-cost hardware techniques would allow to tolerate temporary faults (including so called soft errors caused by radiation), provided that some technique based on re-execution of the last operation is used. Synthesis results obtained for a 90nm CMOS technology have confirmed significant hardware and power consumption savings of the proposed approach over commonly used duplication with comparison. Introducing one extra pipeline stage in the self-checking version of the basic arithmetic blocks has allowed to significantly reduce the delay overhead compared to our previous design.}
}
@article{XIE201887,
title = {Uncertain data classification with additive kernel support vector machine},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {87-97},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16302701},
author = {Zongxia Xie and Yong Xu and Qinghua Hu},
keywords = {Uncertain data, Additive kernel, Support vector machines, Classification},
abstract = {In this work, a classification learning algorithm is designed within the framework of support vector machines through modeling uncertain data with additive kernels, which are introduced to calculate the similarity between uncertain samples characterized by probability density functions (PDFs). The PDFs are used as features of the uncertain samples, where the value of a feature is not a single value, but a set of values that represent the probability distribution of the noise. This is different with the existing methods which represent an uncertain sample by a set of new samples around it, but use the farthest or nearest value in the distribution to construct the optimal hyperplane. With the properties of kernel functions, we can easily extend additive kernels to compute the similarity between samples described with multiple uncertain features. Furthermore, we introduce an efficient algorithm to compute the kernel functions, and solve the additive kernel SVMs. The experimental results show the efficiency of additive-kernel SVMs in uncertain data classification.}
}
@article{SENAPATI2016201,
title = {Modelling heat, water and carbon fluxes in mown grassland under multi-objective and multi-criteria constraints},
journal = {Environmental Modelling & Software},
volume = {80},
pages = {201-224},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216300457},
author = {Nimai Senapati and Per-Erik Jansson and Pete Smith and Abad Chabbi},
keywords = {Modelling heat, water, carbon flux, Multi-objective and multi-criteria constraints, Model performance, Parameter uncertainty},
abstract = {A Monte Carlo-based calibration and uncertainty assessment was performed for heat, water and carbon (C) fluxes, simulated by a soil-plant-atmosphere system model (CoupModel), in mown grassland. Impact of different multi-objective and multi-criteria constraints was investigated on model performance and parameter behaviour. Good agreements between hourly modelled and measurement data were obtained for latent and sensible heat fluxes (R2 = 0.61, ME = 0.48 MJ m−2 day−1), soil water contents (R2 = 0.68, ME = 0.34%) and carbon-dioxide flux (R2 = 0.60, ME = −0.18 g C m−2 day−1). Multi-objective and multi-criteria constraints were efficient in parameter conditioning, reducing simulation uncertainty and identifying critical parameters. Enforcing multi-constraints separately on heat, water and C processes resulted in the highest model improvement for that specific process, including some improvement too for other processes. Imposing multi-constraints on all groups of variables, associated with heat, water and C fluxes together, resulted in general effective parameters conditioning and model improvement.}
}
@article{CRAIG2020104728,
title = {Flexible watershed simulation with the Raven hydrological modelling framework},
journal = {Environmental Modelling & Software},
volume = {129},
pages = {104728},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104728},
url = {https://www.sciencedirect.com/science/article/pii/S136481521931165X},
author = {James R. Craig and Genevieve Brown and Robert Chlumsky and R. Wayne Jenkinson and Georg Jost and Konhee Lee and Juliane Mai and Martin Serrer and Nicholas Sgro and Mahyar Shafii and Andrew P. Snowdon and Bryan A. Tolson},
keywords = {Watershed model, Distributed hydrological modelling, Modelling framework, Hydrology, Object-oriented, Surface water models},
abstract = {A general formulation of a surface water hydrological model is posed that enables multiple numerical schemes, model structures, discretization schemes, and interpolation approaches to be tested, compared, and/or used for assessment of structural uncertainty and algorithm skill. This formulation is the basis for the object-oriented and open source hydrological modelling framework Raven. Raven is uniquely capable of fully emulating and then modifying or extending a number of existing hydrological models of differing design, including the UBC watershed model, GR4J, MOHYSE, HMETS, and HBV-EC. More than 100 compatible process algorithm options and more than 80 interchangeable options for both routing and the estimation of forcings support the testing of trillions of possible model configurations. It supports general spatial discretization, including grids, subbasin/hydrologic response units, lumped, or triangulated irregular network models. Raven’s utility for examining the impact of model choices and sensitivity to structural model uncertainty is demonstrated with a set of simple test cases.}
}
@article{KAO2021628,
title = {Optimal expansion paths for hospitals of different types: Viewpoint of scope economies and evidence from Chinese hospitals},
journal = {European Journal of Operational Research},
volume = {289},
number = {2},
pages = {628-638},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720306366},
author = {Chiang Kao and Rui-Zhi Pang and Shiang-Tai Liu and Xue-Jie Bai},
keywords = {Data envelopment analysis, Economies of scope, Returns to scope},
abstract = {Hospitals have different departments for treating specific diseases. Some hospitals are specialized into one department while others are diversified into many departments. Since many inputs for different departments can be shared, running a diversified hospital is probably less costly than running several specialized hospitals separately due to economies of scope. In this paper, we examine whether expanding the scope of hospitals with a limited number of departments is worthwhile using hospitals in China as the context. The degree of economies of scope is measured in terms of efficiencies gained via a data envelopment analysis. The results show that economies of scope exist for expanding general hospitals lacking one department into general hospitals with all departments. In cases of very specialized hospitals with departments such as dentistry and ophthalmology, it is more efficient to operate them separately. Adding these departments to hospitals with a narrow scope exhibits diseconomies of scope. An expansion network is constructed, with eight optimal paths identified, to guide hospitals of different types to expand their scopes into general comprehensive hospitals stage by stage in a scope-economic way. Since the department to be added is different at each stage of the expansion, the returns to scope do not exhibit a consistent trend for different methods of expansion.}
}
@article{MISHRA2020949,
title = {Adaptive boosting of weak regressors for forecasting of crop production considering climatic variability: An empirical assessment},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {8},
pages = {949-964},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817301726},
author = {Subhadra Mishra and Debahuti Mishra and Gour Hari Santra},
keywords = {AdaBoost, Linear regression, Lasso regression, ridge regression, SVR linear regression, SVR polynomial regression, SVR RBF regression, Crop yield forecasting},
abstract = {Crop yield forecasting based on different climatic conditions for coastal regions is a critical process. In this study, regression based adaptive boosting prediction model is presented, using the datasets of Kharif and Rabi seasons along with the climatic features of three coastal districts belonging to Odisha located in India. This study discusses and experiments on the different weak regressors, such as: linear, lasso, ridge, SVR regression, proposes strong predictors by avoiding the shortcomings of individual weak regressors and propagating the benefits of AdaBoost to improve the predictive accuracy on learning problems. AdaBoost helps to get a combined output of the weak regressors into a weighted sum that represents the final output of the boosted strong regressor and also the output of the weak regressors which is likely to be twisted in favour of wrongly predicted instances adaptively. It has been observed from the experiments that, the decision of weak regressors vary due to frequent, inherent attributes of climatic conditions for crop production. Obtained numerical simulation results in terms of errors, various performance measures and statistical analysis demonstrated have highlighted the attractiveness of the proposed strong regressors compared to weak regressors forecasting methods for crop production.}
}
@article{AOUAM2010151,
title = {Robust strategies for natural gas procurement},
journal = {European Journal of Operational Research},
volume = {205},
number = {1},
pages = {151-158},
year = {2010},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2009.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0377221709009436},
author = {Tarik Aouam and Ronald Rardin and Jawad Abrache},
keywords = {Natural gas, Risk management, Procurement strategies, Supply mix, Stochastic programming},
abstract = {In order to serve their customers, natural gas local distribution companies (LDCs) can select from a variety of financial and non-financial contracts. The present paper is concerned with the choice of an appropriate portfolio of natural gas purchases that would allow a LDC to satisfy its demand with a minimum tradeoff between cost and risk, while taking into account risk associated with modeling error. We propose two types of strategies for natural gas procurement. Dynamic strategies model the procurement problem as a mean-risk stochastic program with various risk measures. Naive strategies hedge a fixed fraction of winter demand. The hedge is allocated equally between storage, futures and options. We propose a simulation framework to evaluate the proposed strategies and show that: (i) when the appropriate model for spot prices and its derivatives is used, dynamic strategies provide cheaper gas with low risk compared to naive strategies. (ii) In the presence of a modeling error, dynamic strategies are unable to control the variance of the procurement cost though they provide cheaper cost on average. Based on these results, we define robust strategies as convex combinations of dynamic and naive strategies. The weight of each strategy represents the fraction of demand to be satisfied following this strategy. A mean–variance problem is then solved to obtain optimal weights and construct an efficient frontier of robust strategies that take advantage of the diversification effect.}
}
@article{TEO2016259,
title = {Do shareholders favor business analytics announcements?},
journal = {The Journal of Strategic Information Systems},
volume = {25},
number = {4},
pages = {259-276},
year = {2016},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S096386871630083X},
author = {Thompson S.H. Teo and Rohit Nishant and Pauline B.L. Koh},
keywords = {Business analytics, Event study, Resource-based theory, Signaling theory},
abstract = {Despite the growing acceptance of business analytics (BA) as a tool for making smarter business decisions, past research has rarely investigated shareholder reactions to BA announcements. We use signaling theory and resource-based theory (RBT) as our theoretical lens. The results show that BA announcements generate positive abnormal returns, thereby providing empirical evidence that shareholders view BA as beneficial. The results also suggest that characteristics that are more salient to shareholders are rewarded. Specifically, firms implementing BA systems from market-leading vendors obtain more positive stock market reactions compared with other firms. Announcements convey more benefits to overbought stocks than oversold stocks, and generate higher positive return in firms with high sales growth and high return on assets (ROA). Overall, empirical evidence favors signaling theory over RBT.}
}
@article{MUKHERJEE2018147,
title = {Optimal design of Shewhart–Lepage type schemes and its application in monitoring service quality},
journal = {European Journal of Operational Research},
volume = {266},
number = {1},
pages = {147-167},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717308032},
author = {Amitava Mukherjee and Rudra Sen},
keywords = {Quality control, Adaptive Lepage–Gastwirth () scheme, Outliers, Service quality, Simultaneous monitoring},
abstract = {We generalize popular distribution-free (nonparametric) Shewhart–Lepage scheme for simultaneously monitoring of location and scale parameters using an adaptive approach. This approach is known as percentile modifications of ranks (or adaptive Gastwirth score) in the statistical literature. This is a powerful tool to improve rank tests to detect a shift in the process. The adaptive Gastwirth score is not much familiar among quality control practitioners and therefore rarely used in practice. Nevertheless, such scores are very useful in detecting various types of shifts in the process characteristics. Considering its distinct advantages, we develop a new class of Shewhart-type adaptive Lepage–Gastwirth (ALG) scheme. We discuss optimal implementation strategies of the proposed scheme to achieve lower out-of-control (OOC) average run length (ARL) and false alarm rate (FAR). This scheme is typically designed to monitor service quality where the reference sample may be non-normal. Post signal follow-up procedures of the proposed Shewhart-type optimal ALG chart is discussed. We illustrate the use of optimal ALG charts with a recent data on Vancouver city call centre service quality monitoring.}
}
@article{STRITIH2019300,
title = {Quantifying uncertainties in earth observation-based ecosystem service assessments},
journal = {Environmental Modelling & Software},
volume = {111},
pages = {300-310},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218300884},
author = {Ana Stritih and Peter Bebi and Adrienne Grêt-Regamey},
keywords = {Ecosystem services, Earth observation, Uncertainty, Bayesian network, Avalanche protection},
abstract = {Ecosystem service (ES) assessments are widely promoted as a tool to support decision-makers in ecosystem management, and the mapping of ES is increasingly supported by the spatial data on ecosystem properties provided by Earth Observation (EO). However, ES assessments are often associated with high levels of uncertainty, which affects their credibility. We demonstrate how different types of information on ES (including EO data, process models, and expert knowledge) can be integrated in a Bayesian Network, where the associated uncertainties are quantified. The probabilistic approach is used to map the provision and demand of avalanche protection, an important regulating service in mountain regions, and to identify the key sources of uncertainty. The model outputs show high uncertainties, mainly due to uncertainties in process modelling. Our results demonstrate that the potential of EO to improve the accuracy of ES assessments cannot be fully utilized without an improved understanding of ecosystem processes.}
}
@article{GLASS2020693,
title = {Spatial scale and product mix economies in U.S. banking with simultaneous spillover regimes},
journal = {European Journal of Operational Research},
volume = {284},
number = {2},
pages = {693-711},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719310768},
author = {Anthony J. Glass and Amangeldi Kenjegaliev and Karligash Kenjegalieva},
keywords = {Productivity and competitiveness, Internal and external returns to scale, Spatial cost function, Branch networks},
abstract = {The literature on bank scale economies focuses on the familiar type of returns to scale that are internal to the firm. Using a spatial approach, we analyze returns to scale for banks that are made up of external (i.e., spillover) economies. We extend ray-scale economies (RSE), expansion-path scale economies (EPSE) and expansion-path subadditivity (EPSU) to the spatial case. This involves introducing direct and composite and decomposed indirect RSE, EPSE and EPSU. These direct and indirect measures relate to the cost implications for a firm from a change in: (i) the firm’s output levels that are, as is standard, under its control; and (ii) the composite/decomposed spillover effect on the firm’s output levels, which is primarily, but not entirely, outside its control. We include an application to U.S. banks (1998–2015) that allows a bank to simultaneously belong to a number of spatial networks, which is typically what we observe for firms. For large banks we find constant direct RSE and EPSE, and zero composite indirect RSE and constant composite indirect EPSE. These composite indirect results do not counteract any policy suggestions from the direct RSE and EPSE concerning the debate on whether there should be size caps on very large U.S. banks. The direct RSE and EPSE for large banks suggest that these banks use society’s resources efficiently to provide their services. Size caps on very large banks would place downward pressure on these direct RSE and EPSE results, which could lead to large banks using society’s resources inefficiently.}
}
@article{WANG2018756,
title = {Multi label text classification method based on co-occurrence latent semantic vector space},
journal = {Procedia Computer Science},
volume = {131},
pages = {756-764},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918307014},
author = {Rujuan Wang and Gang Chen and Xin Sui},
keywords = {Random Forest (RF), Multi label text, Co-Occurrence Latent Semantic Vector Space (CLSVS)},
abstract = {Aiming at the problem of conceptual ambiguity and underlying semantic structure of multi label text categorization, an ensemble classification method is proposed, which combines random forest (RF) algorithm and semantic core co-occurrence latent semantic vector space (CLSVSM). Through the random segmentation of words, the diversity of integration is increased, and the different orthogonal projection of low dimensional implicit semantic space is obtained. Random forest can effectively solve binary classification problem, and implicit semantics reveals the underlying semantic structure of text. The combination of them can represent the diversity and accuracy of individuals. The experimental results on Yahoo dataset demonstrate the effectiveness of the proposed method, which is superior to other methods in Hamming loss, coverage, first error and average accuracy.}
}
@article{PASCOA201615,
title = {Exploratory study on vineyards soil mapping by visible/near-infrared spectroscopy of grapevine leaves},
journal = {Computers and Electronics in Agriculture},
volume = {127},
pages = {15-25},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2016.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916303064},
author = {R.N.M.J. Páscoa and M. Lopo and C.A. {Teixeira dos Santos} and A.R. Graça and J.A. Lopes},
keywords = {Near infrared spectroscopy, Soils, Vineyards, Leaves, Precision agriculture, Chemometrics},
abstract = {This work demonstrates the possibility of discriminating vineyard soils through the non-destructive and in-situ visible/near infrared monitoring of leaves. A portable Vis/NIR spectrometer was applied for monitoring in-situ Vitis vinifera leaves in vineyards of two wine regions in Portugal in the maturation period. Leaves reflectance spectra of different grapevine varieties planted in different vineyard locations (distinct soil taxonomic types) were analyzed by principal component analysis and partial least squares discriminant analysis. Soil discriminant models based on leaves Vis/NIR spectra yielded for both vineyards approximately 95% correct soil taxonomic predictions. This methodology was then applied to monitor all plants within a 0.3ha vineyard block in the Dão vineyard resulting in a highly detailed soil taxonomic map built exclusively from leaves Vis/NIR spectra. A comparison with the existing soil map proved that the NIR spectroscopy based estimation was very similar. Even though further studies are needed, namely in different maturation stages and other geographical regions, to ensure reliability of this technique, results in this work showed that it can be used as an additional auxiliary tool for obtaining vineyard soil maps. Its main advantages over pedological reference procedures are speed and cost efficiency analysis.}
}
@article{HASHMI2018230,
title = {Statistical analysis and performance evaluation of optical array receivers for deep-space optical communications under random tracking errors},
journal = {Physical Communication},
volume = {31},
pages = {230-238},
year = {2018},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1874490717305888},
author = {Ali J. Hashmi and Ali A. Eftekhar and Ali Adibi and Farid Amoozegar},
abstract = {Telescope array receiver is a viable architecture for Earth-based reception in a deep-space optical communication system. In this paper, effects of random tracking errors on the performance of optical array receivers for an inter-planetary deep-space optical communications link between Earth and Mars is investigated. The paper has two major parts. In the first part, statistical analysis and mathematical modeling of the impact of tracking errors on general direct-detection optical communication receivers is presented. The analytical results show that tracking errors could severely degrade the performance of optical receivers; hence, these need to be compensated, especially in a deep-space link. In the second part, design and analysis of a closed-loop tracking subsystem for telescope array receivers operating in a deep-space link is presented. An end-to-end simulation platform for communication between Earth and Mars is implemented that incorporates direct-detection array receivers and the proposed tracking subsystems to alleviate the effects of random tracking errors. Extreme channel conditions, i.e., maximum distance, strong background noise and turbulence conditions are modeled to evaluate the performance bounds. Simulations results depict that in worst-case channel conditions, the proposed architecture mitigates the impact of tracking errors to be within reasonable limits. Comparison of achievable data rates show that in the presence of random tracking errors, replacement of a large telescope (10m diameter) with an array of relatively smaller-sized telescopes (i.e., 100, 1m telescopes) results in acceptable performance loss (i.e., <13%). Hence, performance degradation due to tracking errors does not pose a major limitation in employing array architecture in deep-space communication. The presented analysis further strengthens the viability of array architecture compared to a monolithic, large telescope for deep-space communications.}
}
@article{WANG201730,
title = {Characterizing Android apps’ behavior for effective detection of malapps at large scale},
journal = {Future Generation Computer Systems},
volume = {75},
pages = {30-45},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17307720},
author = {Xing Wang and Wei Wang and Yongzhong He and Jiqiang Liu and Zhen Han and Xiangliang Zhang},
keywords = {Android, Malicious apps detection, Feature comparison},
abstract = {Android malicious applications (malapps) have surged and been sophisticated, posing a great threat to users. How to characterize, understand and detect Android malapps at a large scale is thus a big challenge. In this work, we are motivated to discover the discriminatory and persistent features extracted from Android APK files for automated malapp detection at a large scale. To achieve this goal, firstly we extract a very large number of features from each app and categorize the features into two groups, namely, app-specific features as well as platform-defined features. These feature sets will then be fed into four classifiers (i.e., Logistic Regression, linear SVM, Decision Tree and Random Forest) for the detection of malapps. Secondly, we evaluate the persistence of app-specific and platform-defined features on classification performance with two data sets collected in different time periods. Thirdly, we comprehensively analyze the relevant features selected by Logistic Regression classifier to identify the contributions of each feature set. We conduct extensive experiments on large real-world app sets consisting of 213,256 benign apps collected from six app markets, 4,363 benign apps from Google Play market, and 18,363 malapps. The experimental results and our analysis give insights regarding what discriminatory features are most effective to characterize malapps for building an effective and efficient malapp detection system. With the selected discriminatory features, the Logistic Regression classifier yields the best true positive rate as 96% with a false positive rate as 0.06%.}
}
@article{GIATRAKOS2020587,
title = {Omnibus outlier detection in sensor networks using windowed locality sensitive hashing},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {587-609},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17319180},
author = {Nikos Giatrakos and Antonios Deligiannakis and Minos Garofalakis and Yannis Kotidis},
keywords = {Sensor network, Outlier, Locality sensitive hashing, Streaming window model, Similarity estimation},
abstract = {Wireless Sensor Networks (WSNs) have become an integral part of cutting edge technological paradigms such as the Internet-of-Things (IoT) which incorporates a variety of smart application scenarios. WSNs include tiny sensors (motes), with constrained hardware capabilities and limited power supply that can collaboratively function in an unsupervised manner for a long period of time. Their purpose is to continuously monitor quantities of interest and provide answers to application queries. Sensor data streams are inherently spatiotemporal in nature, both because mote measurements form multidimensional time series and due to the spatial reference on the data based on the realm sensed by a mote. Motes are designed to be inexpensive, and thus sensory hardware is prone to temporary or permanent failures yielding faulty measurements. Such measurements may unpredictably forge a query answer, while truthful but abnormal mote samples may indicate undergoing phenomena. Therefore, outlier detection in sensor networks is of utmost importance. With limited power supply and communication being by far the main culprit in energy drain, outlier detection techniques in WSNs should achieve appropriate balance between reducing communication and providing real-time, continuously updated outlier reports. Prior works employ probabilistic or best effort approaches to accomplish the task, which either unpredictably compromise outlier detection accuracy or fail to explicitly tune the amount of communicated data. In this work, we introduce an omnibus outlier detection solution over spatiotemporally referenced sensor data that is capable of: (a) directly trading communication reduction for outlier detection quality with predictable accuracy guarantees, (b) accommodating both uni- and multi-dimensional outlier definitions, (c) operating under various streaming window models and (d) incorporating a wide variety of similarity measures to judge outliers.}
}
@article{VILBERGSDOTTIR201418,
title = {Assessing the reliability, validity and acceptance of a classification scheme of usability problems (CUP)},
journal = {Journal of Systems and Software},
volume = {87},
pages = {18-37},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213002136},
author = {Sigurbjorg Groa Vilbergsdottir and Ebba Thora Hvannberg and Effie Lai-Chong Law},
keywords = {Usability problems, Defect classification, Validity},
abstract = {The aim of this study was to evaluate the Classification of Usability Problems (CUP) scheme. The goal of CUP is to classify usability problems further to give user interface developers better feedback to improve their understanding of usability problems, help them manage usability maintenance, enable them to find effective fixes for UP, and prevent such problems from reoccurring in the future. First, reliability was evaluated with raters of different levels of expertise and experience in using CUP. Second, acceptability was assessed with a questionnaire. Third, validity was assessed by developers in two field studies. An analytical comparison was also made to three other classification schemes. CUP reliability results indicated that the expertise and experience of raters are critical factors for assessing reliability consistently, especially for the more complex attributes. Validity analysis results showed that tools used by developers must be tailored to their working framework, knowledge and maturity. The acceptability study showed that practitioners are concerned with the effort spent in applying any tool. To understand developers’ work and the implications of this study two theories are presented for understanding and prioritising UP. For applying classification schemes, the implications of this study are that training and context are needed.}
}
@article{KHAYAM2007298,
title = {Markov and multifractal wavelet models for wireless MAC-to-MAC channels},
journal = {Performance Evaluation},
volume = {64},
number = {4},
pages = {298-314},
year = {2007},
issn = {0166-5316},
doi = {https://doi.org/10.1016/j.peva.2006.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166531606000538},
author = {Syed A. Khayam and Hayder Radha and Selin Aviyente and J.R. Deller},
keywords = {Channel model, Bit-errors, 802.11b networks, Long-range dependence, Markov chains},
abstract = {Statistical understanding of bit-errors above the physical layer facilitates the design and verification of wireless protocols and applications. In this paper, we provide analysis and modeling of bit-errors at the 802.11b MAC layer. We show that MAC-to-MAC bit-errors at 2 and 5.5 Mbps are Markov, whereas those at 11 Mbps are long-range dependent (LRD). For the 2 and 5.5 Mbps channels, we observe that low-complexity hierarchical and hidden Markov models cannot characterize the bit-error processes, and consequently high-order full-state Markov chains are employed for accurate channel characterization. A multifractal wavelet model is employed to accurately capture the LRD 11 Mbps bit-error process.}
}
@article{KUCZYNSKI2004256,
title = {Hidden Markov Modeling of Error Patterns and Soft Outputs for Simulation of Wideband CDMA Transmission Systems},
journal = {AEU - International Journal of Electronics and Communications},
volume = {58},
number = {4},
pages = {256-267},
year = {2004},
issn = {1434-8411},
doi = {https://doi.org/10.1078/1434-8411-54100241},
url = {https://www.sciencedirect.com/science/article/pii/S1434841104702319},
author = {Peter Kuczynski and Arnaud Rigollé and Wolfgang H. Gerstacker and Johannes B. Huber},
keywords = {Channel modeling, Hidden Markov models, CDMA},
abstract = {Summary
In this paper, a Hidden Markov Modeling (HMM) technique for a fast and accurate simulation of bit errors and soft outputs in wireless communication systems is presented. HMMs with continuous probability distributions are considered. Soft outputs and bit errors are combined to error patterns. We focus on binary phase–shift keying (BPSK) modulation for direct–sequence spread spectrum (code–division multiple access, CDMA) transmission as proposed e.g. ∼for the third generation wireless communication system UMTS (uplink for the frequency division duplex mode (FDD)). Comparisons of simulated bit error rates for HMM models and Rake receivers are shown for AWGN, flat fading, and vehicular channel conditions. In order to assess the ability of the HMM to describe the dynamical behaviour of the channel a comparison for transmission with interleaving and convolutional coding is presented. Furthermore calculated autocorrelation functions of the error patterns and error gap distributions corresponding to the Rake receiver and to the HMM, respectively, are presented. Our investigations show a strong dependence of the required HMM order on Eb/N0 and the channel conditions. The degree of accordance of the HMM outputs and the training data is examined based on calculated statistical scoring indicators.}
}
@article{CHEN201263,
title = {Recursive robust least squares support vector regression based on maximum correntropy criterion},
journal = {Neurocomputing},
volume = {97},
pages = {63-73},
year = {2012},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2012.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0925231212003785},
author = {Xiaobo Chen and Jian Yang and Jun Liang and Qiaolin Ye},
keywords = {Support vector machine, Correntropy, Robust regression, Particle swarm optimization},
abstract = {Least squares support vector machine for regression (LSSVR) is an efficient method for function estimation problem. However, its solution is prone to large noise and outliers since it depends on the minimum of the sum of squares error (SSE) on training samples. To tackle this problem, in this paper, a novel regression model termed as recursive robust LSSVR (R2LSSVR) is proposed to obtain robust estimation for data in the presence of outliers. The idea is to build a regression model in the kernel space based on maximum correntropy criterion and regularization technique. An iterative algorithm derived from half-quadratic optimization is further developed to solve R2LSSVR with theoretically guaranteed convergence. It also reveals that R2LSSVR is closely related to the original LSSVR since it essentially solves adaptive weighted LSSVR iteratively. Furthermore, a hyperparameters selection method for R2LSSVR is presented based on particle swarm optimization (PSO) such that multiple hyperparameters in R2LSSVR can be estimated effectively for better performance. The feasibility of this method is examined on some simulated and benchmark datasets. The experimental results demonstrate the good robust performance of the proposed method.}
}
@article{XU2015124,
title = {Data-driven methods to improve baseflow prediction of a regional groundwater model},
journal = {Computers & Geosciences},
volume = {85},
pages = {124-136},
year = {2015},
note = {Statistical learning in geoscience modelling: Novel algorithms and challenging case studies},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2015.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0098300415001284},
author = {Tianfang Xu and Albert J. Valocchi},
keywords = {Statistical learning, Baseflow, Predictive error},
abstract = {Physically‐based models of groundwater flow are powerful tools for water resources assessment under varying hydrologic, climate and human development conditions. One of the most important topics of investigation is how these conditions will affect the discharge of groundwater to rivers and streams (i.e. baseflow). Groundwater flow models are based upon discretized solution of mass balance equations, and contain important hydrogeological parameters that vary in space and cannot be measured. Common practice is to use least squares regression to estimate parameters and to infer prediction and associated uncertainty. Nevertheless, the unavoidable uncertainty associated with physically‐based groundwater models often results in both aleatoric and epistemic model calibration errors, thus violating a key assumption for regression-based parameter estimation and uncertainty quantification. We present a complementary data-driven modeling and uncertainty quantification (DDM-UQ) framework to improve predictive accuracy of physically‐based groundwater models and to provide more robust prediction intervals. First, we develop data-driven models (DDMs) based on statistical learning techniques to correct the bias of the calibrated groundwater model. Second, we characterize the aleatoric component of groundwater model residual using both parametric and non-parametric distribution estimation methods. We test the complementary data-driven framework on a real-world case study of the Republican River Basin, where a regional groundwater flow model was developed to assess the impact of groundwater pumping for irrigation. Compared to using only the flow model, DDM-UQ provides more accurate monthly baseflow predictions. In addition, DDM-UQ yields prediction intervals with coverage probability consistent with validation data. The DDM-UQ framework is computationally efficient and is expected to be applicable to many geoscience models for which model structural error is not negligible.}
}
@article{POELMANS201017,
title = {Complexity and performance of urban expansion models},
journal = {Computers, Environment and Urban Systems},
volume = {34},
number = {1},
pages = {17-27},
year = {2010},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2009.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0198971509000453},
author = {Lien Poelmans and Anton {Van Rompaey}},
keywords = {Urban expansion, Belgium, Logistic regression, Cellular automata, Validation},
abstract = {Urban expansion and spatial patterns of urban land have a large effect on many socioeconomic and environmental processes. A wide variety of modelling approaches has been introduced to predict and simulate future urban development. These models are often based on the interpretation of various determining factors that are used to create a probability map. The main objective of this paper is to evaluate the performance of different modelling approaches for simulating spatial patterns of urban expansion in Flanders and Brussels in the period 1988–2000. Hereto, a set of urban expansion models with increasing complexity was developed based on: (i) logistic regression equations taking various numbers of determining variables into account, (ii) CA transition rules and (iii) hybrid procedures, combining both approaches. The outcome of each model was validated in order to assess the predictive value of the three modelling approaches and of the different determining variables that were used in the logistic regression models. The results show that a hybrid model structure, integrating (static) determining factors (distance to the main roads, distance to the largest cities, employment potential, slope and zoning status of the land) and (dynamic) neighbourhood interactions produces the most accurate probability map. The study, however, points out that it is not useful to make a statement on the validity of a model based on only one goodness-of-fit measure. When the model results are validated at multiple resolutions, the logistic regression model, which incorporates only two explanatory variables, outperforms both the CA-based model and the hybrid model.}
}
@article{RINDERKNECHT2014300,
title = {The effect of ambiguous prior knowledge on Bayesian model parameter inference and prediction},
journal = {Environmental Modelling & Software},
volume = {62},
pages = {300-315},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002448},
author = {Simon L. Rinderknecht and Carlo Albert and Mark E. Borsuk and Nele Schuwirth and Hans R. Künsch and Peter Reichert},
keywords = {Intersubjective knowledge, Imprecise probabilities, Interval probabilities, Robust Bayesian analysis, Density Ratio Class, Bayesian inference, Marginalization and prediction},
abstract = {Environmental modeling often requires combining prior knowledge with information obtained from data. The robust Bayesian approach makes it possible to consider ambiguity in this prior knowledge. Describing such ambiguity using sets of probability distributions defined by the Density Ratio Class has important conceptual advantages over alternative robust formulations. Earlier studies showed that the Density Ratio Class is invariant under Bayesian inference and marginalization. We prove that (i) the Density Ratio Class is also invariant under propagation through deterministic models, whereas (ii) predictions of a stochastic model with parameters defined by a Density Ratio Class are embedded in a Density Ratio Class. These invariance properties make it possible to describe sequential learning and prediction under a unified framework. We developed numerical algorithms to minimize the additional computational burden relative to the use of single priors. Practical feasibility of these methods is demonstrated by their application to a simple ecological model.}
}
@article{PRIYADARSHINEE2017341,
title = {Understanding and predicting the determinants of cloud computing adoption: A two staged hybrid SEM - Neural networks approach},
journal = {Computers in Human Behavior},
volume = {76},
pages = {341-362},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217304491},
author = {Pragati Priyadarshinee and Rakesh D. Raut and Manoj Kumar Jha and Bhaskar B. Gardas},
keywords = {Cloud computing adoption, Risk analysis model, SEM, ANN, Indian industries},
abstract = {Along with water, gas, electricity, and telephone, cloud computing has been considered as the fifth utility. Like other utility services available in today's social computing services are readily available on demand (Buyya, Yeo, Venugopal, Broberg, & Brandic, 2009). The purpose of the study is to develop a hybrid two-stage, structural equation modeling (SEM) – artificial neural network (ANN) model to predict motivators affecting cloud computing adoption services in the Indian private organizations. This research article proposes a new paradigm by extending the Technology Organization Environment Model (TOE) with external factors, namely, perceived IT security risk and risk analysis for the first time in a technology adoption study. One of the core contributions of the study is the introduction of new factors, perceived IT security risk and risk analysis. Data were collected from 660 professional experts and analyzed using structural equation modeling (SEM) and artificial neural network (ANN) modeling. The SEM results showed that perceived IT security risk (PITR), risk analysis (RA), technology innovation (TI), management style (MS) and trust (T) have a significant influence on cloud computing adoption. The only exceptions were the usage of technology (UT) and industry usage (IU) which witnessed statistically insignificant influence on cloud computing adoption. Furthermore, the results obtained from SEM were employed as input to the artificial neural network (ANN) model and results showed that ‘trust’, ‘perceived IT security risk’, and ‘management style’ as most important predictors in cloud computing adoption.}
}
@article{KULK2008136,
title = {Quantifying requirements volatility effects},
journal = {Science of Computer Programming},
volume = {72},
number = {3},
pages = {136-175},
year = {2008},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2008.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642308000464},
author = {G.P. Kulk and C. Verhoef},
keywords = {Requirements volatility, IT portfolio management, Quantitative IT portfolio management, Volatility benchmark, IT dashboard, Requirements metric, Requirements creep, Scope creep, Requirements scrap, Requirements churn, Compound monthly growth rate, Volatility tolerance factor, -ratio, -ratio, Requirements volatility dashboard},
abstract = {In an organization operating in the bancassurance sector we identified a low-risk IT subportfolio of 84 IT projects comprising together 16,500 function points, each project varying in size and duration, for which we were able to quantify its requirements volatility. This representative portfolio stems from a much larger portfolio of IT projects. We calculated the volatility from the function point countings that were available to us. These figures were aggregated into a requirements volatility benchmark. We found that maximum requirements volatility rates depend on size and duration, which refutes currently known industrial averages. For instance, a monthly growth rate of 5% is considered a critical failure factor, but in our low-risk portfolio we found more than 21% of successful projects with a volatility larger than 5%. We proposed a mathematical model taking size and duration into account that provides a maximum healthy volatility rate that is more in line with the reality of low-risk IT portfolios. Based on the model, we proposed a tolerance factor expressing the maximal volatility tolerance for a project or portfolio. For a low-risk portfolio its empirically found tolerance is apparently acceptable, and values exceeding this tolerance are used to trigger IT decision makers. We derived two volatility ratios from this model, the π-ratio and the ρ-ratio. These ratios express how close the volatility of a project has approached the danger zone when requirements volatility reaches a critical failure rate. The volatility data of a governmental IT portfolio were juxtaposed to our bancassurance benchmark, immediately exposing a problematic project, which was corroborated by its actual failure. When function points are less common, e.g. in the embedded industry, we used daily source code size measures and illustrated how to govern the volatility of a software product line of a hardware manufacturer. With the three real-world portfolios we illustrated that our results serve the purpose of an early warning system for projects that are bound to fail due to excessive volatility. Moreover, we developed essential requirements volatility metrics that belong on an IT governance dashboard and presented such a volatility dashboard.}
}
@article{BIGUN1995599,
title = {Risk analysis of catastrophes using experts' judgements: An empirical study on risk analysis of major civil aircraft accidents in Europe},
journal = {European Journal of Operational Research},
volume = {87},
number = {3},
pages = {599-612},
year = {1995},
note = {Operational Research in Europe},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(95)00233-2},
url = {https://www.sciencedirect.com/science/article/pii/0377221795002332},
author = {Elizabeth Saers Bigün},
keywords = {Aggregating opinions, Aircraft accidents, Bayesian methods, Calibration, Catastrophes, Decision analysis, Expert's opinions, Probability assessments, Reconciliation, Risk analysis, Subjective probabilities, Knowledge representation},
abstract = {We present an empirical study performed on risk analysis of major civil aircraft accidents in Europe by using expert judgements. The main goal is to investigate in practice some theoretical models which have been constructed previously. These models calibrate and aggregate judgements of the experts in order to predict the future risks. The statistical tools which are used in this work are Bayesian. The main results of this pilot study are: (i) The models which we used in order to predict the future risks seem to work satisfactorily. (ii) Calibration of the experts' assessed risks are needed. (iii) We introduced two different methods to estimate the assessment error variances of the experts. The first method is built on probability assessments and the second method on interval assessments. The differences between these two methods are in general not remarkably prominent. (iv) All individual distributions are positively skewed. The aggregated distributions are less skewed.}
}
@article{LITT20131649,
title = {Understanding social network site users’ privacy tool use},
journal = {Computers in Human Behavior},
volume = {29},
number = {4},
pages = {1649-1656},
year = {2013},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213000526},
author = {Eden Litt},
keywords = {Social network sites, Privacy, Reputation, Self-presentation, Privacy management, Digital inequality},
abstract = {Every day hundreds of millions of people log into social network sites and deposit terabytes of data as they share status updates, photographs, and more. This article explores how background factors, motivations, and social network site experiences relate to people’s use of social network site technology to protect their privacy. The findings indicate that during technology-mediated communication on social network sites, not only do traditional privacy factors relate to the technological boundaries people enact, but people’s experiences with the mediating technology itself do, too. The results also identify privacy inequalities, in which certain groups are more likely to take advantage of the technology to protect their privacy—suggesting that some individuals’ information and reputations may be more at risk than others’.}
}
@article{KAECK2020767,
title = {VIX derivatives, hedging and vol-of-vol risk},
journal = {European Journal of Operational Research},
volume = {283},
number = {2},
pages = {767-782},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719309452},
author = {Andreas Kaeck and Norman J. Seeger},
keywords = {Risk management, VIX options, Stochastic volatility of volatility, Hedging performance,},
abstract = {We study the empirical hedging performance of alternative VIX option pricing models. Recent advances in the literature find evidence of asymmetric volatility-of-volatility (similar to the leverage effect in equity markets), stochastic mean-reversion and jumps. Using such findings in our model framework, we show that while sophisticated models have superior pricing performance and can explain a range of stylized facts in the VIX derivatives market, their hedging performance is inferior to a simple Black model hedge. We also study the empirical performance of regime-dependent hedge ratio adjustments commonly applied in equity markets.}
}
@article{MARTIN2001237,
title = {Application of a hybrid process simulation model to a software development project},
journal = {Journal of Systems and Software},
volume = {59},
number = {3},
pages = {237-246},
year = {2001},
note = {Software Process Simulation Modeling},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(01)00065-6},
url = {https://www.sciencedirect.com/science/article/pii/S0164121201000656},
author = {Robert Martin and David Raffo},
keywords = {Simulation, Simulation modelling, Software process, Software process modeling},
abstract = {Simulation models of the software development process can be used to evaluate potential process changes. Careful evaluation should consider the change within the context of the project environment. While system dynamics models have been used to model the project environment, discrete event and state-based models are more useful when modeling process activities. Hybrid models of the software development process can examine questions that cannot be answered by either system dynamics models or discrete event models alone. In this paper, we present a detailed hybrid model of a software development process currently in use at a major industrial developer. We describe the model and show how the model was used to evaluate simultaneous changes to both the process and the project environment.}
}
@article{PARRY2013104,
title = {A Bayesian sensitivity analysis applied to an Agent-based model of bird population response to landscape change},
journal = {Environmental Modelling & Software},
volume = {45},
pages = {104-115},
year = {2013},
note = {Thematic Issue on Spatial Agent-Based Models for Socio-Ecological Systems},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212002265},
author = {Hazel R. Parry and Christopher J. Topping and Marc C. Kennedy and Nigel D. Boatman and Alistair W.A. Murray},
keywords = {Agent-based model, ALMaSS, Land use policy, Sensitivity analysis, Set-aside removal, Skylarks, Emulator, Meta-model, Uncertainty, BACCO},
abstract = {Agricultural land management has important impacts on land use and vegetation that can rapidly induce ecosystem change. Birds are often used as indicators of such impacts of landscape change on ecosystems. However, predicting the response of birds to changes in their environment is an ongoing challenge. Agent-based models (ABMs) have the potential to provide useful insights but have not been widely used in such studies to date. This paper illustrates the use of agent-based modelling for policy decision-making, using the case study of the impacts of the removal of set-aside land on Skylark populations in Denmark. In order to address the importance of critical interpretation of ABMs, we introduce a novel methodology with which to analyze the sensitivity of an ABM, Bayesian Analysis of Computer Code Outputs (BACCO). BACCO constructs an emulator of the model in order to provide a rapid and thorough sensitivity analysis. This allows us to identify input parameters in the model that require more rigorous parameterization, as some parameters are highly sensitive and are found to produce spurious results when varied even a small amount.}
}
@article{DUTTA2018596,
title = {Product graph-based higher order contextual similarities for inexact subgraph matching},
journal = {Pattern Recognition},
volume = {76},
pages = {596-611},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317304892},
author = {Anjan Dutta and Josep Lladós and Horst Bunke and Umapada Pal},
keywords = {Subgraph matching, Product graph, Random walks, Backtrackless walks, Contextual similarities, Graphic recognition},
abstract = {Many algorithms formulate graph matching as an optimization of an objective function of pairwise quantification of nodes and edges of two graphs to be matched. Pairwise measurements usually consider local attributes but disregard contextual information involved in graph structures. We address this issue by proposing contextual similarities between pairs of nodes. This is done by considering the tensor product graph (TPG) of two graphs to be matched, where each node is an ordered pair of nodes of the operand graphs. Contextual similarities between a pair of nodes are computed by accumulating weighted walks (normalized pairwise similarities) terminating at the corresponding paired node in TPG. Once the contextual similarities are obtained, we formulate subgraph matching as a node and edge selection problem in TPG. We use contextual similarities to construct an objective function and optimize it with a linear programming approach. Since random walk formulation through TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities and better discrimination among the nodes and edges. Experimental results shown on synthetic as well as real benchmarks illustrate that higher order contextual similarities increase discriminating power and allow one to find approximate solutions to the subgraph matching problem.}
}
@article{DEBRUIN2005828,
title = {Stochastic simulation of large grids using free and public domain software},
journal = {Computers & Geosciences},
volume = {31},
number = {7},
pages = {828-836},
year = {2005},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2005.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0098300405000257},
author = {S. {de Bruin} and A.J.W. {de Wit}},
keywords = {Geostatistics, Sequential indicator simulation, Bash script, Gslib, Quadtree},
abstract = {This paper proposes a tiled map procedure enabling sequential indicator simulation on grids consisting of several tens of millions of cells, without putting excessive memory requirements. Spatial continuity across map tiles is handled by conditioning adjacent tiles on their shared boundaries. Tiles across the area can be characterized by dissimilar models of spatial continuity (semi-variograms) thus relieving the requirement of a global stationarity decision. Additionally, the approach provides a simple mechanism for reseeding the pseudo random number generator. Implementation of the algorithm involved small modifications to a GSLIB program and Bash and awk scripting. The software was stable on several platforms, including 32-bit systems with a 4Gb memory addressing limit. In an experiment we simulated 25 realizations of a 11,274×13,000 grid representing local uncertainty in the Dutch land cover at 25m resolution. With the objective of mimicking the typical absence of well-distributed hard reference data, the simulations were only conditioned on local prior class probabilities and semi-variograms. Output was evaluated on the basis of reproduction of target levels of (1) cover type proportions, (2) overall class label accuracy and (3) spatially averaged local Shannon entropy. As expected, the realized statistics differed significantly from the target levels. However, the differences were consistent over the borders and the insides of map tiles. Thus, they did not result from the tiled map procedure but rather should be attributed to the used semi-conditional sequential indicator simulator. The current implementation can easily be adapted to accept other simulation algorithms.}
}
@article{MENENDEZ2012639,
title = {A replicated field intervention study evaluating the impact of a highly adjustable chair and office ergonomics training on visual symptoms},
journal = {Applied Ergonomics},
volume = {43},
number = {4},
pages = {639-644},
year = {2012},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2011.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0003687011001517},
author = {Cammie Chaumont Menéndez and Benjamin C. Amick and Michelle Robertson and Lianna Bazzani and Kelly DeRango and Ted Rooney and Anne Moore},
keywords = {Office ergonomics intervention, Chair, Training, Visual symptoms, Replication, Multi-level modeling},
abstract = {Objective
Examine the effects of two office ergonomics interventions in reducing visual symptoms at a private sector worksite.
Methods
A quasi-experimental study design evaluated the effects of a highly adjustable chair with office ergonomics training intervention (CWT group) and the training only (TO group) compared with no intervention (CO group). Data collection occurred 2 and 1 month(s) pre-intervention and 2, 6 and 12 months post-intervention. During each data collection period, a work environment and health questionnaire (covariates) and daily health diary (outcomes) were completed. Multilevel statistical models tested hypotheses.
Results
Both the training only intervention (p<0.001) and the chair with training intervention (p=0.01) reduced visual symptoms after 12 months.
Conclusion
The office ergonomics training alone and coupled with a highly adjustable chair reduced visual symptoms. In replicating results from a public sector worksite at a private sector worksite the external validity of the interventions is strengthened, thus broadening its generalizability.}
}
@article{ADAMS2007499,
title = {Light vehicle fuelling errors in the UK: The nature of the problem, its consequences and prevention},
journal = {Applied Ergonomics},
volume = {38},
number = {5},
pages = {499-511},
year = {2007},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2006.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S000368700600158X},
author = {P. Adams and G.C. David},
keywords = {Error, Vehicles, Fuelling},
abstract = {Errors arising during the fuelling of light vehicles are increasing. It is estimated that around 300,000 misfuellings occurred in the UK in 2001 alone, with direct costs of approximately £35 million and considerable inconvenience caused to all those involved. This study has investigated the causes of fuelling errors. A hierarchical task analysis of the fuelling of light vehicles was developed and data gathered from 23 individuals who had misfuelled. Errors were found to have occurred because the physical and psychological conditions on the filling station forecourt presented sufficient opportunity for an error producing sequence of events to be triggered. These occurred when specific factors were present either singly or in combination in the fuelling environment, creating an error ‘pathway’. The probability of an error occurring is dependent upon the strength and type of influence these factors have on the performance of the fuelling task. It is proposed that errors are best prevented by applying ergonomic principles to the design, and/or modification of filling station layout and equipment. In this way, the error pathways may be broken and successful fuelling encouraged.}
}
@article{VILKKUMAA2014772,
title = {Optimal strategies for selecting project portfolios using uncertain value estimates},
journal = {European Journal of Operational Research},
volume = {233},
number = {3},
pages = {772-783},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221713007789},
author = {Eeva Vilkkumaa and Juuso Liesiö and Ahti Salo},
keywords = {Decision analysis, Portfolio selection, Bayesian methods, Value of information},
abstract = {Practically all organizations seek to create value by selecting and executing portfolios of actions that consume resources. Typically, the resulting value is uncertain, and thus organizations must take decisions based on ex ante estimates about what this future value will be. In this paper, we show that the Bayesian modeling of uncertainties in this selection problem serves to (i) increase the expected future value of the selected portfolio, (ii) raise the expected number of selected actions that belong to the optimal portfolio ex post, and (iii) eliminate the expected gap between the realized ex post portfolio value and the estimated ex ante portfolio value. We also propose a new project performance measure, defined as the probability that a given action belongs to the optimal portfolio. Finally, we provide analytic results to determine which actions should be re-evaluated to obtain more accurate value estimates before portfolio selection. In particular, we show that the optimal targeting of such re-evaluations can yield a much higher portfolio value in return for the total resources that are spent on the execution of actions and the acquisition of value estimates.}
}
@incollection{HOLLINGSWORTH2000109,
title = {Resource-aware meta-computing},
editor = {Marvin V. Zelkowits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {53},
pages = {109-169},
year = {2000},
booktitle = {Emphasizing Distributed Systems},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(00)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S0065245800800054},
author = {Jeffrey K. Hollingsworth and Peter J. Keleher and Kyung D. Ryu},
abstract = {Meta-computing, is an increasingly popular and useful method of obtaining resources to solve large computational problems. However, meta-computer environments pose a number of unique challenges, many of which have yet to be addressed effectively. Among these are dynamicism in both applications and environments, and heterogeneity at several different levels. This chapter discusses current approaches to these problems, and uses them in the Active Harmony system as a running example. Harmony supports an interface that allows applications to export tuning alternatives to the higher-level system. By exposing different parameters that can be changed at runtime, applications can be automatically adapted to changes in their execution environment caused by other programs, the addition or deletion of nodes, or changes in the availability of resources like communication links. Applications expose not only options, but also expected resource utilization with each option and the effect that the option will have on the application's performance. We discuss how this flexibility can be used to tune the overall performance of a collection of applications in a system.}
}
@article{KULJIS200137,
title = {An appraisal of web-based simulation: whither we wander?},
journal = {Simulation Practice and Theory},
volume = {9},
number = {1},
pages = {37-54},
year = {2001},
issn = {0928-4869},
doi = {https://doi.org/10.1016/S0928-4869(01)00032-5},
url = {https://www.sciencedirect.com/science/article/pii/S0928486901000325},
author = {Jasna Kuljis and Ray J. Paul},
keywords = {Web-based simulation, Simulation languages, Java, Distributed simulation},
abstract = {The direction that web-based simulation modelling is taking is determined and deliberated. Environments and languages for web-based simulation are reviewed, particularly Java-based approaches. Web-based applications are discussed. After proposing a summary of the review, ways of working that will have an unpredictable effect on the future of simulation modelling are proposed. The future direction of web-based simulation is speculated on, given the argument pursued in the paper.}
}
@article{COSTELLO2016311,
title = {Who views online extremism? Individual attributes leading to exposure},
journal = {Computers in Human Behavior},
volume = {63},
pages = {311-320},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216303600},
author = {Matthew Costello and James Hawdon and Thomas Ratliff and Tyler Grantham},
keywords = {Online victimization, Online extremism, Exposure to online hate, Routine activities theory, Flocking and feathering},
abstract = {Who is likely to view materials online maligning groups based on race, nationality, ethnicity, sexual orientation, gender, political views, immigration status, or religion? We use an online survey (N = 1034) of youth and young adults recruited from a demographically balanced sample of Americans to address this question. By studying demographic characteristics and online habits of individuals who are exposed to online extremist groups and their messaging, this study serves as a precursor to a larger research endeavor examining the online contexts of extremism. Descriptive results indicate that a sizable majority of respondents were exposed to negative materials online. The materials were most commonly used to stereotype groups. Nearly half of negative material centered on race or ethnicity, and respondents were likely to encounter such material on social media sites. Regression results demonstrate African-Americans and foreign-born respondents were significantly less likely to be exposed to negative material online, as are younger respondents. Additionally, individuals expressing greater levels of trust in the federal government report significantly less exposure to such materials. Higher levels of education result in increased exposure to negative materials, as does a proclivity towards risk-taking.}
}
@article{LEOKUMAR2017294,
title = {State of The Art-Intense Review on Artificial Intelligence Systems Application in Process Planning and Manufacturing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {65},
pages = {294-329},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617301896},
author = {S.P. {Leo Kumar}},
keywords = {Artificial Intelligence, Expert System, Computer Aided Process Planning, Manufacturing, Evolutionary Techniques},
abstract = {Artificial Intelligence (AI) systems applications are widespread due to its domain independent characteristics. In this work, an attempt has been made for review on AI applications in Computer Aided Process Planning (CAPP) and manufacturing. Primarily, uniqueness of present review work addressed by analysis of existing review articles. The review work comprise of three main elements; 1. Feature based design, a primary input for a CAPP system, 2. Expert System (ES) usefulness in Process Planning (PP) and manufacturing and 3. Evolutionary approach applications. The review begins with an overview and the use of AI systems in decision making. Research works exemplified for the past three and half decades (1981–2016) are analyzed in terms of feature based modeling, Standards for Exchange of Product Model data approach, ES in PP, scheduling, manufacturing and miscellaneous applications. Role of Evolutionary Techniques (ET) in intelligent system development, execution of PP activities and manufacturing are described. A statistical analysis on existing review articles, number of publications, domain specific articles and percentage contribution of each area are carried out. Finally, research gaps are identified and the possible future research directions are presented.}
}
@article{SHAO2015933,
title = {An effective semi-cross-validation model selection method for extreme learning machine with ridge regression},
journal = {Neurocomputing},
volume = {151},
pages = {933-942},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214012715},
author = {Zhifei Shao and Meng Joo Er and Ning Wang},
keywords = {Extreme learning machine, Ridge regression, Regularized ELM},
abstract = {Extreme Learning Machine (ELM) has attracted comprehensive attentions as a universal function approximator with its extremely fast learning speed and good generalization performance. Compared to other learning methods for Single Layer Feedforward Networks (SLFNs), the unique feature of the ELM is that the input parameters of hidden neurons are randomly generated rather than being iteratively tuned, and thereby dramatically reducing the computational burden. However, it has been pointed out that the randomness of the ELM parameters would result in fluctuating performance. In this paper, we systematically investigate the performance stabilization effect brought by a regularized variant of the ELM, named Regularized ELM (RELM). Furthermore, by using the PREdiction Sum of Squares (PRESS) statistics formula and a unique property of the RELM, we propose a semi-cross-validation algorithm to effectively realize a robust RELM-based model selection for SLFNs, termed as Automatic Regularized Extreme Learning Machine with Leave-One-Out cross-validation (AR-ELM-LOO). The simulation results show that the AR-ELM-LOO can significantly reduce the randomness performance of the ELM and it can produce nearly identical results as the full cross-validation procedure.}
}
@article{CRUZ2018195,
title = {Dynamic classifier selection: Recent advances and perspectives},
journal = {Information Fusion},
volume = {41},
pages = {195-216},
year = {2018},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2017.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517304074},
author = {Rafael M.O. Cruz and Robert Sabourin and George D.C. Cavalcanti},
keywords = {Multiple classifier systems, Ensemble of classifiers, Dynamic classifier selection, Dynamic ensemble selection, Classifier competence, Survey},
abstract = {Multiple Classifier Systems (MCS) have been widely studied as an alternative for increasing accuracy in pattern recognition. One of the most promising MCS approaches is Dynamic Selection (DS), in which the base classifiers are selected on the fly, according to each new sample to be classified. This paper provides a review of the DS techniques proposed in the literature from a theoretical and empirical point of view. We propose an updated taxonomy based on the main characteristics found in a dynamic selection system: (1) The methodology used to define a local region for the estimation of the local competence of the base classifiers; (2) The source of information used to estimate the level of competence of the base classifiers, such as local accuracy, oracle, ranking and probabilistic models, and (3) The selection approach, which determines whether a single or an ensemble of classifiers is selected. We categorize the main dynamic selection techniques in the DS literature based on the proposed taxonomy. We also conduct an extensive experimental analysis, considering a total of 18 state-of-the-art dynamic selection techniques, as well as static ensemble combination and single classification models. To date, this is the first analysis comparing all the key DS techniques under the same experimental protocol. Furthermore, we also present several perspectives and open research questions that can be used as a guide for future works in this domain.}
}
@article{ALEKSEEV20051311,
title = {Adjoint correction and bounding of error using lagrange form of truncation term},
journal = {Computers & Mathematics with Applications},
volume = {50},
number = {8},
pages = {1311-1332},
year = {2005},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2005.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0898122105003834},
author = {A.K. Alekseev and I.M. Navon},
keywords = {Differential approximation, Lagrange truncation term, adjoint problem,  error estimation, Error bound},
abstract = {The a posteriori error evaluation based on differential approximation of a finite-difference scheme and adjoint equations is addressed. The differential approximation is composed of primal equations and a local truncation error determined by a Taylor series in Lagrange form. This approach provides the feasibility of both refining the solution and using the Holder inequality for asymptotic bounding of the remaining error.}
}