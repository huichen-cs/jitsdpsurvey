@article{https://doi.org/10.1049/iet-sen.2019.0278,
author = {Zhu, Kun and Zhang, Nana and Ying, Shi and Zhu, Dandan},
title = {Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network},
journal = {IET Software},
volume = {14},
number = {3},
pages = {185-195},
keywords = {neural nets, learning (artificial intelligence), denoising autoencoder, software defect prediction, basic defect features, mainstream deep learning techniques, just-in-time defect prediction model, autoencoder convolutional neural network, convolution neural network, cross-project defect prediction experiments},
doi = {https://doi.org/10.1049/iet-sen.2019.0278},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0278},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-sen.2019.0278},
abstract = {Just-in-time defect prediction is an important and useful branch in software defect prediction. At present, deep learning is a research hotspot in the field of artificial intelligence, which can combine basic defect features into deep semantic features and make up for the shortcomings of machine learning algorithms. However, the mainstream deep learning techniques have not been applied yet in just-in-time defect prediction. Therefore, the authors propose a novel just-in-time defect prediction model named DAECNN-JDP based on denoising autoencoder and convolutional neural network in this study, which has three main advantages: (i) Different weights for the position vector of each dimension feature are set, which can be automatically trained by adaptive trainable vector. (ii) Through the training of denoising autoencoder, the input features that are not contaminated by noise can be obtained, thus learning more robust feature representation. (iii) The authors leverage a powerful representation-learning technique, convolution neural network, to construct the basic change features into the abstract deep semantic features. To evaluate the performance of the DAECNN-JDP model, they conduct extensive within-project and cross-project defect prediction experiments on six large open source projects. The experimental results demonstrate that the superiority of DAECNN-JDP on five evaluation metrics.},
year = {2020}
}
@article{https://doi.org/10.1002/smr.2381,
author = {Eken, Beyza and Tufan, Selda and Tunaboylu, Alper and Guler, Tevfik and Atar, Rifat and Tosun, Ayse},
title = {Deployment of a change-level software defect prediction solution into an industrial setting},
journal = {Journal of Software: Evolution and Process},
volume = {33},
number = {11},
pages = {e2381},
keywords = {change-level defect prediction, deployment, industrial case study, online prediction},
doi = {https://doi.org/10.1002/smr.2381},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2381},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2381},
abstract = {Abstract Applying change-level software defect prediction (SDP) in practice has several challenges regarding model validation techniques, data accuracy, and prediction performance consistency. A few studies report on these challenges in an industrial context. We share our experience in integrating an SDP into an industrial context. We investigate whether an “offline” SDP could reflect its “online” (real-life) performance, and other deployment decisions: the model re-training process and update period. We employ an online prediction strategy by considering the actual labels of training commits at the time of prediction and compare its performance against an offline prediction. We empirically assess the online SDP's performance with various lengths of the time gap between the train and test set and model update periods. Our online SDP's performance could successfully reach its offline performance. The time gap between the train and test commits, and model update period significantly impacts the online performance by 37\% and 18\% in terms of probability of detection (pd), respectively. We deploy the best SDP solution (73\% pd) with an 8-month time gap and a 3-day update period. Contextual factors may determine the model performance in practice, its consistency, and trustworthiness. As future work, we plan to investigate the reasons for fluctuations in model performance over time.},
year = {2021}
}
@article{https://doi.org/10.1049/sfw2.12040,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Xue, Lei and Li, Wei and Catolino, Gemma},
title = {A compositional model for effort-aware Just-In-Time defect prediction on android apps},
journal = {IET Software},
volume = {n/a},
number = {n/a},
pages = {},
keywords = {fault tolerance, software performance evaluation, software quality},
doi = {https://doi.org/10.1049/sfw2.12040},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12040},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/sfw2.12040},
abstract = {Abstract Android apps have played important roles in daily life and work. To meet the new requirements from users, the apps encounter frequent updates, which involves a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for apps to timely identify whether the new code commits can introduce defects into apps, aiming to assure their quality. In general, high-quality features are benefits for improving the classification performance. In addition, the number of defective commit instances is much fewer than that of clean ones, that is the defect data is class imbalanced. In this study, a novel compositional model, called KPIDL, is proposed to conduct the JIT defect prediction task for Android apps. More specifically, KPIDL first exploits a feature learning technique to preprocess original data for obtaining better feature representation, and then introduces a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to alleviate the class imbalance issue by considering the prior probability of the two types of classes. The experiments were conducted on a benchmark defect data consisting of 15 Android apps. The experimental results show that the proposed KPIDL model performs significantly better than 25 comparative methods in terms of two effort-aware performance indicators in most cases.}
}
@article{https://doi.org/10.1049/iet-sen.2017.0148,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke},
title = {Progress on approaches to software defect prediction},
journal = {IET Software},
volume = {12},
number = {3},
pages = {161-175},
keywords = {software quality, software reliability, quality assurance, research and development, software defect prediction, software engineering journals, defect-prone software modules, software quality assurance, machine learning-based prediction algorithms, data manipulation, effort-aware prediction, empirical studies},
doi = {https://doi.org/10.1049/iet-sen.2017.0148},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2017.0148},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-sen.2017.0148},
abstract = {Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014–April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.},
year = {2018}
}
@article{https://doi.org/10.1002/spe.2927,
author = {Kang, Jonggu and Ryu, Duksan and Baik, Jongmoon},
title = {Predicting just-in-time software defects to reduce post-release quality costs in the maritime industry},
journal = {Software: Practice and Experience},
volume = {51},
number = {4},
pages = {748-771},
keywords = {industrial application, just-in-time prediction, maintenance, maritime transportation, software defect prediction, software quality assurance},
doi = {https://doi.org/10.1002/spe.2927},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2927},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2927},
abstract = {Abstract Background The importance of software in maritime transportation is rapidly increasing as the industry seeks to develop and utilize innovative future ships, which can be realized using software technology. Due to the safety-critical nature of ships, software quality assurance (SQA) has become an essential prerequisite for such development. Objective Based on the unique characteristics of the maritime domain, the purpose of this study was to achieve effective SQA resource allocation to reduce post-release quality costs. Method Software defect prediction (SDP) is employed to predict defects in newly developed software based on models trained with past software defects and to update information using machine learning. This study demonstrated that just-in-time SDP is applicable to maritime domain practice and can reduce post-release quality costs via combination with an estimation model, qCOPLIMO. Results Using real-world datasets collected from the maritime industry, performance and cost-benefit analyses of SDP were performed. A successful model was obtained that meets the performance criterion of 0.75 in within-project defect prediction (WPDP) but not cross-project defect prediction (CPDP). In addition, the cost-benefit analysis results showed that 20\% effort enables the detection of 56\% of defects on average and that the post-release quality cost can be reduced by 37.3\% in the maritime domain. Conclusion SDP can be successfully applied to the maritime domain. Further, it is desirable to utilize WPDP instead of CPDP once minimum high-quality commits are available that can be identified as defective or not. Finally, SDP can help reduce review effort and post-release quality costs.},
year = {2021}
}
@article{https://doi.org/10.1111/coin.12229,
author = {Lingden, P. and Alsadoon, Abeer and Prasad, P.W.C. and Alsadoon, Omar Hisham and Ali, Rasha S. and Nguyen, Vinh Tran Quoc},
title = {A novel modified undersampling (MUS) technique for software defect prediction},
journal = {Computational Intelligence},
volume = {35},
number = {4},
pages = {1003-1020},
keywords = {correlation feature selection, machine learning, modified undersampling, software defect prediction},
doi = {https://doi.org/10.1111/coin.12229},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12229},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/coin.12229},
note = { COIN-OA-11-18-2860.R3},
abstract = {Abstract Background and aim: Many sophisticated data mining and machine learning algorithms have been used for software defect prediction (SDP) to enhance the quality of software. However, real-world SDP data sets suffer from class imbalance, which leads to a biased classifier and reduces the performance of existing classification algorithms resulting in an inaccurate classification and prediction. This work aims to improve the class imbalance nature of data sets to increase the accuracy of defect prediction and decrease the processing time. Methodology: The proposed model focuses on balancing the class of data sets to increase the accuracy of prediction and decrease processing time. It consists of a modified undersampling method and a correlation feature selection (CFS) method. Results: The results from ten open source project data sets showed that the proposed model improves the accuracy in terms of F1-score to 0.52 ∼ 0.96, and hence it is proximity reached best F1-score value in 0.96 near to 1 then it is given a perfect performance in the prediction process. Conclusion: The proposed model focuses on balancing the class of data sets to increase the accuracy of prediction and decrease processing time using the proposed model.},
year = {2019}
}
@article{https://doi.org/10.1049/iet-sen.2018.5439,
author = {Yu, Qiao and Jiang, Shujuan and Qian, Junyan and Bo, Lili and Jiang, Li and Zhang, Gongjie},
title = {Process metrics for software defect prediction in object-oriented programs},
journal = {IET Software},
volume = {14},
number = {3},
pages = {283-292},
keywords = {object-oriented programming, software fault tolerance, software maintenance, software metrics, software quality, software packages, defect rates, process metrics, evolution-oriented defect prediction, object-oriented programs, software evolution, modern software system, historical defects, traditional software defect prediction methods, evolution data},
doi = {https://doi.org/10.1049/iet-sen.2018.5439},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5439},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-sen.2018.5439},
abstract = {Software evolution is an important activity in the life cycle of a modern software system. In the process of software evolution, the repair of historical defects and the increasing demands may introduce new defects. Therefore, evolution-oriented defect prediction has attracted much attention of researchers in recent years. At present, some researchers have proposed the process metrics to describe the characteristics of software evolution. However, compared with the traditional software defect prediction methods, the research on evolution-oriented defect prediction is still inadequate. Based on the evolution data of object-oriented programs, this study presented two new process metrics from the defect rates of historical packages and the change degree of classes. To show the effectiveness of the proposed process metrics, the authors made comparisons with the code metrics and other process metrics. An empirical study was conducted on 33 versions of nine open-source projects. The results showed that adding the proposed process metrics could improve the performance of evolution-oriented defect prediction effectively.},
year = {2020}
}
@article{https://doi.org/10.1002/smr.2362,
author = {Guo, Shikai and Dong, Jian and Li, Hui and Wang, Jiahui},
title = {Software defect prediction with imbalanced distribution by radius-synthetic minority over-sampling technique},
journal = {Journal of Software: Evolution and Process},
volume = {33},
number = {7},
pages = {e2362},
keywords = {imbalanced learning, software defect prediction, software quality},
doi = {https://doi.org/10.1002/smr.2362},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2362},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2362},
abstract = {Abstract Software defect prediction, which can identify the defect-prone modules, is an effective technology to ensure the quality of software products. Due to the importance in software maintenance, many learning-based software defect prediction models are presented in recent years. Actually, the defects usually occupy a very small proportions in software source codes; thus, the imbalanced distributions between defect-prone modules and non-defect-prone modules increase the learning difficulty of the classification task. To address this issue, we present a random over-sampling mechanism used to generate minority-class samples from high-dimensional sampling space to deal with the imbalanced distributions in software defect prediction, in which two constraints are applied to provide a robust way to generate new synthetic samples, that is, scaling the random over-sampling scope to a reasonable area and distinguishing the majority-class samples in a critical region. Based on nine open datasets of software projects, we experimentally verify that our presented method is effective on predict the defect-prone modules, and the effect is superior to the traditional imbalanced processing methods.},
year = {2021}
}
@article{https://doi.org/10.1002/smr.2234,
author = {Chen, Xiang and Mu, Yanzhou and Qu, Yubin and Ni, Chao and Liu, Meng and He, Tong and Liu, Shangqing},
title = {Do different cross-project defect prediction methods identify the same defective modules?},
journal = {Journal of Software: Evolution and Process},
volume = {32},
number = {5},
pages = {e2234},
keywords = {cross-project defect prediction, diversity analysis, empirical study, software defect prediction},
doi = {https://doi.org/10.1002/smr.2234},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2234},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2234},
note = {e2234 smr.2234},
abstract = {Abstract Cross-project defect prediction (CPDP) is needed when the target projects are new projects or the projects have less training data, since these projects do not have sufficient historical data to build high-quality prediction models. The researchers have proposed many CPDP methods, and previous studies have conducted extensive comparisons on the performance of different CPDP methods. However, to the best of our knowledge, it remains unclear whether different CPDP methods can identify the same defective modules, and this issue has not been thoroughly explored. In this article, we select 12 state-of-the-art CPDP methods, including eight supervised methods and four unsupervised methods. We first compare the performance of these methods in the same experiment settings on five widely used datasets (ie, NASA, SOFTLAB, PROMISE, AEEEM, and ReLink) and rank these methods via the Scott-Knott test. Final results confirm the competitiveness of unsupervised methods. Then we perform diversity analysis on defective modules for these methods by using the McNemar test. Empirical results verify that different CPDP methods may lead to difference in the modules predicted as defective, especially when the comparison is performed between the supervised methods and unsupervised methods. Finally, we also find there exist a certain number of defective modules, which cannot be correctly identified by any of the CPDP methods or can be correctly identified by only one CPDP method. These findings can be utilized to design more effective methods to further improve the performance of CPDP.},
year = {2020}
}
@article{https://doi.org/10.1002/stvr.1761,
author = {Eivazpour, Zeinab and Keyvanpour, Mohammad Reza},
title = {CSSG: A cost-sensitive stacked generalization approach for software defect prediction},
journal = {Software Testing, Verification and Reliability},
volume = {31},
number = {5},
pages = {e1761},
keywords = {class imbalance learning, cost of misclassification, cost-sensitive learning, data imbalance, ensemble learning, software defect prediction},
doi = {https://doi.org/10.1002/stvr.1761},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1761},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1761},
note = {e1761 stvr.1761},
abstract = {Summary The prediction of software artifacts on defect-prone (DP) or non-defect-prone (NDP) classes during the testing phase helps minimize software business costs, which is a classification task in software defect prediction (SDP) field. Machine learning methods are helpful for the task, although they face the challenge of data imbalance distribution. The challenge leads to serious misclassification of artifacts, which will disrupt the predictor's performance. The previously developed stacking ensemble methods do not consider the cost issue to handle the class imbalance problem (CIP) over the training dataset in the SDP field. To bridge this research gap, in the cost-sensitive stacked generalization (CSSG) approach, we try to combine the staking ensemble learning method with cost-sensitive learning (CSL) since the CSL purpose is to reduce misclassification costs. In the cost-sensitive stacked generalization (CSSG) approach, logistic regression (LR) and extremely randomized trees classifiers in cases of CSL and cost-insensitive are used as a final classifier of stacking scheme. To evaluate the performance of CSSG, we use six performance measures. Several experiments are carried out to compare the CSSG with some cost-sensitive ensemble methods on 15 benchmark datasets with different imbalance levels. The results indicate that the CSSG can be an effective solution to the CIP than other compared methods.},
year = {2021}
}
@article{https://doi.org/10.1002/spe.2606,
author = {Zhu, Xiaoyan and Niu, Binbin and Whitehead Jr., E. James and Sun, Zhongbin},
title = {An empirical study of software change classification with imbalance data-handling methods},
journal = {Software: Practice and Experience},
volume = {48},
number = {11},
pages = {1968-1999},
keywords = {bug prediction, change classification, ensemble learning, imbalance data, resampling},
doi = {https://doi.org/10.1002/spe.2606},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2606},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2606},
note = { spe.2606},
abstract = {Summary Bug prediction in software code changes can help developers to find out and fix bugs immediately when they are introduced, thus to improve the effectiveness and validity of bug fixing. In data mining, this problem can be regarded as a change classification task. However, one of its key characteristics, ie, class-imbalance, holds back the performance of standard classification methods. In this paper, we consider a quantity of imbalance data-handling methods and extract a more comprehensive groups of change features, aiming to achieve better change classification performance. Two different types of imbalance data-handling methods, namely, resampling and ensemble learning methods, are employed. Especially, we explore the performance of their combination. To compare the performance of different imbalance data-handling methods, an experiment with 10 open source projects is conducted. Four classification methods, including J48, Naïve Bayes, SMO, and Random Forest, are used as standard classifiers and as the base classifiers, respectively. Moreover, contribution of different groups of change features are evaluated. Experimental results show that imbalance data-handling methods can improve the performance of change classification and the combination methods, which take advantage of both ensemble learning and resampling, perform better than using ensemble learning methods or resampling methods individually. Of the studied imbalance data-handling methods, the combination of Bagging and random undersampling with J48 as the base classifier yields out better prediction results than those achieved by other methods. Additionally, of the collected change features, text vector features accounts for a larger proportion than others.},
year = {2018}
}
@article{https://doi.org/10.1002/sec.547,
author = {Bartsch, Steffen},
title = {Policy override in practice: model, evaluation, and decision support},
journal = {Security and Communication Networks},
volume = {7},
number = {1},
pages = {139-156},
keywords = {security usability, authorization, policy override, privilege escalation, break-glass, decision support},
doi = {https://doi.org/10.1002/sec.547},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.547},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sec.547},
abstract = {ABSTRACT The predominant strategy in restricting permissions in information systems is to limit users on the basis of the ‘need-to-know’ principle. Although appropriate in highly security-sensitive contexts, this culture of protection will, in other contexts, often reduce users' productivity and is seen as a hassle because the everyday exceptions to the routine tasks can be severely hindered. This paper proposes a more flexible authorization model, policy override, which allows end users to override authorization in a controlled manner. In this article, I describe the authorization model and its implementation in a medium enterprise's business application. I evaluated policy override use over a period of 1 year through quantitative and qualitative analysis to identify challenges and offer advice on the implementation of policy override in practice. One important challenge is the setting of adequate bounds for policy override. To overcome this obstacle, I propose and evaluate a qualitative risk-based calculus that offers decision support to balance additional risks of policy override with the benefits of more flexible authorization. Copyright © 2012 John Wiley \& Sons, Ltd.},
year = {2014}
}
@article{https://doi.org/10.1002/smr.2290,
author = {Goyal, Anjali and Sardana, Neetu},
title = {IMNRFixer: A hybrid approach to alleviate class-imbalance problem for predicting the fixability of Non-Reproducible bugs},
journal = {Journal of Software: Evolution and Process},
volume = {n/a},
number = {n/a},
pages = {e2290},
keywords = {bug fixing, bug report, class-imbalance, classification, ensemble techniques, machine learning, mining software repositories, Non-Reproducible bugs, prediction tool, sampling},
doi = {https://doi.org/10.1002/smr.2290},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2290},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2290},
note = {e2290 smr.2290},
abstract = {Abstract Software maintenance is an important phase in the software development life cycle. Software projects maintain bug repositories to gather, organize, and keep track of bug reports. These bug reports are resolved by numerous software developers. Whenever the reported bug does not get resolved by the assigned developer, he marks the resolution of bug report as Non-Reproducible (NR). When NR bugs are reconsidered, few of them get resolved, and their resolution changes from NR to fix (NRF). The main aim of this paper is to predict these fixable NRF bug reports. A major challenge in predicting NRF bugs from NR bugs is that only a small portion of NR bugs get fixed, i.e., class-imbalance problem. For example, NRF bugs account for only 8.64\%, 4.73 \%, 4.56\%, and 1.06\% in NetBeans, Eclipse, Open Office, and Mozilla Firefox projects respectively. In this paper, we work on improving the classification performance on these imbalanced datasets. We propose IMNRFixer, a novel and hybrid NRF prediction tool. IMNRFixer uses three different techniques to combat class-imbalance problem: undersampling, oversampling, and ensemble models. We evaluate the performance of IMNRFixer models on four large and open-source projects of Bugzilla repository. Our results show that IMNRFixer outperforms conventional machine learning techniques. IMNRFixer achieves performance up to 71.7\%, 93.1\%, 91.7\%, and 96.5\% while predicting the minority class (NRF) for NetBeans, Eclipse, Open Office, and Mozilla Firefox projects, respectively.}
}
@article{https://doi.org/10.1049/iet-sen.2020.0166,

title = {Guest Editorial: Knowledge Discovery for Software Development (KDSD)},
journal = {IET Software},
volume = {14},
number = {3},
pages = {183-184},
doi = {https://doi.org/10.1049/iet-sen.2020.0166},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2020.0166},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-sen.2020.0166},
year = {2020}
}
@article{https://doi.org/10.1002/spe.3055,
author = {Srirama, Satish Narayana and Buyya, Rajkumar},
title = {Post golden jubilee year of the software journal: New research trends and strengthening advisory editorial team},
journal = {Software: Practice and Experience},
volume = {n/a},
number = {n/a},
pages = {},
doi = {https://doi.org/10.1002/spe.3055},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3055},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.3055}
}
@article{https://doi.org/10.1111/coin.12219,
author = {Meng, Shunmei and Li, Qianmu and Wu, Taoran and Huang, Weijia and Zhang, Jing and Li, Weimin},
title = {A fault-tolerant dynamic scheduling method on hierarchical mobile edge cloud computing},
journal = {Computational Intelligence},
volume = {35},
number = {3},
pages = {577-598},
keywords = {dynamic scheduling, fault-tolerant scheduling, hierarchical cloud, mobile edge cloud computing},
doi = {https://doi.org/10.1111/coin.12219},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12219},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/coin.12219},
note = { 10.1111/coin.12219},
abstract = {Abstract Mobile edge cloud computing has been a promising computing paradigm, where mobile users could offload their application workloads to low-latency local edge cloud resources. However, compared with remote public cloud resources, conventional local edge cloud resources are limited in computation capacity, especially when serve large number of mobile applications. To deal with this problem, we present a hierarchical edge cloud architecture to integrate the local edge clouds and public clouds so as to improve the performance and scalability of scheduling problem for mobile applications. Besides, to achieve a trade-off between the cost and system delay, a fault-tolerant dynamic resource scheduling method is proposed to address the scheduling problem in mobile edge cloud computing. The optimization problem could be formulated to minimize the application cost with the user-defined deadline satisfied. Specifically, firstly, a game-theoretic scheduling mechanism is adopted for resource provisioning and scheduling for multiprovider mobile applications. Then, a mobility-aware dynamic scheduling strategy is presented to update the scheduling with the consideration of mobility of mobile users. Moreover, a failure recovery mechanism is proposed to deal with the uncertainties during the execution of mobile applications. Finally, experiments are designed and conducted to validate the effectiveness of our proposal. The experimental results show that our method could achieve a trade-off between the cost and system delay.},
year = {2019}
}
@article{https://doi.org/10.1002/(SICI)1097-024X(200003)30:3<199::AID-SPE296>3.0.CO;2-2,
author = {Fitzgerald, Robert and Knoblock, Todd B. and Ruf, Erik and Steensgaard, Bjarne and Tarditi, David},
title = {Marmot: an optimizing compiler for Java},
journal = {Software: Practice and Experience},
volume = {30},
number = {3},
pages = {199-232},
keywords = {compilers, language translation, optimization, Java},
doi = {https://doi.org/10.1002/(SICI)1097-024X(200003)30:3<199::AID-SPE296>3.0.CO;2-2},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-024X%28200003%2930%3A3%3C199%3A%3AAID-SPE296%3E3.0.CO%3B2-2},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-024X%28200003%2930%3A3%3C199%3A%3AAID-SPE296%3E3.0.CO%3B2-2},
abstract = {Abstract The Marmot system is a research platform for studying the implementation of high level programming languages. It currently comprises an optimizing native-code compiler, runtime system, and libraries for a large subset of Java. Marmot integrates well-known representation, optimization, code generation, and runtime techniques with a few Java-specific features to achieve competitive performance. This paper contains a description of the Marmot system design, along with highlights of our experience applying and adapting traditional implementation techniques to Java. A detailed performance evaluation assesses both Marmot's overall performance relative to other Java and C++ implementations, and the relative costs of various Java language features in Marmot-compiled code. Our experience with Marmot has demonstrated that well-known compilation techniques can produce very good performance for static Java applications – comparable or superior to other Java systems, and approaching that of C++ in some cases. Copyright © 2000 John Wiley \& Sons, Ltd.},
year = {2000}
}
@article{https://doi.org/10.1002/asi.20228,
author = {MacIntosh-Murray, Anu and Choo, Chun Wei},
title = {Information behavior in the context of improving patient safety},
journal = {Journal of the American Society for Information Science and Technology},
volume = {56},
number = {12},
pages = {1332-1345},
doi = {https://doi.org/10.1002/asi.20228},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20228},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20228},
abstract = {Abstract Although it is assumed that information about patient safety and adverse events will be used for improvement and organizational learning, we know little about how this actually happens in patient care settings. This study examines how organizational and professional practices and beliefs related to patient safety influence (1) how health care providers and managers make sense of patient safety risks and adverse events, and (2) the flow and use of information for making improvements. The research is based on an ethnographic case study of a medical unit in a large tertiary care hospital in Canada. The study found that front-line staff are task driven, coping with heavy workloads that limit their attention to and recognition of potential information needs and knowledge gaps. However, a surrogate in an information-related role—an “information/change agent”—may intervene successfully with staff and engage in preventive maintenance and repair of routines. The article discusses four key functions of the information/change agent (i.e., boundary spanner, information seeker, knowledge translator, and change champion) in the context of situated practice and learning. All four functions are important for facilitating changes to practice, routines, and the work environment to improve patient safety.},
year = {2005}
}
@article{https://doi.org/10.1111/1468-0394.00064,

title = {News},
journal = {Expert Systems},
volume = {15},
number = {1},
pages = {59-69},
doi = {https://doi.org/10.1111/1468-0394.00064},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0394.00064},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0394.00064},
year = {1998}
}
@article{https://doi.org/10.1002/sec.879,
author = {Ulltveit-Moe, Nils and Oleshchuk, Vladimir},
title = {Enforcing mobile security with location-aware role-based access control},
journal = {Security and Communication Networks},
volume = {9},
number = {5},
pages = {429-439},
keywords = {location-aware RBAC, GeoXACML, mobile security},
doi = {https://doi.org/10.1002/sec.879},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.879},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sec.879},
abstract = {Abstract This paper describes how location-aware role-based access control (RBAC) can be implemented on top of the Geospatial eXtensible Access Control Markup Language (GeoXACML). It furthermore sketches how spatial separation of duty constraints (both static and dynamic) can be implemented using GeoXACML on top of the XACML RBAC profile. The solution uses physical addressing of geographical locations, which facilitates easy deployment of authorisation profiles to the mobile device. Location-aware RBAC can be used to implement location-dependent access control and also other security enhancing solutions on mobile devices, such as location-dependent device locking, firewall, intrusion prevention or payment anti-fraud systems. The system has been implemented and tested, both to verify the server capacity and also the client capacity running on a mobile device. Copyright © 2013 John Wiley \& Sons, Ltd.},
year = {2016}
}

