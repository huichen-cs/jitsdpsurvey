"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"[Journal First] Are Fix-Inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-Time Defect Prediction","S. McIntosh; Y. Kamei","McGill Univ., Montr√©al, QC, Canada; Kyushu Univ., Fukuoka, Japan","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","2 Sep 2018","2018","","","560","560","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3182514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453123","Just In Time prediction;Defect prediction;Mining software repositories","Software engineering;Predictive models;History;Fluctuations;Training;Data models;Software","data mining;just-in-time;learning (artificial intelligence);public domain software;software fault tolerance;software management","fix-inducing changes;JIT models;just-in-time models;just-in-time defect prediction;Qt systems;OpenStack systems;fix-inducing code changes;code change properties","","1","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Poster: Bridging Effort-Aware Prediction and Strong Classification - A Just-in-Time Software Defect Prediction Study","Y. Guo; M. Shepperd; N. Li","Xi'an Jiaotong Univ., Xian, China; Brunel Univ. London, Uxbridge, UK; Northwestern Polytech. Univ., Xian, China","2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)","30 Aug 2018","2018","","","325","326","Context: Most research into software defect prediction ignores the differing amount of effort entailed in searching for defects between software components. The result is sub-optimal solutions in terms of allocating testing resources. Recently effort-aware (EA) defect prediction has sought to redress this deficiency. However, there is a gap between previous classification research and EA prediction. Objective: We seek to transfer strong defect classification capability to efficient effort-aware software defect prediction. Method: We study the relationship between classification performance and the cost-effectiveness curve experimentally (using six open-source software data sets). Results: We observe extremely skewed distributions of change size which contributes to the lack of relationship between classification performance and the ability to find efficient test orderings for defect detection. Trimming allows all effort-aware approaches bridging high classification capability to efficient effort-aware performance. Conclusion: Effort distributions dominate effort-aware models. Trimming is a practical method to handle this problem.","2574-1934","978-1-4503-5663-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449561","Software;defect prediction;effort-aware;just-in-time","Predictive models;Software engineering;Testing;Measurement;Open source software;Bridges","just-in-time;pattern classification;program testing;public domain software","just-in-time software defect prediction study;software components;EA prediction;open-source software data sets;effort-aware models;effort-aware software defect prediction;defect classification capability;effort distributions","","","","","","30 Aug 2018","","","IEEE","IEEE Conferences"
"JITLine: A Simpler, Better, Faster, Finer-grained Just-In-Time Defect Prediction","C. Pornprasit; C. K. Tantithamthavorn","Monash University,Melbourne,Australia; Monash University,Melbourne,Australia","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","369","379","A Just-In-Time (JIT) defect prediction model is a classifier to predict if a commit is defect-introducing. Recently, CC2Vec-a deep learning approach for Just-In-Time defect prediction-has been proposed. However, CC2Vec requires the whole dataset (i.e., training + testing) for model training, assuming that all unlabelled testing datasets would be available beforehand, which does not follow the key principles of just-in-time defect predictions. Our replication study shows that, after excluding the testing dataset for model training, the F-measure of CC2Vec is decreased by 38.5% for OpenStack and 45.7% for Qt, highlighting the negative impact of excluding the testing dataset for Just-In-Time defect prediction. In addition, CC2Vec cannot perform fine-grained predictions at the line level (i.e., which lines are most risky for a given commit). In this paper, we propose JITLine-a Just-In-Time defect prediction approach for predicting defect-introducing commits and identifying lines that are associated with that defect-introducing commit (i.e., defective lines). Through a case study of 37,524 commits from OpenStack and Qt, we find that our JITLine approach is at least 26%-38% more accurate (F-measure), 17%-51% more cost-effective (PCI@20%LOC), 70-100 times faster than the state-of-the-art approaches (i.e., CC2Vec and DeepJIT) and the fine-grained predictions at the line level by our approach are 133%-150% more accurate (Top-10 Accuracy) than the baseline NLP approach. Therefore, our JITLine approach may help practitioners to better prioritize defect-introducing commits and better identify defective lines.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463103","Software Quality Assurance;Just In Time Defect Prediction;Explainable AI","Training;Deep learning;Computational modeling;Predictive models;Software;Data mining;Artificial intelligence","data handling;just-in-time;learning (artificial intelligence);natural language processing;program testing;software engineering","Just-In-Time defect prediction model;CC2Vec-a deep learning approach;model training;unlabelled testing datasets;testing dataset;fine grained prediction;just-in-time defect prediction approach;identifying lines;defective lines;JITLine approach;faster finer grained just in time defect prediction;simpler finer grained just in time defect prediction;NLP","","","","44","","28 Jun 2021","","","IEEE","IEEE Conferences"
"Explainable Just-In-Time Bug Prediction: Are We There Yet?","R. Aleithan","York University, Canada","2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","7 May 2021","2021","","","129","131","Explaining the prediction results of software bug prediction models is a challenging task, which can provide useful information for developers to understand and fix the predicted bugs. Recently, Jirayus et al.'s proposed to use two model-agnostic techniques (i.e., LIME and iBreakDown) to explain the prediction results of bug prediction models. Although their experiments on file-level bug prediction show promising results, the performance of these techniques on explaining the results of just-in-time (i.e., change-level) bug prediction is unknown. This paper conducts the first empirical study to explore the explainability of these model-agnostic techniques on just-in-time bug prediction models. Specifically, this study takes a three-step approach, 1) replicating previously widely used just-in-time bug prediction models, 2) applying Local Interpretability Model-agnostic Explanation Technique (LIME) and iBreakDown on the prediction results, and 3) manually evaluating the explanations for buggy instances (i.e. positive predictions) against the root cause of the bugs. The results of our experiment show that LIME and iBreakDown fail to explain defect prediction explanations for just-in-time bug prediction models, unlike file-level. This paper urges for new approaches for explaining the results of just-in-time bug prediction models.","2574-1926","978-1-6654-1219-3","10.1109/ICSE-Companion52605.2021.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402344","Bug prediction;Prediction Explanation","Computer bugs;Predictive models;Software;Task analysis;Software engineering","explanation;formal verification;program debugging","defect prediction explanations;software bug prediction;explainable just in time bug prediction;local interpretability model-agnostic explanation;iBreakDown","","","","13","","7 May 2021","","","IEEE","IEEE Conferences"
"Simplified Deep Forest Model based Just-In-Time Defect Prediction for Android Mobile Apps","K. Zhao; Z. Xu; T. Zhang; Y. Tang","Wuhan University,School of Computer Science,Wuhan,China; Chongqing University,School of Big Data and Software Engineering,Chongqing,China; Macau University of Science and Technology,Faculty of Information Technology,Macao,China; The Hong Kong Polytechnic University,Department of Computing,Hong Kong,China","2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)","11 Dec 2020","2020","","","222","222","The popularity of mobile devices has led to an explosive growth in the number of mobile apps in which Android mobile apps are the mainstream. Android mobile apps usually undergo frequent update due to new requirements proposed by users. Just-In-Time (JIT) defect prediction is appropriate for this scenario for quality assurance because it can provide timely feedback by determining whether a new code commit will introduce defects into the apps. As defect prediction performance usually relies on the quality of the data representation and the used classification model, in this work, we modify a state-of-the-art model, called Simplified Deep Forest (SDF) to conduct JIT defect prediction for Android mobile apps. This method uses a cascade structure with ensemble forests for representation learning and classification. We conduct experiments on 10 Android mobile apps and experimental results show that SDF performs significantly better than comparative methods in terms of three performance indicators.","","978-1-7281-8913-0","10.1109/QRS51102.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282763","n/a","Forestry;Software quality;Predictive models;Data models;Mobile applications;Software reliability;Security","just-in-time;learning (artificial intelligence);mobile computing;software quality","defect prediction performance;JIT defect prediction;Android mobile apps;just-in-time defect prediction;simplified deep forest model","","","","0","","11 Dec 2020","","","IEEE","IEEE Conferences"
"Deep Learning for Just-in-Time Defect Prediction","X. Yang; D. Lo; X. Xia; Y. Zhang; J. Sun","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2015 IEEE International Conference on Software Quality, Reliability and Security","24 Sep 2015","2015","","","17","26","Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.","","978-1-4673-7989-2","10.1109/QRS.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272910","Deep Learning;Just-In-Time Defect Prediction;Deep Belief Network;Cost Effectiveness","Feature extraction;Logistics;Machine learning;Computer bugs;Measurement;Training;Software quality","just-in-time;learning (artificial intelligence);pattern classification;software quality","deep learning;just-in-time defect prediction;change-level defect prediction;software quality;machine learning literature;machine learning classifier","","101","","40","","24 Sep 2015","","","IEEE","IEEE Conferences"
"Just-In-Time Bug Prediction in Mobile Applications: The Domain Matters!","G. Catolino","Univ. of Salerno, Salerno, Italy","2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems (MOBILESoft)","11 Jul 2017","2017","","","201","202","Bug prediction allows developers to focus testing efforts on specific areas of software systems. While this topic has been extensively studied for traditional applications, investigations on mobile apps are still missing. In this paper we preliminarily study the effectiveness of a previously defined Just-In-Time bug prediction model applied on five mobile apps. Key results indicate the poor performance of the model and the need of further research on the topic.","","978-1-5386-2669-6","10.1109/MOBILESoft.2017.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7972741","","Mobile communication;Computer bugs;Predictive models;Context;Size measurement","just-in-time;mobile computing;program debugging;program testing;software engineering","just-in-time bug prediction;mobile applications;software system testing;mobile apps","","4","","13","","11 Jul 2017","","","IEEE","IEEE Conferences"
"Enhancing Just-in-Time Defect Prediction Using Change Request-based Metrics","H. D. Tessema; S. L. Abebe","Addis Ababa University,Addis Ababa Institute of Technology,Addis Ababa,Ethiopia; Addis Ababa University,Addis Ababa Institute of Technology,Addis Ababa,Ethiopia","2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","11 May 2021","2021","","","511","515","Identifying defective software components as early as their commit helps to reduce significant software development and maintenance costs. In recent years, several studies propose to use just-in-time (JIT) defect prediction techniques to identify changes that could introduce defects at check-in time. To predict defect introducing changes, JIT defect prediction approaches use change metrics collected from software repositories. These change metrics, however, capture code and code change related information. Information related to the change requests (e.g., clarity of change request and difficulty to implement the change) that could determine the change's proneness to introducing new defects are not studied. In this study, we propose to augment the publicly available change metrics dataset with six change request- based metrics collected from issue tracking systems. To build the prediction model, we used five machine learning algorithms: AdaBoost, XGBoost, Deep Neural Network, Random Forest and Logistic Regression. The proposed approach is evaluated using a dataset collected from four open source software systems, i.e., Eclipse platform, Eclipse JDT, Bugzilla and Mozilla. The results show that the augmented dataset improves the performance of JIT defect prediction in 19 out of 20 cases. F1-score of JIT defect prediction in the four systems is improved by an average of 4.8%, 3.4%, 1.7%, 1.1% and 1.1% while using AdaBoost, XGBoost, Deep Neural Network, Random Forest and Logistic Regression, respectively.","1534-5351","978-1-7281-9630-5","10.1109/SANER50967.2021.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426021","Just-in-Time Software Defect Prediction;Software Bugs;Issue Tracking Systems;Software Metrics","Measurement;Machine learning algorithms;Conferences;Neural networks;Predictive models;Maintenance engineering;Random forests","data mining;just-in-time;learning (artificial intelligence);neural nets;program debugging;public domain software;regression analysis;software engineering;software maintenance;software metrics;software quality","defect introducing changes;JIT defect prediction approaches;software repositories;code change related information;publicly available change metrics dataset;prediction model;open source software systems;change request-based metrics;identifying defective software components;significant software development;maintenance costs;just-in-time defect prediction techniques;check-in time","","","","14","","11 May 2021","","","IEEE","IEEE Conferences"
"Effort-Aware Just-in-Time Bug Prediction for Mobile Apps Via Cross-Triplet Deep Feature Embedding","Z. Xu; K. Zhao; T. Zhang; C. Fu; M. Yan; Z. Xie; X. Zhang; G. Catolino","Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, China, and School of Big Data and Software Engineering, Chongqing University, Chongqing 401331 China (e-mail: zhouxullx@cqu.edu.cn).; School of Computer Science, Wuhan University, Wuhan 430072 China (e-mail: kszhao@whu.edu.cn).; Faculty of Information Technology, Macau University of Science and Technology, Macau 999078 China (e-mail: tazhang@must.edu.mo).; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, China, and School of Big Data and Software Engineering, Chongqing University, Chongqing 401331 China (e-mail: clfu@cqu.edu.cn).; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, China, and School of Big Data and Software Engineering, Chongqing University, Chongqing 401331 China (e-mail: mengy@cqu.edu.cn).; School of Computer Science, Wuhan University, Wuhan 430072 China (e-mail: xiezhiwen@mails.ccnu.edu.cn).; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, China, and School of Big Data and Software Engineering, Chongqing University, Chongqing 401331 China (e-mail: xhongz@cqu.edu.cn).; Jheronimus Academy of Data Science, Tilburg University, Tilburg 90153 The Netherlands (e-mail: g.catolino@tilburguniversity.edu).","IEEE Transactions on Reliability","","2021","PP","99","1","17","Just-in-time (JIT) bug prediction is an effective quality assurance activity that identifies whether a code commit will introduce bugs into the mobile app, aiming to provide prompt feedback to practitioners for priority review. Since collecting sufficient labeled bug data is not always feasible for some mobile apps, one possible approach is to leverage cross-app models. In this work, we propose a new cross-triplet deep feature embedding method, called CDFE, for cross-app JIT bug prediction task. The CDFE method incorporates a state-of-the-art cross-triplet loss function into a deep neural network to learn high-level feature representation for the cross-app data. This loss function adapts to the cross-app feature learning task and aims to learn a new feature space to shorten the distance of commit instances with the same label and enlarge the distance of commit instances with different labels. In addition, this loss function assigns higher weights to losses caused by cross-app instance pairs than that by intra-app instance pairs, aiming to narrow the discrepancy of cross-app bug data. We evaluate our CDFE method on a benchmark bug dataset from 19 mobile apps with two effort-aware indicators. The experimental results on 342 cross-app pairs show that our proposed CDFE method performs better than 14 baseline methods.","1558-1721","","10.1109/TR.2021.3066170","National Key Research and Development Project(grant numbers:2018YFB2101200); National Natural Science Foundation of China(grant numbers:62002034); Fundamental Research Funds for the Central Universities(grant numbers:2020CDCGRJ072 2020CDJQY-A021); China Postdoctoral Science Foundation(grant numbers:2020M673137); Natural Science Foundation of Chongqing in China(grant numbers:cstc2020jcyj-bshX0114); Science and Technology Development Fund of Macau(grant numbers:0047/2020/A1); Faculty Research Grant Projects of MUST(grant numbers:FRG-20-008-FI); European Commission(grant numbers:825040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9400509","Cross-triplet feature embedding;effort-aware performance;just-in-time bug prediction;metric learning;mobile app bug prediction","Computer bugs;Predictive models;Mobile applications;Feature extraction;Deep learning;Task analysis;Data models","","","","","","","IEEE","12 Apr 2021","","","IEEE","IEEE Early Access Articles"
"The Impact of Mislabeled Changes by SZZ on Just-in-Time Defect Prediction","Y. Fan; X. Xia; D. A. da Costa; D. Lo; A. E. Hassan; S. Li","College of Computer Science and Technology, Ningbo Research Institute, Zhejiang University, Hangzhou, China; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Information Science Department, University of Otago, Dunedin, New Zealand; School of Information Systems, Singapore Management University, Singapore, Singapore; School of Computing, Queen's University, Kingston, ON, Canada; College of Computer Science and Technology, Ningbo Research Institute, Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","12 Aug 2021","2021","47","8","1559","1586","Just-in-Time (JIT) defect prediction-a technique which aims to predict bugs at change level-has been paid more attention. JIT defect prediction leverages the SZZ approach to identify bug-introducing changes. Recently, researchers found that the performance of SZZ (including its variants) is impacted by a large amount of noise. SZZ may considerably mislabel changes that are used to train a JIT defect prediction model, and thus impact the prediction accuracy. In this paper, we investigate the impact of the mislabeled changes by different SZZ variants on the performance and interpretation of JIT defect prediction models. We analyze four SZZ variants (i.e., B-SZZ, AG-SZZ, MA-SZZ, and RA-SZZ) that are proposed by prior studies. We build the prediction models using the labeled data by these four SZZ variants. Among the four SZZ variants, RA-SZZ is least likely to generate mislabeled changes, and we construct the testing set by using RA-SZZ. All of the four prediction models are then evaluated on the same testing set. We choose the prediction model built on the labeled data by RA-SZZ as the baseline model, and we compare the performance and metric importance of the models trained using the labeled data by the other three SZZ variants with the baseline model. Through a large-scale empirical study on a total of 126,526 changes from ten Apache open source projects, we find that in terms of various performance measures (AUC, F1-score, G-mean and Recall@20%), the mislabeled changes by B-SZZ and MA-SZZ are not likely to cause a considerable performance reduction, while the mislabeled changes by AG-SZZ cause a statistically significant performance reduction with an average difference of 1-5 percent. When considering developers' inspection effort (measured by LOC) in practice, the changes mislabeled B-SZZ and AG-SZZ lead to 9-10 and 1-15 percent more wasted inspection effort, respectively. And the mislabeled changes by B-SZZ lead to significantly more wasted effort. The mislabeled changes by MA-SZZ do not cause considerably more wasted effort. We also find that the top-most important metric for identifying bug-introducing changes (i.e., number of files modified in a change) is robust to the mislabeling noise generated by SZZ. But the second- and third-most important metrics are more likely to be impacted by the mislabeling noise, unless random forest is used as the underlying classifier.","1939-3520","","10.1109/TSE.2019.2929761","National Key Research and Development Program of China(grant numbers:2018YFB1003904); NSFC Program(grant numbers:61602403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765743","Just-in-time defect prediction;SZZ;noisy data;mining software repositories","Predictive models;Data models;Computer bugs;Measurement;Inspection;Analytical models;Testing","data mining;just-in-time;program debugging;program testing;public domain software;software maintenance;software metrics","RA-SZZ;mislabeled changes;baseline model;MA-SZZ;bug-introducing change identification;JIT defect prediction model;bug detection;just-in-time defect prediction;B-SZZ;AG-SZZ;testing set;labeled data;mining software repositories","","3","","85","IEEE","18 Jul 2019","","","IEEE","IEEE Journals"
"Simplified Deep Forest Model Based Just-in-Time Defect Prediction for Android Mobile Apps","K. Zhao; Z. Xu; T. Zhang; Y. Tang; M. Yan","School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; Faculty of Information Technology, Macau University of Science and Technology, Taipa, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China","IEEE Transactions on Reliability","8 Jun 2021","2021","70","2","848","859","The popularity of mobile devices has led to an explosive growth in the number of mobile apps in which Android mobile apps are the mainstream. Android mobile apps usually undergo frequent update due to new requirements proposed by users. Just-in-time (JIT) defect prediction is appropriate for this scenario for quality assurance because it can provide timely feedback by determining whether a new code commit will introduce defects into the apps. As defect-prediction performance usually relies on the quality of the data representation and the used classification model, in this work, we propose a model, called Simplified Deep Forest (SDF), to conduct JIT defect prediction for Android mobile apps. SDF modifies a state-of-the-art deep forest model by removing the multigrained scanning operation that is designed for data with a high-dimensional feature space. It uses a cascade structure with ensemble forests for representation learning and classification. We conduct experiments on 10 Android mobile apps and experimental results show that SDF performs significantly better than comparative methods in terms of 3 performance indicators.","1558-1721","","10.1109/TR.2021.3060937","National Natural Science Foundation of China(grant numbers:62002034); China Postdoctoral Science Foundation(grant numbers:2020M673137); Fundamental Research Funds for the Central Universities(grant numbers:2020CDCGRJ072,2020CDJQY-A021); Natural Science Foundation of Chongqing in China(grant numbers:cstc2020jcyj-bshX0114); Science and Technology Development Fund of Macau(grant numbers:0047/2020/A1); Faculty Research Grant Projects of MUST(grant numbers:FRG-20-008-FI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380555","Deep forest;feature representation learning;just-in-time (JIT) defect prediction;mobile apps;quality assurance","Forestry;Mobile applications;Predictive models;Feature extraction;Task analysis;Open source software;Vegetation","just-in-time;learning (artificial intelligence);mobile computing","JIT defect prediction;state-of-the-art deep forest model;10 Android mobile apps;Simplified Deep Forest model;just-in-time defect prediction;defect-prediction performance","","1","","50","IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Watch out for Extrinsic Bugs! A Case Study of their Impact in Just-In-Time Bug Prediction Models on the OpenStack project","G. Rodriguezperez; M. Nagappan; G. Robles","Cheriton School of Computer Science, University of Waterloo, 8430 Waterloo, Ontario, Canada, (e-mail: gema.rodriguez-perez@uwaterloo.ca); Cheriton School of Computer Science, University of Waterloo, 8430 Waterloo, Ontario, Canada, (e-mail: mei.nagappan@uwaterloo.ca); Sistemas Telematicos y Computacion, Universidad Rey Juan Carlos, 16776 Madrid, Madrid, Spain, (e-mail: grex@gsyc.urjc.es)","IEEE Transactions on Software Engineering","","2020","PP","99","1","1","Intrinsic bugs are bugs for which a bug-introducing change can be identified in the version control system of a software. In contrast, extrinsic bugs are caused by external changes to a software, such as errors in external APIs; thereby they do not have an explicit bug-introducing change in the version control system. Although most previous research literature has assumed that all bugs are of intrinsic nature, in a previous study, we show that not all bugs are intrinsic. This paper shows an example of how considering extrinsic bugs can affect software engineering research. Specifically, we study the impact of extrinsic bugs in Just-In-Time bug prediction by partially replicating a recent study by McIntosh and Kamei on JIT models. These models are trained using properties of earlier bug-introducing changes. Since extrinsic bugs do not have bug-introducing changes in the version control system, we manually curate McIntosh and Kamei's dataset to distinguish between intrinsic and extrinsic bugs. Then, we address their original research questions, this time removing extrinsic bugs, to study whether bug-introducing changes are a moving target in Just-In-Time bug prediction. Finally, we study whether characteristics of intrinsic and extrinsic bugs are different. Our results show that intrinsic and extrinsic bugs are of different nature. When removing extrinsic bugs the performance is different up to 16 % Area Under the Curve points. This indicates that our JIT models obtain a more accurate representation of the real world. We conclude that extrinsic bugs negatively impact Just-In-Time models. Furthermore, we offer evidence that extrinsic bugs should be further investigated, as they can significantly impact how software engineers understand bugs.","1939-3520","","10.1109/TSE.2020.3021380","Ministerio de Economa Industria y Competitividad Gobierno de Espaa(grant numbers:RTI2018-101963-B-100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185031","Bugs;Extrinsic Bugs;Intrinsic Bugs;Mislabeled Bugs;Bug-introducing changes;Just-In-Time;Bug Prediction","Computer bugs;Predictive models;Software;Data models;Control systems;Analytical models;Context modeling","","","","","","","IEEE","2 Sep 2020","","","IEEE","IEEE Early Access Articles"
"Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction","S. McIntosh; Y. Kamei","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Principles of Software Languages Group (POSL), Kyushu University, Fukuoka, Japan","IEEE Transactions on Software Engineering","14 May 2018","2018","44","5","412","428","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","1939-3520","","10.1109/TSE.2017.2693980","Natural Sciences and Engineering Research Council of Canada (NSERC); JSPS KAKENHI(grant numbers:15H05306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898457","Just-In-Time prediction;defect prediction;mining software repositories","Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration","data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software)","Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories","","25","","48","IEEE","12 Apr 2017","","","IEEE","IEEE Journals"
"Code Churn: A Neglected Metric in Effort-Aware Just-in-Time Defect Prediction","J. Liu; Y. Zhou; Y. Yang; H. Lu; B. Xu","State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","11","19","Background: An increasing research effort has devoted to just-in-time (JIT) defect prediction. A recent study by Yang et al. at FSE'16 leveraged individual change metrics to build unsupervised JIT defect prediction model. They found that many unsupervised models performed similarly to or better than the state-of-the-art supervised models in effort-aware JIT defect prediction. Goal: In Yang et al.'s study, code churn (i.e. the change size of a code change) was neglected when building unsupervised defect prediction models. In this study, we aim to investigate the effectiveness of code churn based unsupervised defect prediction model in effort-aware JIT defect prediction. Methods: Consistent with Yang et al.'s work, we first use code churn to build a code churn based unsupervised model (CCUM). Then, we evaluate the prediction performance of CCUM against the state-of-the-art supervised and unsupervised models under the following three prediction settings: cross-validation, time-wise cross-validation, and cross-project prediction. Results: In our experiment, we compare CCUM against the state-of-the-art supervised and unsupervised JIT defect prediction models. Based on six open-source projects, our experimental results show that CCUM performs better than all the prior supervised and unsupervised models. Conclusions: The result suggests that future JIT defect prediction studies should use CCUM as a baseline model for comparison when a novel model is proposed.","","978-1-5090-4039-1","10.1109/ESEM.2017.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8169980","Defect;prediction;changes;just-in-time;code churn;unsupervised models;supervised models","Predictive models;Measurement;Software;Buildings;Data models;Linear regression;Computational modeling","learning (artificial intelligence);public domain software;software metrics;source code (software)","code churn;effort-aware just-in-time defect;just-in-time defect prediction;unsupervised JIT defect prediction model;effort-aware JIT defect prediction;code change;CCUM;cross-project prediction;supervised JIT defect prediction models","","8","","39","","11 Dec 2017","","","IEEE","IEEE Conferences"
"How Well Just-In-Time Defect Prediction Techniques Enhance Software Reliability?","Y. Tian; N. Li; J. Tian; W. Zheng","Northwestern Polytechnical University,School of Computer Science,Xi‚Äôan,China; Northwestern Polytechnical University,School of Computer Science,Xi‚Äôan,China; Southern Methodist University,Department of Computer Science,Dallas,TX,USA; Northwestern Polytechnical University,School of Software,Xi‚Äôan,China","2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)","11 Dec 2020","2020","","","212","221","Many Just-In-Time defect prediction (JIT) techniques, which anticipate defect-prone software changes, have been proposed in recent years. Researchers have evaluated these techniques from different perspectives and have drawn inconsistent conclusions about which JIT defect prediction techniques are the most effective and efficient. This paper evaluates JIT techniques from a reliability perspective. For short-term early evaluation, we measure JIT predictive performance on early exposed defects. While for long-term evaluation, we quantify the overall reliability improvement resulted from JIT. A case study applying 11 state-of-the-art JIT methods on 18 large open-source projects has shown: 1) Different JIT methods have their own individual strengths for different purposes, 2) in general, RandomForest is the most effective method in short-term software reliability improvement, and CBS+ performs best in long-term reliability improvement; 3) JIT prediction accuracy is highly correlated to overall reliability improvement.","","978-1-7281-8913-0","10.1109/QRS51102.2020.00038","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282785","Defect prediction;Just-in-time;Software reliability","Software quality;Predictive models;Reliability engineering;Software reliability;Software measurement;Security;Open source software","just-in-time;public domain software;random forests;software reliability","defect-prone software changes;JIT defect prediction techniques;JIT techniques;long-term evaluation;short-term software reliability improvement;just-in-time defect prediction techniques;JIT predictive performance;random forest","","","","23","","11 Dec 2020","","","IEEE","IEEE Conferences"
"JITBot: An Explainable Just-In-Time Defect Prediction Bot","C. Khanan; W. Luewichana; K. Pruktharathikoon; J. Jiarpakdee; C. Tantithamthavorn; M. Choetkiertikul; C. Ragkhitwetsagul; T. Sunetnanta","Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand; Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand; Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand; Monash University,Faculty of Information Technology (FIT),Melbourne,Australia; Monash University,Faculty of Information Technology (FIT),Melbourne,Australia; Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand; Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand; Mahidol University,Faculty of Information and Communication Technology (ICT),Bangkok,Thailand","2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)","24 Dec 2020","2020","","","1336","1339","Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explain-ability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/.","2643-1572","978-1-4503-6768-4","","ARC(grant numbers:DE200100941); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286007","Software Quality Assurance;Just-In-Time Defect Prediction","Pipelines;Software quality;Predictive models;Risk management;Monitoring;Software development management;Software engineering","just-in-time;learning (artificial intelligence);pattern classification;program debugging;program testing;project management;public domain software;risk management;software engineering","bug-introducing changes;explain-ability;software analytics applications;commits;modern software development platforms;Just-In-Time defect prediction framework","","","","11","","24 Dec 2020","","","IEEE","IEEE Conferences"
"DeepJIT: An End-to-End Deep Learning Framework for Just-in-Time Defect Prediction","T. Hoang; H. Khanh Dam; Y. Kamei; D. Lo; N. Ubayashi",Singapore Management University; University of Wollongong; Kyushu University; Singapore Management University; Kyushu University,"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","34","45","Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36-11.02% for the project QT and 9.51-13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC).","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816772","deep learning;just-in-time defect prediction;convolutional neural network","Feature extraction;Software;Predictive models;Convolutional codes;Deep learning;Natural language processing;Testing","learning (artificial intelligence);software fault tolerance;software quality","end-to-end deep learning framework;just-in-time defect prediction;software quality assurance efforts;defective code;change-level defect prediction;JIT defect prediction models;defective changes;historical changes;code changes;DeepJIT;JIT defect prediction approaches;machine learning techniques","","8","","78","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Cross-Project Just-in-Time Bug Prediction for Mobile Apps: An Empirical Assessment","G. Catolino; D. Di Nucci; F. Ferrucci",University of Salerno; Vrije Universiteit Brussel; University of Salerno,"2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft)","29 Aug 2019","2019","","","99","110","Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42, 543 commits extracted from the COMMIT GURU platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.","","978-1-7281-3395-9","10.1109/MOBILESoft.2019.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816921","JIT Bug Prediction;Metrics;Empirical Study","Computer bugs;Predictive models;Measurement;Mobile applications;Context modeling;Feature extraction;Data models","Bayes methods;just-in-time;learning (artificial intelligence);mobile computing;pattern classification;program debugging;program testing;public domain software;software metrics","cross-project just-in-time bug prediction;mobile apps;empirical assessment;defect-prone source code entities;software systems;research community;commit-level;mobile environment;within-project scenario;mobile context;cross-project JIT bug prediction models;feature selection technique;ensemble techniques;classifiers impact;Naive Bayes;multicollinearity","","3","","91","","29 Aug 2019","","","IEEE","IEEE Conferences"
"A Just-in-Time Compiler for a Reconfigurable Testing Platform","M. El-Kadri; V. Groza; R. Abielmona; M. Assaf","School of Information Technology and Engineering, University of Ottawa, Ontario, Canada; School of Information Technology and Engineering, University of Ottawa, Ontario, Canada; School of Information Technology and Engineering, University of Ottawa, Ontario, Canada; School of Information Technology and Engineering, University of Ottawa, Ontario, Canada","2006 IEEE Instrumentation and Measurement Technology Conference Proceedings","12 Mar 2007","2006","","","628","632","With the evolution of reconfigurable architectures emerged the field of run-time reconfiguration (RTR). This paper presents a just-in-time (JIT) compiler for a reconfigurable testing platform making use of the ISCAS 85 combinational benchmark circuits making use of the stuck-at fault model. The compiler is discussed in detail as well as some preliminary results of the platform running different circuit under test (CUTs). This paper is part of the ERACE (Groza et al., 2004) project at the GEMS research lab","1091-5281","0-7803-9359-7","10.1109/IMTC.2006.328637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124402","embedded cores-based systems;circuit under test (CUT);test pattern generator (TPG);FPGA;Embedded RTOS/MicroBlaze Softcore Processor","Circuit testing;Circuit faults;Coprocessors;Hardware;Central Processing Unit;System testing;Fault detection;Benchmark testing;Test pattern generators;Field programmable gate arrays","automatic test pattern generation;fault simulation;field programmable gate arrays;just-in-time;program compilers;reconfigurable architectures","just-in-time compiler;reconfigurable testing platform;reconfigurable architectures;run-time reconfiguration;stuck-at fault model;circuit under test;ERACE project;GEMS research lab;embedded cores-based systems;test pattern generator;FPGA;embedded RTOS-MicroBlaze Softcore Processor","","1","","7","","12 Mar 2007","","","IEEE","IEEE Conferences"
"The Relationship between Commit Message Detail and Defect Proneness in Java Projects on GitHub","J. G. Barnett; C. K. Gathuru; L. S. Soldano; S. McIntosh","McGill Univ., Montr√©al, QC, Canada; McGill Univ., Montr√©al, QC, Canada; McGill Univ., Montr√©al, QC, Canada; McGill Univ., Montr√©al, QC, Canada","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","26 Jan 2017","2016","","","496","499","Just-In-Time (JIT) defect prediction models aim to predict the commits that will introduce defects in the future. Traditionally, JIT defect prediction models are trained using metrics that are primarily derived from aspects of the code change itself (e.g., the size of the change, the author's prior experience). In addition to the code that is submitted during a commit, authors write commit messages, which describe the commit for archival purposes. It is our position that the level of detail in these commit messages can provide additional explanatory power to JIT defect prediction models. Hence, in this paper, we analyze the relationship between the defect proneness of commits and commit message volume (i.e., the length of the commit message) and commit message content (approximated using spam filtering technology). Through analysis of JIT models that were trained using 342 GitHub repositories, we find that our JIT models outperform random guessing models, achieving AUC and Brier scores that range between 0.63-0.96 and 0.01-0.21, respectively. Furthermore, our metrics that are derived from commit message detail provide a statistically significant boost to the explanatory power to the JIT models in 43%-80% of the studied systems, accounting for up to 72% of the explanatory power. Future JIT studies should consider adding commit message detail metrics.","","978-1-4503-4186-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832934","Just-In-Time Modeling;Commit Message Detail;Spam Filtering","Measurement;Solid modeling;Analytical models;Computational modeling;Predictive models;Java;Bayes methods","configuration management;Java;just-in-time;project management","Java Projects;GitHub;JIT defect prediction models;just-in-time defect prediction models;commit message volume;commit message content;AUC;Brier scores","","2","","15","","26 Jan 2017","","","IEEE","IEEE Conferences"
"A Replication Study: Just-in-Time Defect Prediction with Ensemble Learning","S. Young; T. Abdou; A. Bener","Data Sci. Lab., Ryerson Univ., Toronto, ON, Canada; Ryerson Univ., Toronto, ON, Canada; Data Sci. Lab., Ryerson Univ., Toronto, ON, Canada","2018 IEEE/ACM 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)","2 Sep 2018","2018","","","42","47","Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score.","","978-1-4503-5723-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452881","Supervised learning by classification;Machine learning approaches;Ensemble methods;Software defect analysis;Deep Learning;Defect Prediction","DSL;Forestry;Machine learning;Prediction algorithms;Classification algorithms;Entropy;Decision trees","decision trees;learning (artificial intelligence);pattern classification;program debugging;program testing;public domain software;software fault tolerance","replication study;just-in-time defect prediction;ensemble learning;change-level defect prediction;software testing;debugging process;defect-prone changes;machine learning;deep ensemble;Bugzilla;Columba;JDT;Platform;Mozilla;PostgreSQL;open source projects;data preprocessing;multiple random forests;classifiers;deep super learner","","","","8","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction","A. Trautsch; S. Herbold; J. Grabowski","University of Goettingen,Institute of Computer Science,Germany; Karlsruhe Institute of Technology,Institute AIFB,Germany; University of Goettingen,Institute of Computer Science,Germany","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","127","138","Software quality evolution and predictive models to support decisions about resource distribution in software quality assurance tasks are an important part of software engineering research. Recently, a fine-grained just-in-time defect prediction approach was proposed which has the ability to find bug-inducing files within changes instead of only complete changes. In this work, we utilize this approach and improve it in multiple places: data collection, labeling and features. We include manually validated issue types, an improved SZZ algorithm which discards comments, whitespaces and refactorings. Additionally, we include static source code metrics as well as static analysis warnings and warning density derived metrics as features. To assess whether we can save cost we incorporate a specialized defect prediction cost model. To evaluate our proposed improvements of the fine-grained just-in-time defect prediction approach we conduct a case study that encompasses 38 Java projects, 492,241 file changes in 73,598 commits and spans 15 years. We find that static source code metrics and static analysis warnings are correlated with bugs and that they can improve the quality and cost saving potential of just-in-time defect prediction models.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240701","Software quality;Software metrics","Measurement;Java;Computer bugs;Static analysis;Software quality;Predictive models;Labeling","Java;program debugging;program diagnostics;quality assurance;software maintenance;software metrics;software quality;source code (software)","static source code metrics;static analysis warnings;software quality evolution;predictive models;software quality assurance tasks;software engineering research;improved SZZ algorithm;specialized defect prediction cost model;just-in-time defect prediction models;just-in-time defect prediction;time 15.0 year","","1","","47","","2 Nov 2020","","","IEEE","IEEE Conferences"
"An Investigation of Cross-Project Learning in Online Just-In-Time Software Defect Prediction","S. Tabassum; L. L. Minku; D. Feng; G. G. Cabral; L. Song","University of Birmingham,UK; University of Birmingham,UK; Xiliu Tech,China; Federal Rural University of Pernambuco,Brazil; University of Birmingham,UK","2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)","21 Dec 2020","2020","","","554","565","Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.","1558-1225","978-1-4503-7121-6","","EPSRC(grant numbers:EP/R006660/2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283981","Software defect prediction;cross-project learning;transfer learning;online learning;verification latency;concept drift;class imbalance","Training;Filtering;Training data;Software;Data models;Software engineering;Software development management","data mining;just-in-time;learning (artificial intelligence);pattern classification;public domain software;software metrics;software quality","cross-project JIT-SDP;cross-project learning;online just-in-time software defect prediction;WP classifiers;10 active open source GitHub projects;software repositories;WP software changes;realistic online learning scenario;within-project data;software project;machine learning classifiers","","","","35","","21 Dec 2020","","","IEEE","IEEE Conferences"
"Supervised vs Unsupervised Models: A Holistic Look at Effort-Aware Just-in-Time Defect Prediction","Q. Huang; X. Xia; D. Lo","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","159","170","Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15% more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.","","978-1-5386-0992-7","10.1109/ICSME.2017.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094418","Change Classification;Cost Effectiveness;Evaluation;Bias","Predictive models;Inspection;Measurement;Computer bugs;Analytical models;Feature extraction;Software","inspection;just-in-time;learning (artificial intelligence);program debugging;public domain software;software cost estimation;software maintenance;software metrics","inspection budget;unsupervised model;code inspection cost;LT;defective software;historical bug data;effort-aware just-in-time defect prediction;JIT defect prediction;CBS;EALR;harmonic mean","","17","","56","","7 Nov 2017","","","IEEE","IEEE Conferences"
"The Impact of Data Merging on the Interpretation of Cross-Project Just-In-Time Defect Models","D. Lin; C. Tantithamthavorn; A. E. Hassan","Centre for Software Excellence, Huawei Technologies Co Ltd Canada, 538302 Kingston, Ontario, Canada, (e-mail: dayi.lin@huawei.com); Faculty of Information Technology, Monash University, 2541 Clayton, Victoria, Australia, (e-mail: chakkrit@monash.edu); School of Computing, Queen's University, Kingston, Ontario, Canada, (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","Just-In-Time (JIT) defect models are classification models that identify the code commits that are likely to introduce defects. Cross-project JIT models have been introduced to address the suboptimal performance of JIT models when historical data is limited. However, many studies built cross-project JIT models using a pool of mixed data from multiple projects (i.e., data merging)---assuming that the properties of defect-introducing commits of a project are similar to that of the other projects, which is likely not true. In this paper, we set out to investigate the interpretation of JIT defect models that are built from individual project data and a pool of mixed project data with and without consideration of project-level variances. Through a case study of 20 datasets of open source projects, we found that (1) the interpretation of JIT models that are built from individual projects varies among projects; and (2) the project-level variances cannot be captured by a JIT model that is trained from a pool of mixed data from multiple projects without considering project-level variances (i.e., a global JIT model). On the other hand, a mixed-effect JIT model that considers project-level variances represents the different interpretations better, without sacrificing performance, especially when the contexts of projects are considered. The results hold for different mixed-effect learning algorithms. When the goal is to derive sound interpretation of cross-project JIT models, we suggest that practitioners and researchers should opt to use a mixed-effect modelling approach that considers individual projects and contexts.","1939-3520","","10.1109/TSE.2021.3073920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408228","Just-In-Time Defect Prediction;Data Merging;Mixed-Effect Model;Cross-Project Defect Prediction","Context modeling;Data models;Predictive models;Measurement;Training;Merging;Planning","","","","","","","IEEE","19 Apr 2021","","","IEEE","IEEE Early Access Articles"
"A large-scale empirical study of just-in-time quality assurance","Y. Kamei; E. Shihab; B. Adams; A. E. Hassan; A. Mockus; A. Sinha; N. Ubayashi","Kyushu University, Fukuoka; Rochester Institute of Technology, Rochester; √âcole Polytechnique de Montr√©al, Montr√©al; Queen's University, Kingston; Avaya Labs Research, Basking Ridge; Research In Motion, Waterloo; Kyushu University, Fukuoka","IEEE Transactions on Software Engineering","23 May 2013","2013","39","6","757","773","Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (‚Äúrisky‚Äù) software changes instead of files or packages. We refer to this type of quality assurance activity as ‚ÄúJust-In-Time Quality Assurance,‚Äù because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that ‚ÄúJust-In-Time Quality Assurance‚Äù may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.","1939-3520","","10.1109/TSE.2012.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341763","Maintenance;software metrics;mining software repositories;defect prediction;just-in-time prediction","Measurement;Quality assurance;Predictive models;Software;Entropy;Object oriented modeling;Accuracy","program testing;software maintenance;software metrics;software quality","just-in-time quality assurance;defect prediction models;defect-prone file identification;defect-prone package identification;software systems;risk model;open source projects;commercial projects;risky changes;cost reduction;defect-prone software change identification;software metrics;software repository mining;software quality assurance activities;source code inspection;unit testing","","196","1","63","","10 Nov 2012","","","IEEE","IEEE Journals"
"Deep Semantic Feature Learning for Software Defect Prediction","S. Wang; T. Liu; J. Nam; L. Tan","Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; School of Computer Science and Electrical Engineering, Handong Global University, Pohang, Korea; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Software Engineering","10 Dec 2020","2020","46","12","1267","1293","Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.","1939-3520","","10.1109/TSE.2018.2877612","Natural Sciences and Engineering Research Council of Canada; National Research Foundation of Korea(grant numbers:2018R1C1B6001919); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502853","Defect prediction;quality assurance;deep learning;semantic features","Semantics;Predictive models;Feature extraction;Quality assurance;Computer bugs;Data models;Prediction models","abstract data types;belief networks;feature extraction;learning (artificial intelligence);program debugging;program testing;programming language semantics;software quality;tree data structures","deep semantic feature learning;software defect prediction;defective code regions;semantic differences;semantic representations;source code files;file-level defect prediction models;source code changes;change-level defect prediction models;file-level within-project defect prediction;change-level within-project defect prediction;DBN-based semantic features;file-level cross-project defect prediction;change-level cross-project defect prediction;representation-learning algorithm;defect prediction features;deep belief network;token vectors;abstract syntax trees","","19","","111","IEEE","23 Oct 2018","","","IEEE","IEEE Journals"
"A Preliminary Evaluation of CPDP Approaches on Just-in-Time Software Defect Prediction","S. Amasaki; H. Aman; T. Yokogawa","Okayama Prefectural University,Department of Systems Engineering,Soja,Japan; Center for Information Technology Ehime University,Matsuyama,Japan; Okayama Prefectural University,Department of Systems Engineering,Soja,Japan","2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","27 Oct 2021","2021","","","279","286","CONTEXT: Just-in-Time defect prediction is to specify the suspicious code commits that might make a product cause defects. Building JIT defect prediction models require a commit history and their fixed defect records. The shortage of commits of new projects motivated research of JIT cross-project defect prediction (CPDP). CPDP approaches proposed for component-level defect prediction were barely evaluated under JIT CPDP. OBJECTIVE: To explore the effects of CPDP approaches for component-level defect prediction where JIT CPDP is adopted. METHOD: A case study was conducted through two commit dataset suites provided in past studies for JIT defect prediction. JIT defect predictions with and without 21 CPDP approaches were compared regarding the classification performance using AUC. The CPDP approaches were also compared with each other. RESULTS: Most CPDP approaches changed the prediction performance of a baseline that simply combined all CP data. A few CPDP approaches could improve the prediction performance significantly. Not a few approaches worsened the performance significantly. The results based on the two suites could specify two CPDP approaches safer than the baseline. The results were inconsistent with a previous study. CONCLUSIONS: CPDP approaches for component-level might be effective for JIT CPDP. Further evaluations were needed to bring a firm conclusion.","","978-1-6654-2705-0","10.1109/SEAA53835.2021.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582597","just-in-time defect prediction;cross-project defect prediction;empirical study","Codes;Buildings;Predictive models;Software;History;Software engineering","","","","","","47","","27 Oct 2021","","","IEEE","IEEE Conferences"
"The Impact of Human Discussions on Just-in-Time Quality Assurance: An Empirical Study on OpenStack and Eclipse","P. Tourani; B. Adams","Polytech. Montreal, Montreal, ON, Canada; Polytech. Montreal, Montreal, ON, Canada","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","23 May 2016","2016","1","","189","200","In order to spot defect-introducing code changes during review before they are integrated into a project's version control system, a variety of defect prediction models have been designed. Most of these models focus exclusively on source code properties, like the number of added or deleted lines, or developer-related measures like experience. However, a code change is only the outcome of a much longer process, involving discussions on an issue report and review discussions on (different versions of) a patch. % Ignoring the characteristics of these activities during prediction is unfortunate, since Similar to how body language implicitly can reveal a person's real feelings, the length, intensity or positivity of these discussions can provide important additional clues about how risky a particular patch is or how confident developers and reviewers are about the patch. In this paper, we build logistic regression models to study the impact of the characteristics of issue and review discussions on the defect-proneness of a patch. Comparison of these models to conventional source code-based models shows that issue and review metrics combined improve precision and recall of the explanatory models up to 10%. Review time and issue discussion lag are amongst the most important metrics, having a positive (i.e., increasing) relation with defect-proneness.","","978-1-5090-1855-0","10.1109/SANER.2016.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476642","Just-In-Time Quality Assurance;Human Discussion Metrics","Measurement;Predictive models;Software;Control systems;Biological system modeling;Data models;Databases","configuration management;just-in-time;project management;quality assurance;regression analysis;software fault tolerance;software quality;software reviews;source code (software)","human discussions;just-in-time quality assurance;OpenStack;Eclipse;defect-introducing code changes;project version control system;defect prediction models;source code properties;developer-related measures;person real feelings;logistic regression models;source code-based models;review metrics;precision;recall;review time;issue discussion lag;defect-proneness","","7","","45","","23 May 2016","","","IEEE","IEEE Conferences"
"An Implementation of Just-in-Time Fault-Prone Prediction Technique Using Text Classifier","K. Mori; O. Mizuno","Grad. Sch. of Sci. & Technol., Kyoto Inst. of Technol., Kyoto, Japan; Grad. Sch. of Sci. & Technol., Kyoto Inst. of Technol., Kyoto, Japan","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","3","","609","612","Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named ""Fault-Prone Filtering"". Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273434","software development support tool;fault prediction;software maintenance;mining software repository;machine learning;spam filter","Predictive models;Filtering;Data mining;Accuracy;Software maintenance;Databases","information filters;pattern classification;public domain software;software fault tolerance;software maintenance;source code (software);text analysis;Web sites","just-in-time fault-prone prediction technique;text classifier;fault prediction;software maintenance;software development;prototype fault-prone module prediction tool;text-filtering based technique;fault-prone filtering;probability;source code;Web page;open source project","","2","","9","","24 Sep 2015","","","IEEE","IEEE Conferences"
"Leveraging Fault Localisation to Enhance Defect Prediction","J. Sohn; Y. Kamei; S. McIntosh; S. Yoo","KAIST,Daejeon,Korea; Kyushu University,Kyushu,Japan; University of Waterloo,Waterloo,Canada; KAIST,Daejeon,Korea","2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","11 May 2021","2021","","","284","294","Software Quality Assurance (SQA) is a resource constrained activity. Research has explored various means of sup-porting that activity. For example, to aid in resource investment decisions, defect prediction identifies modules or changes that are likely to be defective in the future. To support repair activities, fault localisation identifies areas of code that are likely to require change to address known defects. Although the identification and localisation of defects are interdependent tasks, the synergy between defect prediction and fault localisation remains largely underexplored.We hypothesise that modifying code that was suspicious in the past is riskier than modifying code that was not. To validate our hypothesis, in this paper, we employ fault localisation, which localises the root cause of a program failure. We compute the past suspiciousness score of code changes to each fault, and use those scores to (1) define new features for training defect prediction models; and (2) guide the next actions of developers for a commit labelled as fix-inducing. An empirical study of three open-source projects confirms our hypothesis. The new suspiciousness features improve F1 score and balanced accuracy of Just-In-Time (JIT) defect prediction models by 4.2% to 92.2% and by 1.2% to 3.7%, respectively. When guiding developer actions, past code suspiciousness successfully guides developers to a defective file, inspecting two to nine fewer files on average, compared to the baselines based on previous findings on past faults. These results demonstrate the potential of synergies of fault localisation and defect prediction, and lay the groundwork for explorations of that combined space.","1534-5351","978-1-7281-9630-5","10.1109/SANER50967.2021.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425917","defect prediction;fault localisation;search-based software engineering;software quality assurance","Fault diagnosis;Training;Quality assurance;Computational modeling;Software quality;Predictive models;Maintenance engineering","investment;just-in-time;program debugging;program diagnostics;program testing;public domain software;quality assurance;software fault tolerance;software metrics;software quality","fault localisation;modifying code;code changes;training defect prediction models;Just-In-Time defect prediction models;code suspiciousness;defective file;enhance defect prediction;resource constrained activity;known defects","","","","36","","11 May 2021","","","IEEE","IEEE Conferences"
"The Impact of Duplicate Changes on Just-in-Time Defect Prediction","R. Duan; H. Xu; Y. Fan; M. Yan","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083 China and also with the Yunhe (Henan) Information Technology Company Ltd., Zhengzhou 450000 China (e-mail: duanruifeng0905@163.com).; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083 China (e-mail: alex_xuht@hotmail.com).; College of Computer Science and Technology, Zhejiang University, Hangzhou 310007 China (e-mail: yrfan@zju.edu.cn).; School of Big Data and Software Engineering, Chongqing University, Chongqing 401331 China (e-mail: mengy@cqu.edu.cn).","IEEE Transactions on Reliability","","2021","PP","99","1","15","Recently, just-in-time (JIT) defect prediction technique attracted a lot of attention. In JIT defect prediction, all branches and omitting changes outside the main branch should be considered which can significantly affect the performance of JIT defect prediction. However, there are many duplicate changes among all the branches, which are referred to as a pair of changes with identical implementation in different branches. Such changes can influence the calculation of developer experience metrics and are considered as the noisy data for JIT defect prediction. In this article, the impact of duplicate changes on JIT defect prediction is explored. An empirical study on a total of 105¬†828 changes from eight Apache open-source projects is given. We find that 13% of changes from different branches are duplicate among the studied projects. The duplicate changes have a great influence on the model metrics for JIT defect prediction. For 50% of the changes, removing duplicate changes decreases the experience metrics with an average of 6‚Äì55. In addition, the duplicate changes have a significant impact on the evaluation and interpretation of JIT defect prediction models. Removing duplicate changes among the studied projects can significantly improve the performance of JIT defect prediction models ranging from 1 to 125% concerning various performance measures (i.e., area under the curve, Matthews correlation coefficient, and F1). Given the impact of duplicate changes, we suggest that researchers should remove duplicate changes from the original historical changes of software repository when evaluating the performance of JIT defect prediction models in future work.","1558-1721","","10.1109/TR.2021.3061618","National Science Foundation Project of China(grant numbers:61971032); National Natural Science Foundation of China(grant numbers:62002034); Fundamental Research Funds for the Central Universities(grant numbers:2020CDCGRJ072 2020CDJQY-A021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380201","Branches;just-in-time (JIT) defect prediction;mining software repositories;noisy data","Predictive models;Computer bugs;Measurement;Software;Prediction algorithms;Noise measurement;Fans","","","","","","","IEEE","17 Mar 2021","","","IEEE","IEEE Early Access Articles"
"Automatic Feature Exploration and an Application in Defect Prediction","Y. Qiu; Y. Liu; A. Liu; J. Zhu; J. Xu","College of Artificial Intelligence, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Software, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China","IEEE Access","21 Aug 2019","2019","7","","112097","112112","Many software engineering tasks heavily rely on hand-crafted software features, e.g., defect prediction, vulnerability discovery, software requirements, code review, and malware detection. Previous solutions to these tasks usually directly use the hand-crafted features or feature selection techniques for classification or regression, which usually leads to suboptimal results due to their lack of powerful representations of the hand-crafted features. To address the above problem, in this paper, we adopt the effort-aware just-in-time software defect prediction (JIT-SDP), which is a typical hand-crafted-feature-based task, as an example, to exploit new possible solutions. We propose a new model, named neural forest (NF), which uses the deep neural network and decision forest to build a holistic system for the automatic exploration of powerful feature representations that are used for the following classification. NF first employs a deep neural network to learn new feature representations from hand-crafted features. Then, a decision forest is connected after the neural network to perform classification and in the meantime, to guide the learning of feature representation. NF mainly aims at solving the challenging problem of combining the two different worlds of neural networks and decision forests in an end-to-end manner. When compared with previous state-of-the-art defect predictors and five designed baselines on six well-known benchmarks for within- and cross-project defect prediction, NF achieves significantly better results. The proposed NF model is generic to the classification problems which rely on the hand-crafted features.","2169-3536","","10.1109/ACCESS.2019.2934530","Science and Technology Planning Project of Tianjin(grant numbers:17JCZDJC30700,18ZXZNGX00310); Nankai University(grant numbers:63191402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794540","Feature exploration;hand-crafted features;defect prediction","Feature extraction;Forestry;Software;Noise measurement;Neural networks;Task analysis","feature extraction;feature selection;invasive software;just-in-time;learning (artificial intelligence);neural nets;pattern classification;regression analysis;software metrics;software quality","software engineering tasks;feature selection techniques;deep neural network;feature representation;decision forests;cross-project defect prediction;just-in-time software defect prediction;hand-crafted software features;software requirements;malware detection;regression analysis;neural forest;classification problems;hand-crafted-feature-based task","","2","","99","CCBY","12 Aug 2019","","","IEEE","IEEE Journals"
"Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction","G. G. Cabral; L. L. Minku; E. Shihab; S. Mujahid",University of Birmingham (UK) and Federal Rural University of Pernambuco (Brazil); University of Birmingham (UK); Concordia University (Canada); Concordia University (Canada),"2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","26 Aug 2019","2019","","","666","676","Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that re-build classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency -- the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.","1558-1225","978-1-7281-0869-8","10.1109/ICSE.2019.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812072","Software defect prediction;class imbalance;verification latency;online learning;concept drift;ensembles","Software;Training;Machine learning algorithms;Machine learning;Prediction algorithms;Delays","learning (artificial intelligence);pattern classification;safety-critical software;sampling methods;software fault tolerance","SDP approach;predictive performance;class imbalance evolution approach;just-in-time software defect prediction;JIT-SDP approaches;top ranked g-means","","6","","36","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Revisiting the Impact of Concept Drift on Just-in-Time Quality Assurance","K. E. Bennin; N. b. Ali; J. B√∂rstler; X. Yu","Blekinge Institute of Technology,Department of Software Engineering,Karlskrona,Sweden; Blekinge Institute of Technology,Department of Software Engineering,Karlskrona,Sweden; Blekinge Institute of Technology,Department of Software Engineering,Karlskrona,Sweden; City University of Hong Kong,Department of Computer Science,Kowloon Tong,Hong Kong","2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)","11 Dec 2020","2020","","","53","59","The performance of software defect prediction(SDP) models is known to be dependent on the datasets used for training the models. Evolving data in a dynamic software development environment such as significant refactoring and organizational changes introduces new concept to the prediction model, thus making improved classification performance difficult. In this study, we investigate and assess the existence and impact of concept drift on SDP performances. We empirically asses the prediction performance of five models by conducting cross-version experiments using fifty-five releases of five open-source projects. Prediction performance fluctuated as the training datasets changed over time. Our results indicate that the quality and the reliability of defect prediction models fluctuate over time and that this instability should be considered by software quality teams when using historical datasets. The performance of a static predictor constructed with data from historical versions may degrade over time due to the challenges posed by concept drift.","","978-1-7281-8913-0","10.1109/QRS51102.2020.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282807","Defect prediction;Just-in-time Quality assurance;Concept drift","Training;Software quality;Predictive models;Tools;Software reliability;Security;Random forests","learning (artificial intelligence);pattern classification;software maintenance;software metrics;software quality","SDP performances;prediction performance;cross-version experiments;open-source projects;training datasets;defect prediction models;software quality teams;historical datasets;concept drift;just-in-time quality assurance;evolving data;dynamic software development environment;significant refactoring;organizational changes;prediction model;improved classification performance","","","","25","","11 Dec 2020","","","IEEE","IEEE Conferences"
"VulDigger: A Just-in-Time and Cost-Aware Tool for Digging Vulnerability-Contributing Changes","L. Yang; X. Li; Y. Yu","Dept. of Comput. Sci. & Technol., East China Normal Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. & Technol., East China Normal Univ., Shanghai, China","GLOBECOM 2017 - 2017 IEEE Global Communications Conference","15 Jan 2018","2017","","","1","7","It has been widely adopted to minimize the maintenance cost by predicting potential vulnerabilities before code audits in academia and industry. Most previous research dedicated to file/component level vulnerability prediction models is coarse- grained and may suffer from cost-prohibitive and impractical security testing activities. In this paper, we focus on a cost- aware vulnerability prediction model and present a just-in-time change-level code review tool called VulDigger to dig suspicious ones from a sea of code changes. Our contributions benefit from the case study of Mozilla Firefox by constructing a large-scale vulnerability-contributing changes (VCCs) dataset in a semi-automatic fashion. We then further manifest a classification tool with a mixture of established and new metrics derived from both software defect prediction and vulnerability prediction. Consequently, the precision of such tool is extremely promising (i.e., 92%) for an effort-aware software team. We also examine the return on investment by training a regression model to locate most skeptical changes with fewer lines to inspect. Our findings suggest that such model is capable of pinpointing 31% of all VCCs with only 20% of the effort it would take to audit all changes (i.e., 55% better than random predictor). Our outputs can assist as an early step of continuous security inspections as it provides immediate feedback once developers submit changes to their code base.","","978-1-5090-5019-2","10.1109/GLOCOM.2017.8254428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8254428","","Tools;Predictive models;Security;Measurement;Software;Computer bugs;Complexity theory","pattern classification;program diagnostics;program testing;public domain software;regression analysis;security of data;software quality","code audits;file/component level vulnerability prediction models;impractical security testing activities;just-in-time change-level code review tool;VulDigger;code changes;large-scale vulnerability-contributing changes dataset;cost-prohibitive security testing activities;cost-aware vulnerability prediction model;Mozilla Firefox;VCC;continuous security inspections;maintenance cost;code base;skeptical changes;regression model;effort-aware software team;software defect prediction;classification tool;semiautomatic fashion","","","","25","","15 Jan 2018","","","IEEE","IEEE Conferences"
"File-Level Defect Prediction: Unsupervised vs. Supervised Models","M. Yan; Y. Fang; D. Lo; X. Xia; X. Zhang","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Sch. of Software Eng., Chongqing Univ., Chongqing, China; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Sch. of Software Eng., Chongqing Univ., Chongqing, China","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","344","353","Background: Software defect models can help software quality assurance teams to allocate testing or code review resources. A variety of techniques have been used to build defect prediction models, including supervised and unsupervised methods. Recently, Yang et al. [1] surprisingly find that unsupervised models can perform statistically significantly better than supervised models in effort-aware change-level defect prediction. However, little is known about relative performance of unsupervised and supervised models for effort-aware file-level defect prediction. Goal: Inspired by their work, we aim to investigate whether a similar finding holds in effort-aware file-level defect prediction. Method: We replicate Yang et al.'s study on PROMISE dataset with totally ten projects. We compare the effectiveness of unsupervised and supervised prediction models for effort-aware file-level defect prediction. Results: We find that the conclusion of Yang et al. [1] does not hold under within-project but holds under cross-project setting for file-level defect prediction. In addition, following the recommendations given by the best unsupervised model, developers needs to inspect statistically significantly more files than that of supervised models considering the same inspection effort (i.e., LOC). Conclusions: (a) Unsupervised models do not perform statistically significantly better than state-of-art supervised model under within-project setting, (b) Unsupervised models can perform statistically significantly better than state-ofart supervised model under cross-project setting, (c) We suggest that not only LOC but also number of files needed to be inspected should be considered when evaluating effort-aware filelevel defect prediction models.","","978-1-5090-4039-1","10.1109/ESEM.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170121","Replication Study;Inspection Effort;Effortaware Defect Prediction","Predictive models;Software;Data models;Inspection;Measurement;Logistics;Software engineering","software quality","unsupervised prediction models;software defect models;supervised prediction models;effort-aware file-level defect prediction;effort-aware change-level defect prediction","","9","","52","","11 Dec 2017","","","IEEE","IEEE Conferences"
"A Study of Applying Deep Learning-Based Weighted Combinations to Improve Defect Prediction Accuracy and Effectiveness","C. Huang; Arthur; C. Huang; M. Yang; W. Su","National Tsing Hua University,Department of Computer Science,Hsinchu,Taiwan; National Tsing Hua University,Department of Computer Science,Hsinchu,Taiwan; Institute of Health Policy and Management, National Taiwan University,Taipei,Taiwan; Institute of Health Policy and Management, National Taiwan University,Taipei,Taiwan; Digital Home Center, Realtek Semiconductor Corp.,Taipei,Taiwan","2019 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","3 Feb 2020","2019","","","1471","1475","Software errors or bugs are the primary cause of poor software quality. Thus defect prediction is a prominent approach to enhance software quality. It is a common technique for identifying defect-prone programs, which help the practitioners allocate needed quality assurance efforts (e.g., testing and debugging). An accurate prediction may bring significant benefits. However, there is still space for improvements by applying different levels of instances or using some state-of-the-art techniques to construct the prediction models. In this paper, we propose a weighted combination method of activation functions to improve the effectiveness of defect prediction, comprising of weighted arithmetic, geometric, harmonic, contra-harmonic, and cubic combinations. When there are several kinds of classifiers, the method of weighted combinations can be applied to combine the strengths and create a new model or classifier. That is, weighted combination method(s) would be able to combine the advantage of different activation functions to train the neural network in deep learning. Six open-source projects are used to to evaluate the performance of weighted combination methods of activation functions: single, double- and triple-weighted approaches. Experimental results show that double-weighted combinations of activation function outperform single and triple-weighted combinations. It is also worth noting that single activation function outperform triple-weighted combination.","2157-362X","978-1-7281-3804-6","10.1109/IEEM44572.2019.8978786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978786","Just-In-Time Defect Prediction;Deep Belief Network;Weighted Combination;Activation Function","","learning (artificial intelligence);neural nets;program testing;software metrics;software quality","defect prediction accuracy;bugs;software quality;defect-prone programs;quality assurance efforts;prediction models;weighted combination method;cubic combinations;triple-weighted approaches;double-weighted combinations;triple-weighted combination;single activation function","","","","22","","3 Feb 2020","","","IEEE","IEEE Conferences"
"Continuous Defect Prediction: The Idea and a Related Dataset","L. Madeyski; M. Kawalerowicz","Fac. of Comput. Sci. & Manage., Wroclaw Univ. of Sci. & Technol., Wroclaw, Poland; Fac. of Electr. Eng., Autom. Control & Inf., Opole Univ. of Technol., Opole, Poland","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","515","518","We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if committed to a repository with CI enabled.","","978-1-5386-1544-7","10.1109/MSR.2017.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962410","mining software repositories;defect prediction;continuous defect prediction;software repository;open science","Software;Measurement;Databases;Servers;Predictive models;Tools;History","integrated software;public domain software;software development management;software metrics","continuous defect prediction;CDP;continuous integration;software repositories;software projects;distinct commit authors;software process metrics;software defect prediction;TravisTorrent;commit level information;Travis CI server;GitHub open-source projects repositories;risky software changes","","1","","11","","3 Jul 2017","","","IEEE","IEEE Conferences"
"CLEVER: Combining Code Metrics with Clone Detection for Just-in-Time Fault Prevention and Resolution in Large Industrial Projects","M. Nayrolles; A. Hamou-Lhadj",NA; NA,"2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","153","164","Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phases process for intercepting risky commits before they reach the central repository. CLEVER was developed in collaboration with Ubisoft developers. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79% precision and 65% recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7% of the cases.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595198","Bug Prediction;Risky Software Commits;Clone Detection;Software Maintenance","Measurement;Computer bugs;Tools;Cloning;Software maintenance;Software systems","DP industry;program debugging;software maintenance;software metrics","process metrics;software project;Ubisoft developers;code metrics;clone detection;industrial projects;software maintenance;just-in-time fault prevention;commit-guru;CLEVER;risky commits;false positive rates","","","","47","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Predicting Defects with Latent and Semantic Features from Commit Logs in an Industrial Setting","B. Eken; R. Atar; S. Sertalp; A. Tosun",Istanbul Technical University; Ericsson Turkey; Ericsson Turkey; Istanbul Technical University,"2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","27 Jan 2020","2019","","","98","105","Software defect prediction is still a challenging task in industrial settings. Noisy data and/or lack of data make it hard to build successful prediction models. In this study, we aim to build a change-level defect prediction model for a software project in an industrial setting. We combine various probabilistic models, namely matrix factorization and topic modeling, with the expectation of overcoming the noisy and limited nature of industrial settings by extracting hidden features from multiple resources. Commit level process metrics, latent features from commits, and semantic features from commit messages are combined to build the defect predictors with the use of Log Filtering and feature selection techniques, and two machine learning algorithms Naive Bayes and Extreme Gradient Boosting (XGBoost). Collecting data from various sources and applying data pre-processing techniques show a statistically significant improvement in terms of probability of detection by up to 24% when compared to a base model with process metrics only.","2151-0830","978-1-7281-4136-7","10.1109/ASEW.2019.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967412","software defect prediction;matrix factorization;topic modeling;XGBoost","","Bayes methods;feature extraction;feature selection;gradient methods;information filtering;learning (artificial intelligence);matrix decomposition;probability;project management;software fault tolerance;software management;software metrics","XGBoost;extreme gradient boosting;probability of detection;data collection;log filtering;naive Bayes machine learning algorithms;software project;latent features;commit level process metrics;hidden feature extraction;topic modeling;matrix factorization;probabilistic models;change-level defect prediction model;noisy data;software defect prediction;industrial setting;commit logs;semantic features;data pre-processing techniques;feature selection techniques;defect predictors;commit messages","","","","46","","27 Jan 2020","","","IEEE","IEEE Conferences"
"Personalized defect prediction","T. Jiang; L. Tan; S. Kim","University of Waterloo, ON, Canada; University of Waterloo, ON, Canada; Hong Kong University of Science and Technology, China","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","2 Jan 2014","2013","","","279","289","Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.","","978-1-4799-0215-6","10.1109/ASE.2013.6693087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693087","Change classification;machine learning;personalized defect prediction;software reliability","Predictive models;Vectors;Mars;Syntactics;Computer bugs;Training;Feature extraction","Java;Linux;program compilers","personalized defect prediction;separate prediction model;coding styles;commit frequencies;experience levels;different defect patterns;software defect prediction;C software projects;java software projects;Linux kernel;PostgreSQL;Xorg;Eclipse;Lucene;Jackrabbit","","84","","59","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Warning-Introducing Commits vs Bug-Introducing Commits: A tool, statistical models, and a preliminary user study","L. -P. Querel; P. C. Rigby","Concordia University,Department of Computer Science and Software Engineering,Montreal,Canada; Concordia University,Department of Computer Science and Software Engineering,Montreal,Canada","2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC)","28 Jun 2021","2021","","","433","443","This paper partially replicates prior works on building historical commits [1], commit risk modeling [2], and a comparison of statistical bug models and static bug finders [3].We examine 8 Maven-based projects with an average lifespan of 5.8 years. To historically build these projects across a total of 45k commits, we develop a series of techniques, such as flexibly selecting the version of a library that is closest to the commit date. We are able to build a per project average of 78.4% of all commits, a doubling in buildability compared to prior work. We also develop a git blame strategy to assign warnings even when a commit does not build.We run JLint and FindBugs and create a logistic regression model to predict if a commit that introduces a warning has higher odds of introducing a bug. The static bug finders model accounted for only 13% of the deviance, while the statistical bug model accounted for 19.5%. We had expected static bug finder warnings to improve the predictive power of models of bug introducing changes, but we clearly attained a negative result.To understand this negative result, we perform a preliminary user study of developers who introduced new warnings in 37 projects. We found that while warnings might not predict bugs, 53% and 21% of warnings in Findbugs and Jlint respectively are useful. We also study whether just-in-time warnings presentation on each commit impacted usefulness. We find that the later a warning is shown to a developer, the less useful it is perceived to be (a median of 11.5 days versus 23 days for non useful warnings).Based on our findings, we modify the existing COMMITGURU interface to add new warnings to the specific line in a changed file. The empirical study data [4] and WARNINGSGURU tool [5] are publicly available.","2643-7171","978-1-6654-1403-6","10.1109/ICPC52881.2021.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463031","bug detection;static bug finder;statistical bug prediction;user study","Computer bugs;Buildings;Predictive models;Tools;Libraries;Logistics","Java;program debugging;public domain software;regression analysis;software metrics;software quality","project average;logistic regression model;static bug finders model;statistical bug model;static bug finder warnings;just-in-time warnings presentation;commit impacted usefulness;nonuseful warnings;empirical study data;bug-introducing commits;statistical models;historical commits;commit risk modeling;commit date;Maven-based projects","","","","28","","28 Jun 2021","","","IEEE","IEEE Conferences"
"Assessing Personalized Software Defect Predictors","B. Eken","Fac. of Comput. & Inf. Eng., Istanbul Tech. Univ., Istanbul, Turkey","2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)","30 Aug 2018","2018","","","488","491","Software defect prediction models guide developers and testers to identify defect prone software modules in fewer time and effort, compared to manual inspections of the source code. The state-of-the-art predictors on publicly available software engineering data could catch around 70% of the defects. While early studies mostly utilize static code properties of the software, recent studies incorporate the people factor into the prediction models, such as the number of developers that touched a code unit, the experience of the developer, and interaction and cognitive behaviors of developers. Those information could give a stronger clue about the defect-prone parts because they could explain defect injection patterns in software development. Personalization has been emerging in many other systems such as social platforms, web search engines such that people get customized recommendations based on their actions, profiles and interest. Following this point of view, customization in defect prediction with respect to each developer would increase predictions' accuracy and usefulness than traditional, general models. In this thesis, we focus on building a personalized defect prediction framework that gives instant feedback to the developer at change level, based on historical defect and change data. Our preliminary analysis of the personalized prediction models of 121 developers in six open source projects indicate that, a personalized approach is not always the best model when compared to general models built for six projects. Other factors such as project characteristics, developer's historical data, the context and frequency of contributions, and/or development methodologies might affect which model to consider in practice. Eventually, this topic is open to improvement with further empirical studies on each of these factors","2574-1934","978-1-4503-5663-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449634","personalized defect prediction;bug prediction;customization","Data models;Predictive models;Software;Adaptation models;Analytical models;Prediction algorithms;Software algorithms","program testing;public domain software;search engines;software quality","development methodologies;personalized software defect predictors;testers;defect prone software modules;source code;publicly available software engineering data;static code properties;code unit;defect-prone parts;defect injection patterns;software development;personalized defect prediction framework;historical defect;personalized prediction models;Web search engines","","","","","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Online Defect Prediction for Imbalanced Data","M. Tan; L. Tan; S. Dara; C. Mayeux","Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Cisco Syst., Bangalore, India; Cisco Syst., Bangalore, India","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","17 Aug 2015","2015","2","","99","108","Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice. We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced -- there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance. We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these techniques improve the precision of change classification by 12.2-89.5% or 6.4 -- 34.8 percentage points (pp.) on the seven projects. In addition, we integrate change classification in the development process of the proprietary project. We have learned the following lessons: 1) new solutions are needed to convince developers to use and believe prediction results, and prediction results need to be actionable, 2) new and improved classification algorithms are needed to explain the prediction results, and insensible and unactionable explanations need to be filtered or refined, and 3) new techniques are needed to improve the relatively low precision.","1558-1225","978-1-4799-1934-5","10.1109/ICSE.2015.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202954","","Training;Software;Predictive models;Feature extraction;Computer bugs;Data models;Software engineering","pattern classification;public domain software;software performance evaluation;software reliability","online defect prediction techniques;imbalanced data;software reliability;change classification performance evaluation;low prediction performance;cross-validation approach;updatable classification techniques;resampling classification techniques;online change classification techniques;open source projects;proprietary project development process","","96","","53","","17 Aug 2015","","","IEEE","IEEE Conferences"
"Does Socio-Technical Congruence Have an Effect on Continuous Integration Build Failures? An Empirical Study on 10 GitHub Projects","W. Zhang; Z. Chen; B. Luo","State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)","6 Aug 2018","2018","","","333","343","Coordination is important in software development. Socio-Technical Congruence (STC) is proposed to measure the match between coordination requirements and actual coordination activities, and has been proved to have impact on software failures in commercial projects. Continuous defect prediction is aimed to predict defects just in time, which is more meaningful than traditional defect prediction in practice. In this paper, we compute the build-level STC and investigate its usefulness in continuous defect prediction based on 10 GitHub projects. We find that adding STC metrics into logistic regression models can significantly improve both the explanatory power and the predictive power when predicting build failures. Furthermore, we compare the performance of STC and MDL from the aspects of regression and prediction. MDL is short for Missing Developer Links, a deviation of the STC metric. We find that MDL usually performs better than STC. Our work is promising to help detect coordination issues during real time process of software development.","","978-1-5386-7757-5","10.1109/QRS.2018.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424984","socio-technical congruence;coordination breakdown;software quality;continuous integration;defect prediction","Task analysis;Measurement;Software quality;Computational modeling;Servers;Control systems","regression analysis;software metrics","continuous integration build failures;software development;Socio-Technical Congruence;coordination requirements;software failures;build-level STC;regression;STC metric;GitHub projects;defect prediction","","1","","39","","6 Aug 2018","","","IEEE","IEEE Conferences"
"Large-Scale Empirical Studies on Effort-Aware Security Vulnerability Prediction Methods","X. Chen; Y. Zhao; Z. Cui; G. Meng; Y. Liu; Z. Wang","School of Information Science and Technology, Nantong University, Nantong, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Computer School, Beijing Information Science and Technology University, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Transactions on Reliability","3 Mar 2020","2020","69","1","70","87","Security vulnerability prediction (SVP) can identify potential vulnerable modules in advance and then help developers to allocate most of the test resources to these modules. To evaluate the performance of different SVP methods, we should take the security audit and code inspection into account and then consider effort-aware performance measures (such as ACC and P<sub>opt</sub>). However, to the best of our knowledge, the effectiveness of different SVP methods has not been thoroughly investigated in terms of effort-aware performance measures. In this article, we consider 48 different SVP methods, of which 36 are supervised methods and 12 are unsupervised methods. For the supervised methods, we consider 34 software-metric-based methods and two text-mining-based methods. For the software-metric-based methods, in addition to a large number of classification methods, we also consider four state-of-the-art methods (i.e., EALR, OneWay, CBS, and MULTI) proposed in recent effort-aware just-in-time defect prediction studies. For text-mining-based methods, we consider the Bag-of-Word model and the term-frequency-inverse-document-frequency model. For the unsupervised methods, all the modules are ranked in the ascendent order based on a specific metric. Since 12 software metrics are considered when measuring extracted modules, there are 12 different unsupervised methods. To the best of our knowledge, over 40 SVP methods have not been considered in previous SVP studies. In our large-scale empirical studies, we use three real open-source web applications written in PHP as benchmark. These three web applications include 3466 modules and 223 vulnerabilities in total. We evaluate these SVP methods both in the within-project SVP scenario and the cross-project SVP scenario. Empirical results show that two unsupervised methods [i.e., lines of code (LOC) and Halstead's volume (HV)] and four recently proposed state-of-the-art supervised methods (i.e., MULTI, OneWay, CBS, and EALR) can achieve better performance than the other methods in terms of effort-aware performance measures. Then, we analyze the reasons why these six methods can achieve better performance. For example, when using 20% of the entire efforts, we find that these six methods always require more modules to be inspected, especially for unsupervised methods LOC and HV. Finally, from the view of practical vulnerability localization, we find that all the unsupervised methods and the OneWay method have high false alarms before finding the first vulnerable module. This may have an impact on developers' confidence and tolerance, and supervised methods (especially MULTI and text-mining-based methods) are preferred.","1558-1721","","10.1109/TR.2019.2924932","National Natural Science Foundation of China(grant numbers:61702041,61872263,61602267,61202006); Nantong Application Research Plan(grant numbers:JC2018134); Beijing Municipal Education Commission(grant numbers:KM201811232016); Beijing Information Science and Technology University(grant numbers:QXTCP C201906); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809906","Effort-aware performance measures;security vulnerability prediction (SVP);software metric;supervised method;text mining;unsupervised method","Security;Software metrics;Correlation;Open source software","data mining;Internet;public domain software;security of data;software metrics;software quality;text analysis","within-project SVP scenario;cross-project SVP scenario;supervised methods;unsupervised methods;vulnerable module;text-mining-based methods;effort-aware security vulnerability prediction methods;effort-aware just-in-time defect prediction;effort-aware performance measure;software-metric-based methods;bag-of-word model;term-frequency-inverse-document-frequency model;open-source web applications;lines of code;LOC;Halstead's volume","","4","","70","IEEE","22 Aug 2019","","","IEEE","IEEE Journals"
"Time Series Prediction Algorithm for Intelligent Predictive Maintenance","C. Lin; Y. Hsieh; F. Cheng; H. Huang; M. Adnan","Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan; Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan; Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan; Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan; Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan","IEEE Robotics and Automation Letters","14 Jun 2019","2019","4","3","2807","2814","Predictive maintenance aims to find out when the target device (TD) is in the sick state and almost entering the dead state before its actual occurrence to conduct just-in-time maintenance, so as to avoid unexpected TD down time. In this way, not only tool availability and manufacturing quality are improved, but the additional cost of excessive maintenance in preventive maintenance strategy can also be reduced. Among the predictive maintenance technologies proposed by many scholars, exponential model was commonly applied to predict the remaining useful life (RUL) of TD. However, due to the algorithm limitations, when TD is about to die, whether the TD's aging feature suddenly rises or becomes smooth, the exponential model may not be able to keep up with the real-time prediction or even falsely predicts long RUL. To solve the problem of inaccurate RUL prediction, the authors propose the time series prediction (TSP) algorithm. TSP applies the time series analysis model built by information criterion to adapt to the complicated future trend of solving TD fault prediction. Also, the Pre-Alarm Module (PreAM) to make alert of immediate maintenance when a TD is likely to shut down shortly as well as the Death Correlation Index (DCI) to reveal the possibility of entering the dead state are proposed in this work. How to select the most effective predictors and adjust the predictor weights to construct high-performance prediction model are also illustrated in this letter with the tools in various industries (such as solar-cell manufacturing and machine tool industry) being the examples of the TSP algorithm.","2377-3766","","10.1109/LRA.2019.2918684","Intelligent Manufacturing Research Center; Ministry of Education (MOE) in Taiwan; Ministry of Science and Technology of Taiwan, R.O.C.(grant numbers:107-2218-E-006-055,107-2622-8-006-015,107-2622-E-006-002-CC2,105-2221-E-006-255-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721101","Predictive maintenance (PdM);time series analysis (TSA);remaining useful life (RUL);time series prediction (TSP);pre-alarm module (PreAM);death correlation index (DCI);exponential model","Predictive models;Aging;Prediction algorithms;Maintenance engineering;Time series analysis;Correlation;Indexes","cost reduction;exponential distribution;fault diagnosis;machine tools;preventive maintenance;remaining life assessment;time series","time series prediction algorithm;intelligent predictive maintenance;target device;sick state;dead state;just-in-time maintenance;tool availability;manufacturing quality;preventive maintenance strategy;exponential model;TD fault prediction;death correlation index;pre-alarm module;remaining useful life prediction;additional cost reduction","","4","","21","IEEE","23 May 2019","","","IEEE","IEEE Journals"
"Fault class prediction in unsupervised learning using model-based clustering approach","N. Amruthnath; T. Gupta","Industrial and Entrepreneurial Engineering, Western Michigan University, Kalamazoo, MI, 49008, USA; Industrial and Entrepreneurial Engineering, Western Michigan University, Kalamazoo, MI, 49008, USA","2018 International Conference on Information and Computer Technologies (ICICT)","10 May 2018","2018","","","5","12","Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.","","978-1-5386-5384-5","10.1109/INFOCT.2018.8356831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356831","unsupervised learning;fault class detection;predictive maintenance;gaussian mixture model;clustering;just-in-time;TPM","Predictive maintenance;Fans;Vibrations;Unsupervised learning;Prediction algorithms;Clustering algorithms","condition monitoring;fault diagnosis;Gaussian processes;maintenance engineering;production engineering computing;unsupervised learning;vibrations","fault class prediction;unsupervised learning;fault class detection;Gaussian Mixture Model Clustering;condition-based predictive maintenance;K-means algorithm","","8","","33","","10 May 2018","","","IEEE","IEEE Conferences"
"Integration of Fiber Optic Cable Diagnostics within Aerospace Transceivers","C. Kuznia","Ultra Commun., San Marcos, CA","IEEE Conference Avionics Fiber-Optics and Photonics, 2006.","9 Oct 2006","2006","","","58","59","OTDR can enable fault prediction (and circumvention with a certain level of redundancy) and it is possible to enter a new era of fiber optic maintenance concepts. Through fault prediction, is possible to replace or inspect suspect components before catastrophic failures occur. Redundant hardware permits completion of a mission segment with failed components and can lead to just-in-time provisioning of spares at scheduled maintenance periods. Topological loss maps with 10 cm resolution of the entire cable plant can be stored within each transceiver's memory and used as a reference for routine inspections","","1-4244-0408-8","10.1109/AVFOP.2006.1707499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707499","","Optical fibers;Optical fiber cables;Transceivers;Pulse amplifiers;Pollution measurement;High speed optical techniques;Optical fiber testing;Optical transmitters;Optical receivers;Connectors","aerospace instrumentation;cable testing;optical cables;optical time-domain reflectometry;transceivers","fiber optic cable diagnostics;aerospace transceivers;OTDR;fault prediction;fiber optic maintenance concepts;topological loss maps;cable plant;transceiver's memory;10 cm","","3","1","1","","9 Oct 2006","","","IEEE","IEEE Conferences"
"A Non-Intrusive Deep Learning Based Diagnosis System for Elevators","S. Chai; X. I. Li; Y. Jia; Y. He; C. H. Yip; K. K. Cheung; M. Wang","College of Physics and Optoelectronic Engineering, Shenzhen University, Shenzhen, China; Electrical and Mechanical Services Department, Government of the HKSAR of the PRC, Hong Kong; Ergatian Limited, Hong Kong; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; Electrical and Mechanical Services Department, Government of the HKSAR of the PRC, Hong Kong; Electrical and Mechanical Services Department, Government of the HKSAR of the PRC, Hong Kong; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong","IEEE Access","4 Feb 2021","2021","9","","20993","21003","With the ever-growing number of elevators coupled with the aging workforce, diminishing new installations and limited use of maintenance technology, it is increasingly challenging for the owners and responsible parties to maintain the safe and reliable operation of the lift systems. To address this issue, a non-intrusive artificial intelligence (AI) based diagnosis system, aiming at providing fault detection and potential fault prediction for multi-brand lifts without intervening the existing circuitry of the lift installations, is proposed in this paper. The proposed system employs the multivariate long short term memory fully convolutional network (MLSTM-FCN) to learn and analyze the measured signals from the non-intrusive detection system of the elevators. It is capable of (i) giving advance and clear warnings of corrective actions to prevent major equipment breakdowns, and (ii) indicating just-in-time maintenance for enhancing the lift reliability at a low cost. The implementation of the non-intrusive detection system is provided. The design of the diagnostic algorithm is elaborated. Both the simulations and experiments of a commercial elevator have been conducted to verify the effectiveness of the proposed system.","2169-3536","","10.1109/ACCESS.2021.3053858","Electrical and Mechanical Services Department, the Government of the HKSAR of the PRC(grant numbers:4200397034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333567","Deep learning;electric traction system;fault detection;elevator system;artificial intelligence","Elevators;Traction motors;Monitoring;Synchronous motors;Permanent magnet motors;Sensors;Reliability","artificial intelligence;convolutional neural nets;deep learning (artificial intelligence);fault diagnosis;lifts;maintenance engineering;mechanical engineering computing;recurrent neural nets;reliability;security of data","nonintrusive detection system;lift reliability;commercial elevator;maintenance technology;lift systems;fault detection;fault prediction;multibrand lifts;lift installations;multivariate long short term memory fully convolutional network;nonintrusive artificial intelligence based diagnosis;nonintrusive deep learning based diagnosis","","","","22","CCBYNCND","22 Jan 2021","","","IEEE","IEEE Journals"
"Predicting risk of pre-release code changes with Checkinmentor","A. Tarvo; N. Nagappan; T. Zimmermann; T. Bhat; J. Czerwonka","Brown Univ., Providence, RI, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA","2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)","2 Jan 2014","2013","","","128","137","Code defects introduced during the development of the software system can result in failures after its release. Such post-release failures are costly to fix and have negative impact on the reputation of the released software. In this paper we propose a methodology for early detection of faulty code changes. We describe code changes with metrics and then use a statistical model that discriminates between faulty and non-faulty changes. The predictions are done not at a file or binary level but at the change level thereby assessing the impact of each change. We also study the impact of code branches on collecting code metrics and on the accuracy of the model. The model has shown high accuracy and was developed into a tool called CheckinMentor. CheckinMentor was deployed to predict risk for the Windows Phone software. However, our methodology is versatile and can be used to predict risk in a variety of large complex software systems.","2332-6549","978-1-4799-2366-3","10.1109/ISSRE.2013.6698912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698912","code change;risk;software metrics;code branch","Measurement;Software systems;Predictive models;Training;Numerical models;Accuracy","risk analysis;software metrics;software tools;system recovery","risk prediction;prerelease code changes;CheckinMentor;code defects;software system development;post-release failures;released software reputation;faulty code change detection;statistical model;faulty changes;nonfaulty changes;code metrics;Windows Phone software;software systems","","5","1","24","","2 Jan 2014","","","IEEE","IEEE Conferences"
"The Ghost Commit Problem When Identifying Fix-Inducing Changes: An Empirical Study of Apache Projects","C. Rezk; Y. Kamei; S. Mcintosh","Electrical and Computer Engineering, McGill University, 5620 Montreal, Quebec, Canada, H3A 0E9 (e-mail: Christophe.rezk@mail.mcgill.ca); Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Fukuoka, Japan, 819-0395 (e-mail: kamei@ait.kyushu-u.ac.jp); Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada, H3A 0E9 (e-mail: shane.mcintosh@uwaterloo.ca)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","The SZZ approach for identifying fix-inducing changes traces backwards from a commit that fixes a defect to those commits that are implicated in the fix. This approach is at the heart of studies of characteristics of fix-inducing changes, as well as the popular Just-in-Time (JIT) variant of defect prediction. However, some types of commits are invisible to the SZZ approach. We refer to these invisible commits as Ghost Commits. In this paper, we set out to define, quantify, characterize, and mitigate ghost commits that impact the SZZ algorithm during its mapping (i.e., linking defect-fixing commits to those commits that are implicated by the fix) and filtering phases (i.e., removing improbable fix-inducing commits from the set of implicated commits). We mine the version control repositories of 14 open source Apache projects for instances of mapping-phase and filtering-phase ghost commits. We find that (1) 5.66%11.72% of defect-fixing commits of defect-fixing commits only add lines, and thus, cannot be mapped back to implicated commits; (2) 1.05%4.60% of the studied commits only remove lines, and thus, cannot be implicated in future fixes; and (3) that no implicated commits survive the filtering process of 0.35%14.49% defect-fixing commits. Qualitative analysis of ghost commits reveals that 46.5% of 142 addition-only defect-fixing commits add checks (e.g., null-ness or emptiness checks), while 39.7% of 307 removal-only commits clean up (unused) code. Our results suggest that the next generation of SZZ improvements should be language-aware to connect ghost commits to implicated and defect-fixing commits. Based on our observations, we discuss promising directions for mitigation strategies to address each type of ghost commit. Moreover, we implement mitigation strategies for addition-only commits and evaluate those strategies with respect to a baseline approach. The results indicate that our strategies achieve a precision of 0.753, improving the precision of implicated commits by 39.5 percentage points.","1939-3520","","10.1109/TSE.2021.3087419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448382","SZZ;fix-inducing changes;defect-fixing changes","Maintenance engineering;Data mining;Electronic mail;Computer bugs;Information filters;Tools;Taxonomy","","","","","","","IEEE","8 Jun 2021","","","IEEE","IEEE Early Access Articles"
"CC2Vec: Distributed Representations of Code Changes","T. Hoang; H. J. Kang; D. Lo; J. Lawall","Singapore Management University,Singapore; Singapore Management University,Singapore; Singapore Management University,Singapore; Sorbonne University/Inria/LIP6,France","2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)","21 Dec 2020","2020","","","518","529","Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code. To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.","1558-1225","978-1-4503-7121-6","","Singapore National Research Foundation(grant numbers:NRF2016-NRF-ANR003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284081","code embedding;deep learning;code changes","Semantics;Neural networks;Tools;Predictive models;Software;Task analysis;Software engineering","learning (artificial intelligence);neural nets;program debugging;program testing;public domain software;software maintenance","code change;CC2Vec models;removed added code;distributed representation;software patches;manually identified features","","","","62","","21 Dec 2020","","","IEEE","IEEE Conferences"
"Table of contents","",,"2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)","7 Oct 2013","2013","","","1","4","The following topics are dealt with: embedded computer systems; computer architecture; cloud computing; unicores; GPU-architectures; FFT-architectures; design space exploration; pipelined application modeling; SystemC; energy-aware-task-parallelism; NoC; embedded hardware-efficient architecture; configurable FPGA based architecture; configurable CPU based architecture; SOC; array processors; just-in-time modulo scheduling; dynamic task mapping; fast transaction-level dynamic power consumption modeling; rectangular single event transient fault model; manycore architectures; fault-tolerant techniques; and exposed data path architectures.","","978-1-4799-0103-6","10.1109/SAMOS.2013.6621095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621095","","","C++ language;cloud computing;embedded systems;fast Fourier transforms;fault tolerant computing;graphics processing units;integrated circuit design;multiprocessing systems;network-on-chip;parallel architectures;power aware computing;processor scheduling","embedded computer systems;computer architecture;cloud computing;unicores;GPU-architectures;FFT-architectures;design space exploration;pipelined application modeling;SystemC;energy-aware-task-parallelism;embedded hardware-efficient architecture;configurable FPGA based architecture;configurable CPU based architecture;SOC;array processors;NoC;just-in-time modulo scheduling;dynamic task mapping;fast transaction-level dynamic power consumption modeling;rectangular single event transient fault model;manycore architectures;fault-tolerant techniques;exposed data path architectures","","","","","","7 Oct 2013","","","IEEE","IEEE Conferences"
