#
# Foramt
# >><REF_START_PATTERN>Title 
#
# two batches
# 1st submission
# 2nd submission
#


>><[N]>A Large-Scale Empirical Study of Just-in-Time Quality Assurance
[1] E. Arisholm and L.C. Briand, “Predicting Fault-Prone Components
in a Java Legacy System,” Proc. ACM/IEEE Int’l Symp.
Empirical Software Eng., pp. 8-17, 2006.
[2] E. Arisholm, L.C. Briand, and E.B. Johannessen, “A Systematic
and Comprehensive Investigation of Methods to Build and
Evaluate Fault Prediction Models,” The J. Systems and Software,
vol. 83, no. 1, pp. 2-17, 2010.
[3] L. Aversano, L. Cerulo, and C. Del Grosso, “Learning from Bug-
Introducing Changes to Prevent Fault Prone Code,” Proc. Int’l
Workshop Principles of Software Evolution, pp. 19-26, 2007.
[4] V.R. Basili, L.C. Briand, and W.L. Melo, “A Validation of Object-
Oriented Design Metrics as Quality Indicators,” IEEE Trans.
Software Eng., vol. 22, no. 10, pp. 751-761, Oct. 1996.
[5] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. Devanbu,
“Don’t Touch My Code!: Examining the Effects of Ownership on
Software Quality,” Proc. European Software Eng. Conf. and Symp. the
Foundations of Software Eng., pp. 4-14, 2011.
[6] L.C. Briand, V.R. Basili, and C.J. Hetmanski, “Developing
Interpretable Models with Optimized Set Reduction for Identifying
High-Risk Software Components,” IEEE Trans. Software Eng.,
vol. 19, no. 11, pp. 1028-1044, Nov. 1993.
[7] L.C. Briand, J. Wu¨ st, S.V. Ikonomovski, and H. Lounis, “Investigating
Quality Factors in Object-Oriented Designs: An Industrial Case
Study,” Proc. Int’l Conf. Software Eng., pp. 345-354, 1999.
[8] M. Cataldo, A. Mockus, J.A. Roberts, and J.D. Herbsleb, “Software
Dependencies, Work Dependencies, and Their Impact on Failures,”
IEEE Trans. Software Eng., vol. 35, no. 6, pp. 864-878, Nov./
Dec. 2009.
[9] S.R. Chidamber and C.F. Kemerer, “A Metrics Suite for Object
Oriented Design,” IEEE Trans. Software Eng., vol. 20, no. 6, pp. 476-
493, June 1994.
[10] M. D’Ambros, M. Lanza, and R. Robbes, “An Extensive
Comparison of Bug Prediction Approaches,” Proc. Int’l Working
Conf. Mining Software Repositories, pp. 31-41, 2010.
[11] B. Efron, “Estimating the Error Rate of a Prediction Rule:
Improvement on Cross-Validation,” J. Am. Statistical Assoc.,
vol. 78, no. 382, pp. 316-331, 1983.
[12] K.E. Emam, W. Melo, and J.C. Machado, “The Prediction of Faulty
Classes Using Object-Oriented Design Metrics,” J. Systems Software,
vol. 56, pp. 63-75, Feb. 2001.
[13] J. Eyolfson, L. Tan, and P. Lam, “Do Time of Day and Developer
Experience Affect Commit Bugginess,” Proc. Eighth Working Conf.
Mining Software Repositories, pp. 153-162, 2011.
[14] T. Fritz, J. Ou, G.C. Murphy, and E. Murphy-Hill, “A Degree-of-
Knowledge Model to Capture Source Code Familiarity,” Proc.
32nd ACM/IEEE Int’l Conf. Software Eng., 2010.
[15] T.L. Graves, A.F. Karr, J.S. Marron, and H. Siy, “Predicting Fault
Incidence Using Software Change History,” IEEE Trans. Software
Eng., vol. 26, no. 7, pp. 653-661, July 2000.
[16] P.J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy,
“Characterizing and Predicting Which Bugs Get Fixed: An
Empirical Study of Microsoft Windows,” Proc. Int’l Conf. Software
Eng., pp. 495-504, 2010.
[17] T. Gyimothy, R. Ferenc, and I. Siket, “Empirical Validation of
Object-Oriented Metrics on Open Source Software for Fault
Prediction,” IEEE Trans. Software Eng., vol. 31, no. 10, pp. 897-
910, Oct. 2005.
[18] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A
Systematic Review of Fault Prediction Performance in Software
Engineering,” IEEE Trans. Software Eng., vol. 38, no. 6,
pp. 1276-1304, Nov./Dec. 2012.
[19] A.E. Hassan, “Predicting Faults Using the Complexity of Code
Changes,” Proc. Int’l Conf. Software Eng., pp. 16-24, 2009.
[20] I. Herraiz, D.M. German, J.M. Gonzalez-Barahona, and G. Robles,
“Towards a Simplification of the Bug Report form in Eclipse,”
Proc. Int’l Working Conf. Mining Software Repositories, pp. 145-148,
2008.
[21] I. Herraiz, J.M. Gonzalez-Barahona, and G. Robles, “Towards a
Theoretical Model for Software Growth,” Proc. Fourth Int’l
Workshop Mining Software Repositories, p. 21, 2007.
[22] Y. Jiang, B. Cuki, T. Menzies, and N. Bartlow, “Comparing Design
and Code Metrics for Software Quality Prediction,” Proc. Fourth
Int’l Workshop Predictor Models in Software Eng., pp. 11-18, 2008.
[23] Y. Kamei, S. Matsumoto, A. Monden, K. Matsumoto, B. Adams,
and A.E. Hassan, “Revisiting Common Bug Prediction Findings
Using Effort Aware Models,” Proc. Int’l Conf. Software Maintenance,
pp. 1-10, 2010.
[24] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto, and K.
Matsumoto, “The Effects of Over and Under Sampling on Fault-
Prone Module Detection,” Proc. Int’l Symp. Empirical Software Eng.
and Measurement, pp. 196-204, 2007.
[25] T.M. Khoshgoftaar, X. Yuan, and E.B. Allen, “Balancing
Misclassification Rates in Classification-Tree Models of Software
Quality,” Empirical Software Eng., vol. 5, no. 4, pp. 313-
330, 2000.
[26] S. Kim, E.J. Whitehead Jr., and Y. Zhang, “Classifying Software
Changes: Clean or Buggy?” IEEE Trans. Software Eng., vol. 34,
no. 2, pp. 181-196, Mar. 2008.
[27] A.G. Koru, D. Zhang, K.E. Emam, and H. Liu, “An Investigation
into the Functional Form of the Size-Defect Relationship for
Software Modules,” IEEE Trans. Software Eng., vol. 35, no. 2,
pp. 293-304, Mar./Apr. 2009.
[28] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking
Classification Models for Software Defect Prediction: A Proposed
Framework and Novel Findings,” IEEE Trans. Software Eng.,
vol. 34, no. 4, pp. 485-496, 2008.
[29] M. Leszak, D.E. Perry, and D. Stoll, “Classification and Evaluation
of Defects in a Project Retrospective,” J. Systems Software, vol. 61,
no. 3, pp. 173-187, 2002.
[30] P.L. Li, J. Herbsleb, M. Shaw, and B. Robinson, “Experiences and
Results from Initiating Field Defect Prediction and Product Test
Prioritization Efforts at ABB Inc.,” Proc. Int’l Conf. Software Eng.,
pp. 413-422, 2006.
[31] C.L. Mallows, “Some Comments on CP,” Technometrics, vol. 42,
no. 1, pp. 87-94, 2000.
[32] S. Matsumoto, Y. Kamei, A. Monden, and K. Matsumoto, “An
Analysis of Developer Metrics for Fault Prediction,” Proc. Int’l
Conf. Predictive Models in Software Eng., pp. 18:1-18:9, 2010.
[33] T.J. McCabe, “A Complexity Measure,” Proc. Second Int’l Conf.
Software Eng., p. 407, 1976.
[34] T. Mende and R. Koschke, “Revisiting the Evaluation of Defect
Prediction Models,” Proc. Int’l Conf. Predictor Models in Software
Eng., pp. 1-10, 2009.
[35] T. Mende and R. Koschke, “Effort-Aware Defect Prediction
Models,” Proc. European Conf. Software Maintenance and Reeng.,
pp. 109-118, 2010.
[36] T. Menzies, A. Dekhtyar, J. Distefano, and J. Greenwald,
“Problems with Precision: A Response to “Comments on ‘Data
Mining Static Code Attributes to Learn Defect Predictors’“,” IEEE
Trans. Software Eng., vol. 33, no. 9, pp. 637-640, Sept. 2007.
[37] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect Prediction from Static Code Features: Current Results,
Limitations, New Approaches,” Automated Software Eng., vol. 17,
no. 4, pp. 375-407, 2010.
[38] A. Mockus, “Organizational Volatility and Its Effects on Software
Defects,” Proc. Int’l Symp. Foundations of Software Eng., pp. 117-126,
2010.
[39] A. Mockus, R.T. Fielding, and J.D. Herbsleb, “Two Case
Studies of Open Source Software Development: Apache and
Mozilla,” ACM Trans. Software Eng. Methodology, vol. 11, no. 3,
pp. 309-346, 2002.
[40] A. Mockus and J. Herbsleb, “Expertise Browser: A Quantitative
Approach to Identifying Expertise,” Proc. 24th Int’l Conf. Software
Eng., 2002.
[41] A. Mockus and D.M. Weiss, “Predicting Risk of Software
Changes,” Bell Labs Technical J., vol. 5, no. 2, pp. 169-180, 2000.
[42] A. Mockus, P. Zhang, and P.L. Li, “Predictors of Customer
Perceived Software Quality,” Proc. Int’l Conf. Software Eng.,
pp. 225-233, 2005.
[43] R. Moser, W. Pedrycz, and G. Succi, “A Comparative Analysis of
the Efficiency of Change Metrics and Static Code Attributes for
Defect Prediction,” Proc. Int’l Conf. Software Eng., pp. 181-190,
2008.
[44] J.C. Munson and S.G. Elbaum, “Code Churn: A Measure for
Estimating the Impact of Code Change,” Proc. Int’l Conf. Software
Maintenance, p. 24, 1998.
[45] J.C. Munson and T.M. Khoshgoftaar, “The Detection of Fault-
Prone Programs,” IEEE Trans. Software Eng., vol. 18, no. 5, pp. 423-
433, May 1992.
[46] N. Nagappan and T. Ball, “Use of Relative Code Churn Measures
to Predict System Defect Density,” Proc. Int’l Conf. Software Eng.,
pp. 284-292, 2005.
[47] N. Nagappan, T. Ball, and A. Zeller, “Mining Metrics to Predict
Component Failures,” Proc. Int’l Conf. Software Eng., pp. 452-461,
2006.
[48] N. Nagappan, A. Zeller, T. Zimmermann, K. Herzig, and B.
Murphy, “Change Bursts as Defect Predictors,” Proc. Int’l Symp.
Software Reliability Eng., pp. 309-318, 2010.
[49] N. Ohlsson and H. Alberg, “Predicting Fault-Prone Software
Modules in Telephone Switches,” IEEE Trans. Software Eng.,
vol. 22, no. 12, pp. 886-894, Dec. 1996.
[50] T.J. Ostrand, E.J. Weyuker, and R.M. Bell, “Predicting the Location
and Number of Faults in Large Software Systems,” IEEE Trans.
Software Eng., vol. 31, no. 4, pp. 340-355, Apr. 2005.
[51] T.J. Ostrand, E.J. Weyuker, and R.M. Bell, “Programmer-Based
Fault Prediction,” Proc. Int’l Conf. Predictor Models in Software Eng.,
pp. 19:1-19:10, 2010.
[52] R. Purushothaman and D.E. Perry, “Toward Understanding the
Rhetoric of Small Source Code Changes,” IEEE Trans. Software
Eng., vol. 31, no. 6, pp. 511-526, June 2005.
[53] J. Ratzinger, T. Sigmund, and H.C. Gall, “On the Relation of
Refactorings and Software Defect Prediction,” Proc. Int’l
Working Conf. Mining Software Repositories, pp. 35-38, 2008.
[54] E.S. Raymond, The Cathedral and the Bazaar: Musings on Linux and
Open Source by an Accidental Revolutionary. Oreilly & Assoc. Inc,
2001.
[55] E. Shihab, Z.M. Jiang, W.M. Ibrahim, B. Adams, and A.E. Hassan,
“Understanding the Impact of Code and Process Metrics on Post-
Release Defects: A Case Study on the Eclipse Project,” Proc. Int’l
Symp. Empirical Softw. Eng. and Measurement, pp. 4:1-4:10, 2010.
[56] E. Shihab, A. Mockus, Y. Kamei, B. Adams, and A.E. Hassan,
“High-Impact Defects: A Study of Breakage and Surprise
Defects,” Proc. European Software Eng. Conf. and Symp. Foundations
of Software Eng., pp. 300-310, 2011.
[57] J. Sliwerski, T. Zimmermann, and A. Zeller, “When Do Changes
Induce Fixes?” Proc. Int’l Conf. Mining Software Repositories, pp. 1-5,
2005.
[58] A. Vanya, R. Premraj, and H.v. Vliet, “Approximating Change
Sets at Philips Healthcare: A Case Study,” Proc. European Conf.
Software Maintenance and Reeng., pp. 121-130, 2011.
[59] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “Relink: Recovering
Links between Bugs and Changes,” Proc. European Software Eng.
Conf. and Symp. the Foundations of Software Eng., pp. 15-25, 2011.
[60] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and L. Bairavasundaram,
“How Do Fixes Become Bugs?” Proc. 19th ACM SIGSOFT Symp.
and the 13th European Conf. Foundations of Software Eng., pp. 26-36,
2011.
[61] M. Zhou and A. Mockus, “Developer Fluency: Achieving True
Mastery in Software Projects,” Proc. Int’l Symp. Foundations of
Software Eng., pp. 137-146, 2010.
[62] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-Project Defect Prediction: A Large Scale Experiment on
Data vs. Domain vs. Process,” Proc. European Software Eng. Conf.
and Symp. the Foundations of Software Eng., pp. 91-100, 2009.
[63] T. Zimmermann and P. Weisgerber, “Preprocessing CVS Data for
Fine-Grained Analysis,” Proc. Int’l Workshop Mining Software
Repositories, pp. 2-6, May 2004.



>><[N]>A Replication Study: Just-In-Time Defect Prediction with Ensemble Learning
[1] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning:
A Review and New Perspectives. IEEE Transactions on Pattern Analysis and
Machine Intelligence 35, 8 (2013), 1798–1828.
[2] JC Carver. 2010. Towards Reporting Guidelines for Experimental Replications:
A Proposal. In Proceedings of the 1st International Workshop on Replication in
Empirical Software Engineering Research (RESER) [Held during ICSE 2010]. Cape
Town, South Africa, 2–5.
[3] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-scale Empirical Study of
Just-in-time Quality Assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757–773.
[4] Ye Nan, Kian Ming Chai, Wee Sun Lee, and Hai Leong Chieu. 2012. Optimizing
F-measure: A Tale of Two Approaches. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12). 289–296.
[5] Xinli Yang, David Lo, Xin Xia, and Jianling Sun. 2017. TLEL: A Two-layer Ensemble
Learning Approach for Just-in-time Defect Prediction. In Information and Software
Technology, Vol. 87. 206–220.
[6] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep Learning for
Just-in-Time Defect Prediction. In Proceedings - 2015 IEEE International Conference
on Software Quality, Reliability and Security, QRS 2015. 17–26.
[7] Steven Young, Tamer Abdou, and Ayse Bener. 2018. Deep Super Learner: A
Deep Ensemble for Classification Problems. In Proceedings of the 31st Canadian
Conference on Artificial Intelligence (CanadianAI-31).
[8] Zhi-Hua Zhou. 2012. Ensemble Methods: Foundations and Algorithms. Chapman &
Hall/CRC. Machine Learning & Pattern Recognition Series. 236 pages.


>><[N]>An empirical study of just-in-time defect prediction using cross-project models
[1] V. R. Basili, L. C. Briand, andW. L. Melo. A validation
of object-oriented design metrics as quality indicators.
IEEE Trans. Softw. Eng., 22(10):751–761, 1996.
[2] N. Bettenburg, M. Nagappan, and A. E. Hassan. Think
locally, act globally: Improving defect and e↵ort prediction
models. In Proc. Int’l Working Conf. on Mining
Software Repositories (MSR’12), pages 60–69, 2012.
[3] L. Breiman. Random forests. Machine learning,
45(1):5–32, 2001.
[4] F. L. Coolidge. Statistics: A Gentle Introduction.
SAGE Publications (3rd ed.), 2012.
[5] M. D’Ambros, M. Lanza, and R. Robbes. An extensive
comparison of bug prediction approaches. In Proc.
Int’l Working Conf. on Mining Software Repositories
(MSR’10), pages 31–41, 2010.
[6] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy. Predicting
fault incidence using software change history.
IEEE Trans. Softw. Eng., 26(7):653–661, 2000.
[7] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy.
Characterizing and predicting which bugs get
fixed: An empirical study of microsoft windows. In
Proc. Int’l Conf. on Softw. Eng. (ICSE’10), volume 1,
pages 495–504, 2010.
[8] A. E. Hassan. Predicting faults using the complexity
of code changes. In Proc. Int’l Conf. on Softw. Eng.
(ICSE’09), pages 78–88, 2009.
[9] Y. Jiang, B. Cukic, and T. Menzies. Can data transformation
help in the detection of fault-prone modules? In
Proc. Workshop on Defects in Large Software Systems
(DEFECTS’08), pages 16–20, 2008.
[10] Y. Kamei, S. Matsumoto, A. Monden, K. Matsumoto,
B. Adams, and A. E. Hassan. Revisiting common bug
prediction findings using e↵ort aware models. In Proc.
Int’l Conf. on Software Maintenance (ICSM’10), pages
1–10, 2010.
[11] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto,
and K.-i. Matsumoto. The e↵ects of over and under
sampling on fault-prone module detection. In Proc. Int’l
Symposium on Empirical Softw. Eng. and Measurement
(ESEM’07), pages 196–204, 2007.
[12] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan,
A. Mockus, A. Sinha, and N. Ubayashi. A large-scale
empirical study of just-in-time quality assurance. IEEE
Trans. Softw. Eng., 39(6):757–773, 2013.
[13] T. M. Khoshgoftaar and E. B. Allen. Modeling software
quality with classification trees. Recent Advances in
Reliability and Quality Engineering, 2:247–270, 2001.
[14] S. Kim, E. J. Whitehead, and Y. Zhang. Classifying
software changes: Clean or buggy? IEEE Trans. Softw.
Eng., 34(2):181–196, 2008.
[15] E. Kocaguneli, T. Menzies, and J. Keung. On the value
of ensemble e↵ort estimation. IEEE Trans. Softw. Eng.,
38(6):1403–1416, 2012.
[16] A. G. Koru, D. Zhang, K. El Emam, and H. Liu. An
investigation into the functional form of the size-defect
relationship for software modules. IEEE Trans. Softw.
Eng., 35(2):293–304, 2009.
[17] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch.
Benchmarking classification models for software defect
prediction: A proposed framework and novel findings.
IEEE Trans. Softw. Eng., 34(4):485–496, July 2008.
[18] P. L. Li, J. Herbsleb, M. Shaw, and B. Robinson. Experiences
and results from initiating field defect prediction
and product test prioritization e↵orts at ABB Inc. In
Proc. Int’l Conf. on Softw. Eng. (ICSE’06), pages 413–
422, 2006.
[19] S. Matsumoto, Y. Kamei, A. Monden, and K. Matsumoto.
An analysis of developer metrics for fault prediction.
In Proc. Int’l Conf. on Predictive Models in
Softw. Eng. (PROMISE’10), pages 18:1–18:9, 2010.
[20] T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman,
F. Shull, B. Turhan, and T. Zimmermann. Local
versus global lessons for defect prediction and e↵ort
estimation. IEEE Trans. Softw. Eng., 39(6):822–834,
2013.
[21] A. T. Mısırlı, A. B. Bener, and B. Turhan. An industrial
case study of classifier ensembles for locating software
defects. Software Quality Journal, 19(3):515–536, 2011.
[22] A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal, 5(2):169–
180, 2000.
[23] R. Moser, W. Pedrycz, and G. Succi. A comparative
analysis of the efficiency of change metrics and static
code attributes for defect prediction. In Proc. Int’l
Conf. on Softw. Eng. (ICSE’08), pages 181–190, 2008.
[24] N. Nagappan and T. Ball. Use of relative code churn
measures to predict system defect density. In Proc. Int’l
Conf. on Softw. Eng. (ICSE’05), pages 284–292, 2005.
[25] N. Nagappan, T. Ball, and A. Zeller. Mining metrics
to predict component failures. In Proc. Int’l Conf. on
Softw. Eng. (ICSE’06), pages 452–461, 2006.
[26] J. Nam, S. J. Pan, and S. Kim. Transfer defect learning.
In Proc. Int’l Conf. on Softw. Eng. (ICSE’13), pages
382–391, 2013.
[27] N. Ohlsson and H. Alberg. Predicting fault-prone software
modules in telephone switches. IEEE Trans.
Softw. Eng., 22(12):886–894, 1996.
[28] R. Purushothaman and D. E. Perry. Toward understanding
the rhetoric of small source code changes.
IEEE Trans. Softw. Eng., 31(6):511–526, 2005.
[29] J. Ratzinger, T. Sigmund, and H. C. Gall. On the relation
of refactorings and software defect prediction. In
Proc. Int’l Working Conf. on Mining Software Repositories
(MSR’08), pages 35–38, 2008.
[30] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang.
An industrial study on the risk of software changes. In
Proc. European Softw. Eng. Conf. and Symposium on
the Foundations of Softw. Eng. (ESEC/FSE’12), pages
62:1–62:11, 2012.
[31] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce fixes? In Proc. Int’l Working Conf.
on Mining Software Repositories (MSR’05), pages 1–5,
2005.
[32] S. W. Thomas, M. Nagappan, D. Blostein, and A. E.
Hassan. The impact of classifier configuration and classifier
combination on bug localization. IEEE Trans.
Softw. Eng., 39(10):1427–1443, 2013.
[33] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano.
On the relative value of cross-company and withincompany
data for defect prediction. Empirical Software
Engineering, 14(5):540–578, 2009.
[34] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung. Relink:
recovering links between bugs and changes. In Proc. European
Softw. Eng. Conf. and Symposium on the Foundations
of Softw. Eng. (ESEC/FSE’11), pages 15–25,
2011.
[35] F. Xing, P. Guo, and M. R. Lyu. A novel method for
early software quality prediction based on support vector
machine. In Proc. Int’l Symposium on Software Reliability
Engineering (ISSRE’05), pages 10–pp, 2005.
[36] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project defect prediction: a large
scale experiment on data vs. domain vs. process. In
Proc. European Softw. Eng. Conf. and Symposium on
the Foundations of Softw. Eng. (ESEC/FSE’09), pages
91–100, 2009.


>><[N]>An Investigation of Cross-Project Learning in Online Just-In-Time Software Defect Prediction
[1] A. Agrawal and T. Menzies. 2018. Is “better data” better than “better data miners”?:
on the benefits of tuning SMOTE for defect prediction. In Proceedings of the 40th
International Conference on Software Engineering. 1050–1061.
[2] George G Cabral, Leandro L Minku, Emad Shihab, and Suhaib Mujahid. 2019.
Class Imbalance Evolution and Verification Latency in Just-in-Time Software
Defect Prediction. In Proceedings of the 41st International Conference on Software
Engineering (ICSE). 666–676.
[3] Gerardo Canfora, Andrea De Lucia, Massimiliano Di Penta, Rocco Oliveto, Annibale
Panichella, and Sebastiano Panichella. 2013. Multi-objective cross-project
defect prediction. In 2013 IEEE Sixth International Conference on Software Testing,
Verification and Validation. IEEE, 252–261.
[4] Gemma Catolino, Dario Di Nucci, and Filomena Ferrucci. 2019. Cross-Project
Just-in-Time Bug Prediction for Mobile Apps: An Empirical Assessment. In 6th
IEEE/ACM International Conference on Mobile Software Engineering and Systems.
[5] Xiang Chen, Yingquan Zhao, Qiuping Wang, and Zhidan Yuan. 2018. MULTI:
Multi-objective effort-aware just-in-time software defect prediction. Information
and Software Technology 93 (2018), 1–13.
[6] J. Demšar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets.
JMLR 7 (2006), 1–30.
[7] Gregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar. 2015. Learning in
nonstationary environments: A survey. IEEE Computational Intelligence Magazine
10, 4 (2015), 12–25.
[8] Pedro Domingos and Geoff Hulten. 2000. Mining High-speed Data Streams. In
Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD ’00). ACM, New York, NY, USA, 71–80. https:
//doi.org/10.1145/347090.347107
[9] João Gama, Raquel Sebastião, and Pedro Pereira Rodrigues. 2013. On evaluating
stream learning algorithms. Machine learning 90, 3 (2013), 317–346.
[10] Tibor Gyimothy, Rudolf Ferenc, and Istvan Siket. 2005. Empirical Validation of
Object-Oriented Metrics on Open Source Software for Fault Prediction. IEEE
Trans. Softw. Eng. 31, 10 (Oct. 2005), 897–910. https://doi.org/10.1109/TSE.2005.
112
[11] Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2012.
A systematic literature review on fault prediction performance in software engineering.
IEEE Transactions on Software Engineering 38, 6 (2012), 1276–1304.
[12] Zhimin He, Fayola Peters, Tim Menzies, and Ye Yang. 2013. Learning from
open-source projects: An empirical study on defect prediction. In 2013 ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement.
IEEE, 45–54.
[13] Zhimin He, Fengdi Shu, Ye Yang, Mingshu Li, and Qing Wang. 2012. An investigation
on the feasibility of cross-project defect prediction. Automated Software
Engineering 19, 2 (2012), 167–199.
[14] Xiao-Yuan Jing, Fei Wu, Xiwei Dong, and Baowen Xu. 2016. An improved SDA
based defect prediction framework for both within-project and cross-project
class-imbalance problems. IEEE Transactions on Software Engineering 43, 4 (2016),
321–339.
[15] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E. Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072–2106. https://doi.org/10.1007/s10664-015-9400-x
[16] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering (2013).
https://doi.org/10.1109/TSE.2012.70
[17] Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. 2017. Software defect prediction
via convolutional neural network. In 2017 IEEE International Conference on
Software Quality, Reliability and Security (QRS). IEEE, 318–328.
[18] Ruchika Malhotra. 2015. A systematic review of machine learning techniques
for software fault prediction. Applied Soft Computing 27 (2015), 504–518.
[19] Shane McIntosh and Yasutaka Kamei. 2018. Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction. IEEE
Transactions on Software Engineering 44, 5 (2018), 412–428.
[20] Thilo Mende and Rainer Koschke. 2010. Effort-aware defect prediction models.
In 2010 14th European Conference on Software Maintenance and Reengineering.
IEEE, 107–116.
[21] Tim Menzies, Ye Yang, George Mathew, Barry Boehm, and Jairus Hihn. 2017.
Negative results for software effort estimation. Empirical Software Engineering
22, 5 (2017), 2658–2683.
[22] Nikolaos Mittas and Lefteris Angelis. 2012. Ranking and clustering software cost
estimation models through a multiple comparisons algorithm. IEEE Transactions
on software engineering 39, 4 (2012), 537–551.
[23] Jaechang Nam, Sinno Jialin Pan, and Sunghun Kim. 2013. Transfer defect learning.
In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 382–
391.
[24] Annibale Panichella, Rocco Oliveto, and Andrea De Lucia. 2014. Cross-project
defect prediction models: L’union fait la force. In 2014 Software Evolution Week-
IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering
(CSMR-WCRE). IEEE, 164–173.
[25] Christoffer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit guru: analytics
and risk prediction of software commits. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. ACM, 966–969.
[26] Duksan Ryu, Okjoo Choi, and Jongmoon Baik. 2016. Value-cognitive boosting
with a support vector machine for cross-project defect prediction. Empirical
Software Engineering 21, 1 (2016), 43–71.
[27] Duksan Ryu, Jong-In Jang, and Jongmoon Baik. 2017. A transfer cost-sensitive
boosting approach for cross-project defect prediction. Software Quality Journal
25, 1 (2017), 235–272.
[28] Sadia Tabassum, Leandro L. Minku, Danyi Feng, George G. Cabral, and Liyan
Song. 2020. An Investigation of Cross-Project Learning in Online Just-In-Time
Software Defect Prediction – Supplementary Material. http://www.cs.bham.ac.
uk/~minkull/publications/TabassumICSE2020-supplement.pdf
[29] Ming Tan, Lin Tan, Sashank Dara, and Caleb Mayeux. 2015. Online defect prediction
for imbalanced data. In 2015 IEEE/ACM 37th IEEE International Conference
on Software Engineering, Vol. 2. IEEE, 99–108.
[30] Burak Turhan, Tim Menzies, Ayşe B Bener, and Justin Di Stefano. 2009. On the
relative value of cross-company and within-company data for defect prediction.
Empirical Software Engineering 14, 5 (2009), 540–578.
[31] András Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101–132.
[32] Romi Satria Wahono. 2015. A systematic literature review of software defect prediction:
research trends, datasets, methods and frameworks. Journal of Software
Engineering 1, 1 (2015), 1–16.
[33] Shuo Wang, Leandro L Minku, and Xin Yao. 2018. A systematic study of online
class imbalance learning with concept drift. IEEE transactions on neural networks
and learning systems 29, 10 (2018), 4802–4821.
[34] Tiejian Wang, Zhiwu Zhang, Xiaoyuan Jing, and Liqiang Zhang. 2016. Multiple
kernel ensemble learning for software defect prediction. Automated Software
Engineering 23, 4 (2016), 569–590.
[35] Thomas Zimmermann, Nachiappan Nagappan, Harald Gall, Emanuel Giger, and
Brendan Murphy. 2009. Cross-project defect prediction: a large scale experiment
on data vs. domain vs. process. In Proceedings of the the 7th joint meeting of the
European software engineering conference and the ACM SIGSOFT symposium on
The foundations of software engineering. ACM, 91–100.


>><[N]>Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction
[1] E. Giger, M. D’Ambros, M. Pinzger, and H. C. Gall, “Method-level
bug prediction,” in Proc. 6th Int. Symp. Empirical Softw. Eng. Meas.,
2012, pp. 171–180.
[2] H. Hata, O. Mizuno, and T. Kikuno, “Bug prediction based on
fine-grained module histories,” in Proc. 34th Int. Conf. Softw. Eng.,
2012, pp. 200–210.
[3] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting defects for
eclipse,” in Proc. 3rd Int. Workshop Predictor Models Softw. Eng.,
2007, Art. no. 9.
[4] N. Nagappan and T. Ball, “Use of relative code churn measures to
predict system defect density,” in Proc. 27th Int. Conf. Softw. Eng.,
2005, pp. 284–292.
[5] A. Mockus and D. M. Weiss, “Predicting risk of software
changes,” Bell Labs Tech. J., vol. 5, no. 2, pp. 169–180, 2000.
[6] Y. Kamei, et al., “A large-scale empirical study of just-in-time
quality assurance,” Trans. Softw. Eng., vol. 39, no. 6, pp. 757–773,
2013.
[7] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in Proc. 20th Int. Symp.
Foundations Softw. Eng., 2012, pp. 62:1–62:11.
[8] L. A. Belady and M. M. Lehman, “A model of large program
development,” IBM Syst. J., vol. 15, no. 3, pp. 225–252, 1976.
[9] S. Kim, E. J. Whitehead, Jr., and Y. Zhang, “Classifying software
changes: Clean or buggy?” IEEE Trans. Softw. Eng., vol. 34, no. 2,
pp. 181–196, 2008.
[10] O. Kononenko, O. Baysal, L. Guerrouj, Y. Cao, and M. W. Godfrey,
“Investigating code review quality: Do people and participation
matter?” in Proc. 31st Int. Conf. Softw. Maintenance Evol., 2015,
pp. 111–120.
[11] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction
for imbalanced data,” in Proc. 37th Int. Conf. Softw. Eng., 2015,
pp. 99–108.
[12] J. Aranda and G. Venolia, “The secret life of bugs: Going past the
errors and omissions in software repositories,” in Proc. 31st Int.
Conf. Softw. Eng., 2009, pp. 298–308.
[13] G. Antoniol, K. Ayari, M. D. Penta, F. Khomh, and Y.-G.
Gueheneuc, “Is it a bug or an enhancement? A text-based
approach to classify change requests,” in Proc. IBM Centre Adv.
Stud. Conf., 2008, pp. 23:1–23:15.
[14] K. Herzig, S. Just, and A. Zeller, “It’s Not a Bug, It’s a feature:
How misclassification impacts bug prediction,” in Proc. 35th Int.
Conf. Softw. Eng., 2013, pp. 392–401.
[15] C. Bird, et al., “Fair and balanced? bias in bug-fix datasets,” in
Proc. 7th Joint Meeting Eur. Softw. Eng. Conf. Symp. Foundations
Softw. Eng., 2009, pp. 121–130.
[16] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in
defect prediction,” in Proc. 33rd Int. Conf. Softw. Eng., 2011,
pp. 481–490.
[17] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and
K. Matsumoto, “The impact of mislabelling on the performance
and interpretation of defect prediction models,” in Proc. 37th Int.
Conf. Softw. Eng., 2015, pp. 812–823.
[18] T. H. D. Nguyen, B. Adams, and A. E. Hassan, “A case study of
bias in bug-fix datasets,” in Proc. 17th Work. Conf. Reverse Eng..,
2010, pp. 259–268.
[19] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, “Sample size
versus bias in defect prediction,” in Proc. 9th Joint Meeting Eur.
Softw. Eng. Conf. Symp. Foundations Softw. Eng.., 2013, pp. 147–157.
[20] B. Turhan, “On the dataset shift problem in software engineering
prediction models,” Empirical Softw. Eng., vol. 17, no. 1, pp. 62–74,
2012.
[21] T. Menzies, et al., “Local versus global lessons for defect prediction
and effort estimation,” IEEE Trans. Softw. Eng., vol. 39, no. 6,
pp. 334–345, 2013.
[22] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller,
“Predicting faults from cached history,” in Proc. 29th Int. Conf.
Softw. Eng., 2007, pp. 489–498.
[23] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in Proc.
Int. Conf. Softw. Eng., 2013, pp. 382–391.
[24] T. Zimmermann, N. Nagappan, and A. Zeller, “Predicting bugs
from history,” in Software Evolution. Berlin, Germany: Springer,
2008, ch. 4, pp. 69–88.
[25] J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein, “Tracking
concept drift of software projects using defect prediction quality,”
in Proc. 6th Work. Conf. Mining Softw. Repositories, 2009, pp. 51–60.
[26] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “An empirical
study of the impact of modern code review practices on software
quality,” Empirical Softw. Eng., vol. 21, no. 5, pp. 2146–2189,
2016.
[27] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida,
“Investigating code review practices in defective files: An empirical
study of the Qt system,” in Proc. 12th Work. Conf. Mining Softw.
Repositories, 2015, pp. 168–179.
[28] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida,
“Review participation in modern code review: An empirical study
of the Android, Qt, and OpenStack projects,” Empirical Softw. Eng.,
2016. doi: 10.1007/s10664-016-9452-6.
[29] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in Proc. 28th Int. Conf. Softw. Eng., 2006,
pp. 452–461.
[30] M. D’Ambros, M. Lanza, and R. Robbes, “An extensive comparison
of bug prediction approaches,” in Proc. 7th Work. Conf. Mining
Softw. Repositories, 2010, pp. 31–41.
[31] A. E. Hassan, “Predicting faults using the complexity of code
changes,” in Proc. 31st Int. Conf. Softw. Eng.), 2009, pp. 78–88.
[32] S. Matsumoto, Y. Kamei, A. Monden, K. ichi Matsumoto, and
M. Nakamura, “An analysis of developer metrics for fault prediction,”
in Proc. 6th Int. Conf. Predictive Models Softw. Eng., 2010,
pp. 18:1–18:9.
[33] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting fault
incidence using software change history,” IEEE Trans. Softw. Eng.,
vol. 26, no. 7, pp. 653–661, 2000.
[34] A. Porter, H. Siy, A. Mockus, and L. Votta, “Understanding the
sources of variation in software inspections,” ACM Trans. Softw.
Eng. Methodology, vol. 7, no. 1, pp. 41–79, 1998.
[35] E. S. Raymond, The Cathedral and the Bazaar. Sebastopol, CA, USA:
O’Reilly Media, 1999.
[36] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “The impact
of code review coverage and code review participation on software
quality: A case study of the QT, VTK, and ITK projects,” in
Proc. 11th Work. Conf. Mining Softw. Repositories, 2014, pp. 192–201.
[37] J. Sliwerski, T. Zimmermann, and A. Zeller, “When do changes
induce fixes?” in Proc. 2nd Int. Workshop Mining Softw. Repositories,
2005, pp. 1–5.
[38] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using
cross-project models,” Empirical Softw. Eng., vol. 21, no. 5,
pp. 2072–2106, 2016.
[39] S. Kim, T. Zimmermann, K. Pan, and E. J. J. Whitehead,
“Automatic identification of bug-introducing changes,” in Proc.
21st Int. Conf. Automated Softw. Eng., 2006, pp. 81–90.
[40] D. A. da Costa, S. McIntosh, W. Shang, U. Kulesza, R. Coelho, and
A. E. Hassan, “A framework for evaluating the results of the SZZ
approach for identifying bug-introducing changes,” IEEE Trans.
Softw. Eng., 2016, doi: 10.1109/TSE.2016.2616306.
[41] W. S. Sarle, “The VARCLUS Procedure,” in SAS/STAT User’s
Guide, 4th ed. Cary, NC, USA: SAS Institute, Inc., 1990.
[42] F. E. Harrell Jr., K. L. Lee, R. M. Califf, D. B. Pryor, and R. A. Rosati,
“Regression modelling strategies for improved prognostic pre-
diction,” Statist.Med., vol. 3, no. 2, pp. 143–152, 1984.
[43] F. E. Harrell Jr., K. L. Lee, D. B. Matchar, and T. A. Reichert,
“Regression models for prognostic prediction: Advantages, problems,
and suggested solutions,” Cancer Treatment Rep., vol. 69,
no. 10, pp. 1071–1077, 1985.
[44] R. Morales, S. McIntosh, and F. Khomh, “Do code review practices
impact design quality? A case study of the Qt, VTK, and ITK Projects,”
in Proc. Int. Conf. Softw. Eng., 2015, pp. 171–180.
[45] M. Zhou and A. Mockus, “Does the initial environment impact the
future of developers?” in Proc. 33rd Int. Conf. Softw. Eng., 2011,
pp. 271–280.
[46] F. E. Harrell Jr., Regression Modeling Strategies, 2nd ed. Berlin,
Germany: Springer, 2015.
[47] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “ReLink: Recovering
links between bugs and changes,” in Proc. 8th Joint Meeting Eur.
Softw. Eng. Conf. Symp. Foundations Softw. Eng., 2011, pp. 15–25.
[48] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact
of classification techniques on the performance of defect prediction
models,” in Proc. 37th Int. Conf. Softw. Eng., 2015, pp. 789–800.



>><[N]>CC2Vec: Distributed Representations of Code Changes
[1] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 81.
[2] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations.
[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learning
distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 40.
[4] George A Anastassiou. 2011. Univariate hyperbolic tangent neural network
approximation. Mathematical and Computer Modelling 53, 5-6 (2011), 1111–1132.
[5] Lerina Aversano, Luigi Cerulo, and Concettina Del Grosso. 2007. Learning from
bug-introducing changes to prevent fault prone code. In Ninth international
workshop on Principles of software evolution: in conjunction with the 6th ESEC/FSE
joint meeting. ACM, 19–26.
[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).
[7] Yalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei. 2018. Deep attention neural
tensor network for visual question answering. In Proceedings of the European
Conference on Computer Vision (ECCV). 20–35.
[8] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. Oops, my tests broke
the build: An explorative analysis of Travis CI with GitHub. In 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR). IEEE, 356–
367.
[9] Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In
Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.
Association for Computational Linguistics, 31.
[10] Guillaume Bouchard. 2007. Efficient bounds for the softmax function and applications
to approximate inference in hybrid models. In NIPS 2007 workshop for
approximate Bayesian inference in continuous/hybrid systems.
[11] Rich Caruana, Steve Lawrence, and C Lee Giles. 2001. Overfitting in neural nets:
Backpropagation, conjugate gradient, and early stopping. In Advances in neural
information processing systems. 402–408.
[12] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised
learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural
Networks 20, 3 (2009), 542–542.
[13] Brian Cheung. 2012. Convolutional neural networks applied to human face
classification. In 2012 11th International Conference on Machine Learning and
Applications, Vol. 2. IEEE, 580–583.
[14] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. 2013. Improving deep
neural networks for LVCSR using rectified linear units and dropout. In 2013
IEEE international conference on acoustics, speech and signal processing. IEEE,
8609–8613.
[15] Daniel DeFreez, Aditya V Thakur, and Cindy Rubio-González. 2018. Path-based
function embedding and its application to error-handling specification mining. In
Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. ACM,
423–433.
[16] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. 2013. Boa: A
language and infrastructure for analyzing ultra-large-scale software repositories.
In Proceedings of the 2013 International Conference on Software Engineering. IEEE
Press, 422–431.
[17] Vasiliki Efstathiou and Diomidis Spinellis. 2019. Semantic source code models
using identifier embeddings. In Proceedings of the 16th International Conference
on Mining Software Repositories. IEEE Press, 29–33.
[18] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
933–944.
[19] Martin T Hagan and Mohammad B Menhaj. 1994. Training feedforward networks
with the Marquardt algorithm. IEEE transactions on Neural Networks 5, 6 (1994),
989–993.
[20] Jordan Henkel, Shuvendu K Lahiri, Ben Liblit, and Thomas Reps. 2018. Code
vectors: Understanding programs through embedded abstracted symbolic traces.
In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. ACM,
163–174.
[21] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu
Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-time
defect prediction. In Proceedings of the 16th International Conference on Mining
Software Repositories. IEEE Press, 34–45.
[22] Thong Hoang, Julia Lawall, Yuan Tian, Richard J Oentaryo, and David Lo. 2019.
PatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the
Linux Kernel. IEEE Transactions on Software Engineering (2019).
[23] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment
generation. In Proceedings of the 26th Conference on Program Comprehension.
ACM, 200–210.
[24] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing source code using a neural attention model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 2073–2083.
[25] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural
networks for human action recognition. IEEE transactions on pattern analysis
and machine intelligence 35, 1 (2012), 221–231.
[26] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating
commit messages from diffs using neural machine translation. In Proceedings
of the 32nd IEEE/ACM International Conference on Automated Software Engineering.
IEEE Press, 135–146.
[27] Thorsten Joachims. 1999. Svmlight: Support vector machine. SVM-Light Support
Vector Machine http://svmlight. joachims. org/, University of Dortmund 19, 4 (1999).
[28] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072–2106.
[29] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2012. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2012), 757–773.
[30] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering 34, 2 (2008),
181–196.
[31] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar,
1746–1751. https://doi.org/10.3115/v1/D14-1181
[32] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. 2017. Structured
attention networks. arXiv preprint arXiv:1702.00887 (2017).
[33] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 (2014).
[34] Hiroyuki Kirinuki, Yoshiki Higo, Keisuke Hotta, and Shinji Kusumoto. 2014. Hey!
are you committing tangled changes?. In Proceedings of the 22nd International
Conference on Program Comprehension. ACM, 262–265.
[35] Vladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, and Alberto Bacchelli.
2019. PathMiner: a library for mining of path-based representations of code. In
Proceedings of the 16th International Conference on Mining Software Repositories.
IEEE Press, 13–17.
[36] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for
generating natural language summaries of program subroutines. In Proceedings
of the 41st International Conference on Software Engineering. IEEE Press, 795–806.
[37] Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples
using weighted logistic regression. In ICML, Vol. 3. 448–455.
[38] Mario Linares-Vásquez, Luis Fernando Cortés-Coy, Jairo Aponte, and Denys
Poshyvanyk. 2015. Changescribe: A tool for automatically generating commit
messages. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, Vol. 2. IEEE, 709–712.
[39] Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation:
how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. ACM, 373–384.
[40] Subhransu Maji, Lubomir Bourdev, and Jitendra Malik. 2011. Action recognition
from a distributed representation of pose and appearance. In CVPR 2011. IEEE,
3177–3184.
[41] Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze. 2010. Introduction
to information retrieval. Natural Language Engineering 16, 1 (2010),
100–103.
[42] Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction. IEEE
Transactions on Software Engineering 44, 5 (2017), 412–428.
[43] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[44] Audris Mockus and David M Weiss. 2000. Predicting risk of software changes.
Bell Labs Technical Journal 5, 2 (2000), 169–180.
[45] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted
boltzmann machines. In Proceedings of the 27th international conference
on machine learning (ICML-10). 807–814.
528
[46] Trong Duc Nguyen, Anh Tuan Nguyen, and Tien N Nguyen. 2016. Mapping
API elements for code migration with vector representations. In 2016 IEEE/ACM
38th International Conference on Software Engineering Companion (ICSE-C). IEEE,
756–758.
[47] Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N Nguyen.
2017. Exploring API embedding for API usages and applications. In 2017
IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE,
438–449.
[48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting on association for computational linguistics. Association for
Computational Linguistics, 311–318.
[49] Mohammad Masudur Rahman and Chanchal K Roy. 2015. TextRank based search
term identification for software change tasks. In 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE,
540–544.
[50] Mohammad Masudur Rahman, Chanchal K Roy, and Jason A Collins. 2016. Correct:
code reviewer recommendation in github based on cross-project and technology
experience. In 2016 IEEE/ACM 38th International Conference on Software
Engineering Companion (ICSE-C). IEEE, 222–231.
[51] Emad Shihab, Ahmed E Hassan, Bram Adams, and Zhen Ming Jiang. 2012. An
industrial study on the risk of software changes. In Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software Engineering.
ACM, 62.
[52] Richard Socher, Alex Perelygin, JeanWu, Jason Chuang, Christopher D Manning,
Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proceedings of the 2013 conference
on empirical methods in natural language processing. 1631–1642.
[53] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.
[54] Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. 1997. Introduction to multilayer
feed-forward neural networks. Chemometrics and intelligent laboratory
systems 39, 1 (1997), 43–62.
[55] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, Akinori Ihara,
and Kenichi Matsumoto. 2015. The impact of mislabelling on the performance
and interpretation of defect prediction models. In 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering, Vol. 1. IEEE, 812–823.
[56] Bart Theeten, Frederik Vandeputte, and Tom Van Cutsem. 2019. Import2vec
learning embeddings for software libraries. In Proceedings of the 16th International
Conference on Mining Software Repositories. IEEE Press, 18–28.
[57] Yuan Tian, Julia Lawall, and David Lo. 2012. Identifying Linux bug fixing patches.
In Proceedings of the 34th International Conference on Software Engineering. IEEE
Press, 386–396.
[58] Ke Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic neural program
embedding for program repair. arXiv preprint arXiv:1711.07163 (2017).
[59] ShuohangWang and Jing Jiang. 2017. A Compare-Aggregate Model for Matching
Text Sequences. In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. https:
//openreview.net/forum?id=HJTzHtqee
[60] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning
for just-in-time defect prediction. In 2015 IEEE International Conference on
Software Quality, Reliability and Security. IEEE, 17–26.
[61] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard
Hovy. 2016. Hierarchical attention networks for document classification. In
Proceedings of the 2016 conference of the North American chapter of the association
for computational linguistics: human language technologies. 1480–1489.
[62] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig.
2018. Learning to mine aligned code and natural language pairs from stack
overflow. In 2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR). IEEE, 476–486.
529


>><[N]>Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction
[1] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[2] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using crossproject
models,” Empirical Software Engineering (EMSE), vol. 21, no. 5,
pp. 2072–2106, 2015.
[3] N. Nan and D. E. Harter, “Impact of budget and schedule pressure
on software development cycle time and effort,” IEEE Transactions on
Software Engineering (TSE), vol. 35, no. 5, pp. 624–637, 2009.
[4] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
IEEE Transactions on Software Engineering (TSE), vol. 38, no. 6,
pp. 1276–1304, 2012.
[5] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering (TSE), vol. 39,
no. 6, pp. 757–773, 2013.
[6] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in Proceedings of the 20th
ACM SIGSOFT International Symposium on the Foundations of Software
Engineering (FSE), 2012, pp. 1–11.
[7] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction for
imbalanced data,” in Proceedings of the 37th International Conference
on Software Engineering (ICSE), 2015, pp. 99–108.
[8] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,” IEEE Transactions
on Software Engineering (TSE), vol. 44, no. 5, pp. 412–428,
2018.
[9] G. Ditzler, M. Roveri, C. Alippi, and R. Polikar, “Learning in nonstationary
environments: A survey,” IEEE Computational Intelligence
Magazine, vol. 10, no. 4, pp. 12–25, 2015.
[10] S. Wang, L. L. Minku, and X. Yao, “Resampling-based ensemble
methods for online class imbalance learning,” IEEE Transactions on
Knowledge and Data Engineering (TKDE), vol. 27, no. 5, pp. 1356–
1368, 2015.
[11] S. Kim, E. J. W. Jr., and Y. Zhang, “Classifying software changes: Clean
or buggy?” IEEE Transactions on Software Engineering (TSE), vol. 34,
no. 2, pp. 181–196, 2008.
[12] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” in Proceedings of the 17th International Workshop on Mining
Software Repositories, ser. MSR ’05, 2005, pp. 1–5.
[13] J. Eyolfson, L. Tan, and P. Lam, “Do time of day and developer
experience affect commit bugginess?” in Proceedings of the 8th Working
Conference on Mining Software Repositories (MSR), 2011, pp. 153–162.
[14] A. T. Misirli, E. Shihab, and Y. Kamei, “Studying high impact fixinducing
changes,” Empirical Software Engineering Journal (EMSE),
vol. 21, no. 2, pp. 605–641, 2016.
[15] S. Wang, L. L. Minku, and X. Yao, “A systematic study of online class
imbalance learning with concept drift,” IEEE Transactions on Neural
Networks and Learning Systems (TNNLS), 2018.
[16] J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein, “Tracking
concept drift of software projects using defect prediction quality,”
in Proceedings of the 6th Working Conference on Mining Software
Repositories (MSR), 2009, pp. 51–60.
[17] Z. Mahmood, D. Bowes, P. Lane, and T. Hall, “What is the impact of
imbalance on software defect prediction performance?” in Proceedings
of the 11th International Conference on Predictive Models and Data
Analytics in Software Engineering (PROMISE), 2015, pp. 4.1–4.4.
[18] S. Wang and X. Yao, “Using class imbalance learning for software defect
prediction,” IEEE Transactions on Reliability (TR), vol. 62, no. 2, pp.
434–443, 2013.
[19] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto, and K. Matsumoto,
“The effects of over and under sampling on fault-prone module detection,”
in Proceedings of the 1st International Symposium on Empirical
Software Engineering and Measurement (ESEM), 2007, pp. 196–204.
[20] K. E. Bennin, J. Keung, P. Phannachitta, A. Monden, and S. Mensah,
“Mahakil: Diversity based oversampling approach to alleviate the class
imbalance issue in software defect prediction,” IEEE Transactions on
Software Engineering (TSE), vol. 44, no. 6, pp. 534–550, June 2018.
[21] N. C. Oza, “Online bagging and boosting,” in Proceedings of the 2005
IEEE International Conference on Systems, Man and Cybernetics, vol. 3,
2005, pp. 2340–2345.
[22] P. Domingos and G. Hulten, “Mining high-speed data streams,” in
Proceedings of the 6th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2000, pp. 71–80.
[23] P. Zhang, X. Zhu, J. Tan, and L. Guo, “Classifier and cluster ensembles
for mining concept drifting data streams,” in Proceedings of the 2010
IEEE International Conference on Data Mining (ICDM), 2010, pp.
1175–1180.
[24] K. B. Dyer, R. Capo, and R. Polikar, “Compose: A semi-supervised
learning framework for initially labeled non-stationary streaming data,”
IEEE Transactions on Neural Networks and Learning Systems (TNNLS),
vol. 25, no. 1, pp. 12–26, 2013.
[25] A. D. Pozzolo, G. Boracchi, O. Caelen, C. Alippi, and G. Bontempi,
“Credit card fraud detection: a realistic modeling and a novel learning
strategy,” IEEE Transactions on Neural Networks and Learning Systems
(TNNLS), vol. 29, no. 8, pp. 3784–3797, 2018.
[26] M. Shepperd, Q. Song, Z. Sun, and C. Mair, “Data quality: Some
comments on the nasa software defect datasets,” IEEE Transactions on
Software Engineering (TSE), vol. 39, no. 9, pp. 1208–1215, 2013.
[27] C. Rosen, B. Grawi, and E. Shihab, “Commit guru: analytics and
risk prediction of software commits,” in Proceedings of the 10th Joint
Meeting on Foundations of Software Engineering (FSE), 2015, pp. 966–
969.
[28] H. He and E. A. Garcia, “Learning from imbalanced data,” IEEE
Transactions on Knowledge and Data Engineering (TKDE), vol. 21,
no. 9, pp. 1263–1284, 2009.
[29] J. Gama, R. Sebasti˜ao, and P. P. Rodrigues, “On evaluating stream
learning algorithms,” Machine Learning, vol. 90, no. 3, pp. 317–346,
2013.
[30] N. Mittas and L. Angelis, “Ranking and clustering software cost
estimation models through a multiple comparisons algorithm,” IEEE
Transactions on Software Engineering (TSE), vol. 39, no. 4, pp. 537–
551, 2013.
[31] T. Menzies, Y. Yang, G. Mathew, B. Boehm, and J. Hihn, “Negative
results for software effort estimation,” Empirical Software Engineering
(EMSE), vol. 22, no. 5, pp. 2658–2683, 2017.
[32] A. Vargha and H. D. Delaney, “A critique and improvement of the cl
common language effect size statistics of mcgraw and wong,” Journal
of Educational and Behavioral Statistics, vol. 25, no. 2, pp. 101–132,
2000.
[33] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“An empirical comparison of model validation techniques for defect
prediction models,” IEEE Transactions on Software Engineering (TSE),
vol. 43, no. 1, pp. 1–18, 2017.
[34] ——, “The impact of automated parameter optimization on defect
prediction models,” IEEE Transactions on Software Engineering (TSE),
pp. 1–1, 2018.
[35] T. Menzies and M. Shepperd, “Special issue on repeatable results
in software engineering prediction,” Empirical Software Engineering
(EMSE), vol. 17, pp. 1–17, 2012.
[36] L. Song, L. L. Minku, and X. Yao, “The impact of parameter tuning
on software effort estimation using learning machines,” in Proceedings
of the 9th International Conference on Predictive Models in Software
Engineering (PROMISE), 2013, pp. 1–10.




>><[N]>Code churn: A neglected metric in effort-aware just-in-time defect prediction
[1] S. Kim, E. Whitehead, Y. Zhang. Classifying software changes: clean
or buggy? IEEE Transactions on Software Engineering, 2008, 34(2):
181-196.
[2] A. E. Hassan. Predicting faults using the complexity of code changes.
ICSE 2009: 78-88.
[3] A. Meneely, L. Williams, W. Snipes, J. Osborne. Predicting failures
with developer networks and social network analysis. FSE 2008: 13-
23.
[4] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, A. Bener.
Defect prediction from static code features: current results,
limitations, new approaches. Automated Software Engineering, 2010,
17(4): 375-407.
[5] Y. Kamei, E. Shihab, B. Adams, A.E. Hassan, A. Mockus, A. Sinha,
N. Ubayashi. A large-scale empirical study of just-in-time quality
assurance. IEEE Transactions on Software Engineering, 2013, 39(6):
757-773.
[6] T. Fukushima, Y. Kamei, S. Mcintosh, K. Yamashita, N.
Ubayashi. An empirical study of just-in-time defect prediction using
cross-project models. MSR 2014: 172-181.
[7] N. Bettenburg, M. Nagappan, A. E. Hassan. Think locally, act
globally: improving defect and effort prediction models. MSR 2012:
60-69.
[8] T. L. Graves, A. F. Karr, J. S. Marron, H. Siy. Predicting fault
incidence using software change history. IEEE Transactions on
Software Engineering, 2000, 26(7): 653-661.
[9] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, H.
Leung. Effort-aware just-in-time defect prediction: simple
unsupervised models could be better than supervised models. FSE
2016: 157-168.
[10] N. Nagappan, T. Ball, A. Zeller, Mining metrics to predict component
failures. ICSE 2006: 452-461.
[11] L. Cheung, R. Roshandel, N. Medvidovic, L. Golubchik. Early
prediction of software component reliability. ICSE 2008: 111-120.
[12] S. Kim, T. Zimmermann, E. J. Whitehead Jr., A. Zeller. Predicting
faults from cached history. ICSE 2007:489 - 498.
[13] E. Giger, M. DeAmbros, M. Pinzger, H.C. Gall. Method-level bug
prediction. ESEM 2012: 171-180.
[14] S. Kim, H. Zhang, R. Wu, L. Gong, Dealing with noise in defect
prediction. ICSE 2011: 481-490.
[15] H. Valpola. Bayesian ensemble learning for nonlinear factor analysis.
cta Polytechnic Scandinavia. Mathematics and Computing Series
No.108, pp. 26-27
[16] S. Zhong, T. Khoshgoftaar, N. Seliya. Unsupervised learning for
expert-based software quality estimation. HASE 2004: 149-155.
[17] K. Herzig, S. Just, A. Zeller. It's not a bug, it's a feature: how
misclassification impacts bug prediction. ICSE 2013: 392-401.
[18] Y. Zhou, B. Xu, H. Leung, L. Chen. An in-depth study of the
potentially confounding effect of class size in fault prediction. ACM
Transactions on Software Engineering and Methodology, 2014, 23(1).
[19] T.Wang, Z. Zhang, X. Jing, L. Zhang. Multiple kernel ensemble
learning for software defect prediction. Automated Software
Engineering, 2015, 23(4): 569-590.
[20] Mockus, A. and Weiss, D.M. Predicting risk of software changes[J].
Bell Labs Technical Journal, 2000, 5(2): 169-180.
[21] J. Liwerski, T. Zimmermann, A. Zeller. 2005. When do changes
induce fixes. MSR 2005, 30(4): 1-5.
[22] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, L. Bairavasundaram.
How do fixes become bugs. FSE 2011: 26-36.
[23] E. Arisholm,  LC. Briand, EB. Johannessen. A systematic and
comprehensive investigation of methods to build and evaluate fault
prediction models. Journal of Systems and Software, 2010, 83(1): 2-
17.
[24] S. Wang, X. Yao, Using Class Imbalance Learning for Software
Defect Prediction[J]. IEEE Transactions on Reliability, 2013, 62(2):
434-443.
[25] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, A. Bener.
Defect prediction from static code features: current results,
limitations, new approaches. Automated Software Engineering, 2010,
17(4): 375-407.
[26] X. Yang, D. Lo, X. Xia, Y. Zhang, J. Sun. Deep learning for just-intime
defect prediction. QRS 2015:17-26.
[27] F. Rahman, D. Posnett, P. Devanbu. Recalling the "imprecision" of
cross-project defect prediction. FSE 2012.
[28] F. Rahman, P. Devanbu. How, and why, process metrics are better.
ICSE 2013: 432-441.
[29] X. Yang, D. Lo, X. Xia, J. Sun. TLEL: A two-layer ensemble
learning approach for just-in-time defect prediction. Information and
Software Technology, 2017, accepted.
[30] Visual Studio, Microsoft. https://www.visualstudio.com/zh-hans/
[31] M. Tan, L. Tan, S. Dara, C. Mayeux. Online defect prediction for
imbalanced data. ICSE 2015:99-108
[32] W. Fu, T. Menzies. Revisiting unsupervised learning for defect
prediction. arXiv:1703.00132 [cs.SE], 2017.
[33] Y. Benjamini, Y. Hochberg. Controlling the false discovery rate: a
practical and powerful approach to multiple testing. Journal of the
royal statistical society. 1995, 57(1): 289-300
[34] J. Romano, J. D. Kromrey, J. Coraggio, J. Skowronek, L. Devine.
Exploring methods for evaluating group differences on the NSSE and
other surveys: Are the t-test and Cohen'sd indices the most
appropriate choices. SAIR 2006.
[35] J. Eyolfson,  T. Lin, P. Lam. Do time of day and developer
experience affect commit bugginess. MSR 2011: 153-162.
[36] J.C. Munson, S.G. Elbaum. Code churn: a measure for estimating the
impact of code change. ICSM 1998: 24-31.
[37] N. Nagappan, T. Ball. Use of relative code churn measures to predict
system defect density. ICSE 2005: 284-292.
[38] N. Nagappan, T. Ball. Using software dependencies and churn
metrics to predict field failures: An empirical case study. ESEM
2007: 364-373.
[39] Y. Shin, A. Meneely, L. Williams, J.A. Osborne. Evaluating
complexity, code churn, and developer activity metrics as indicators
of software vulnerabilities. IEEE Transactions on Software
Engineering, 2011, 37(6): 772-787.



>><[N]>Commit Guru: Analytics and Risk Prediction of Software Commits
[1] “The economic impacts of inadequate infrastructure for
software testing.”
http://www.nist.gov/director/planning/upload/report02-3.pdf.
[2] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting
defects for Eclipse,” in PROMISE ’07: Proceedings of the
Third International Workshop on Predictor Models in
Software Engineering, 2007, pp. 1–7.
[3] R. Moser, W. Pedrycz, and G. Succi, “A comparative
analysis of the efficiency of change metrics and static code
attributes for defect prediction,” in Proceedings of the 30th
International Conference on Software Engineering, ser.
ICSE ’08, 2008, pp. 181–190.
[4] E. Shihab, “An exploration of challenges limiting pragmatic
software defect prediction,” PhD Thesis, Queen’s University,
2012.
[5] Y. Brun, R. Holmes, M. D. Ernst, and D. Notkin, “Crystal:
Precise and unobtrusive conflict warnings,” in Proceedings of
the 19th ACM SIGSOFT symposium and the 13th European
conference on Foundations of software engineering. ACM,
2011, pp. 444–447.
[6] M. L. Guimarães and A. R. Silva, “Improving early detection
of software merge conflicts,” in Proceedings of the 34th
International Conference on Software Engineering. IEEE
Press, 2012, pp. 342–352.
[7] M. W. Godfrey, A. E. Hassan, J. Herbsleb, G. C. Murphy,
M. Robillard, P. Devanbu, A. Mockus, D. E. Perry, and
D. Notkin, “Future of mining software archives: A
roundtable,” IEEE Software, vol. 26, no. 1, pp. 67 –70,
Jan.-Feb. 2009.
[8] A. Hassan, “The road ahead for mining software
repositories,” in Frontiers of Software Maintenance, 2008,
Oct. 2008, pp. 48 –57.
[9] E. Shihab, “Pragmatic prioritization of software quality
assurance efforts,” in Proceedings of the 33rd International
Conference on Software Engineering, ser. ICSE ’11, 2011,
pp. 1106–1109.
[10] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An
industrial study on the risk of software changes,” in
Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering, ser.
FSE ’12, 2012, pp. 62:1–62:11.
[11] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus,
A. Sinha, and N. Ubayashi, “A large-scale empirical study of
just-in-time quality assurance,” Software Engineering, IEEE
Transactions on, vol. 39, no. 6, pp. 757–773, 2013.
[12] S. Kim, E. J. Whitehead, and Y. Zhang, “Classifying
software changes: Clean or buggy?” Software Engineering,
IEEE Transactions on, vol. 34, no. 2, pp. 181–196, 2008.
[13] A. Hindle, D. M. German, and R. Holt, “What do large
commits tell us?: A taxonomical study of large commits,” in
Proceedings of the 2008 International Working Conference
on Mining Software Repositories, ser. MSR ’08, 2008, pp.
99–108.
[14] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do
changes induce fixes?” ACM sigsoft software engineering
notes, vol. 30, no. 4, pp. 1–5, 2005.
[15] “List of file formats,”
http://en.wikipedia.org/wiki/List_of_file_formats, Jun. 2015.


>><[N]>Cross-Project Just-in-Time Bug Prediction for Mobile Apps: An Empirical Assessment
[1] P. M. Duvall, S. Matyas, and A. Glover, Continuous integration:
improving software quality and reducing risk. Pearson Education,
2007.
[2] G. Booch, Object oriented analysis & design with application. Pearson
Education India, 2006.
[3] K. Beck, Extreme programming explained: embrace change. addisonwesley
professional, 2000.
[4] J. Humble and D. Farley, Continuous Delivery: Reliable Software
Releases through Build, Test, and Deployment Automation (Adobe
Reader). Pearson Education, 2010.
[5] A. Hindle, A. Wilson, K. Rasmussen, E. J. Barlow, J. C. Campbell,
and S. Romansky, “Greenminer: A hardware based mining software
repositories software energy consumption framework,” in Proceedings of
the 11th Working Conference on Mining Software Repositories. ACM,
2014, pp. 12–21.
[6] D. Pagano and W. Maalej, “User feedback in the appstore: An empirical
study,” in Requirements Engineering Conference (RE), 2013 21st IEEE
International. IEEE, 2013, pp. 125–134.
[7] A. Holzer and J. Ondrus, “Mobile application market: A developer’s
perspective,” Telematics and informatics, vol. 28, no. 1, pp. 22–31, 2011.
[8] A. I. Wasserman, “Software engineering issues for mobile application
development,” in FSE/SDP, 2010.
[9] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, “Ar-miner:
mining informative reviews for developers from mobile app marketplace,”
in Proceedings of the 36th International Conference on Software
Engineering. ACM, 2014, pp. 767–778.
[10] A. Di Sorbo, S. Panichella, C. V. Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, “What would users change in my
app? summarizing app reviews for recommending software changes,” in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering. ACM, 2016, pp. 499–510.
[11] F. Palomba, P. Salza, A. Ciurumelea, S. Panichella, H. Gall, F. Ferrucci,
and A. De Lucia, “Recommending and localizing change requests
for mobile apps based on user reviews,” in Proceedings of the 39th
International Conference on Software Engineering. IEEE Press, 2017,
pp. 106–117.
[12] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
IEEE Transactions on Software Engineering, vol. 38, no. 6, pp. 1276–
1304, 2012.
[13] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[14] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi, and
A. E. Hassan, “Studying just-in-time defect prediction using cross-project
models,” Empirical Software Engineering, vol. 21, no. 5, pp. 2072–2106,
2016.
[15] A. Kaur, K. Kaur, and H. Kaur, “Application of machine learning
on process metrics for defect prediction in mobile application,” in
Information Systems Design and Intelligent Applications. Springer,
2016, pp. 81–98.
[16] G. Catolino, “Just-in-time bug prediction in mobile applications: the
domain matters!” in Proceedings of the 4th International Conference
on Mobile Software Engineering and Systems. IEEE Press, 2017, pp.
201–202.
[17] M. Linares-Vásquez, S. Klock, C. McMillan, A. Sabané, D. Poshyvanyk,
and Y.-G. Guéhéneuc, “Domain matters: bringing further evidence of
the relationships among anti-patterns, application domains, and qualityrelated
metrics in java mobile apps,” in Proceedings of the 22nd
International Conference on Program Comprehension. ACM, 2014, pp.
232–243.
[18] H. van Heeringen and E. Van Gorp, “Measure the functional size of a
mobile app: Using the cosmic functional size measurement method,” in
IWSM-MENSURA, 2014.
[19] A. Panichella, R. Oliveto, and A. De Lucia, “Cross-project defect
prediction models: L’union fait la force,” in Software Maintenance,
Reengineering and Reverse Engineering (CSMR-WCRE), 2014 Software
Evolution Week-IEEE Conference on. IEEE, 2014, pp. 164–173.
[20] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact
of classification techniques on the performance of defect prediction
models,” in Proceedings of the 37th International Conference on Software
Engineering-Volume 1. IEEE Press, 2015, pp. 789–800.
[21] T. Wang, W. Li, H. Shi, and Z. Liu, “Software defect prediction based on
classifiers ensemble,” Journal of Information & Computational Science,
vol. 8, no. 16, pp. 4241–4254, 2011.
[22] D. Di Nucci, F. Palomba, R. Oliveto, and A. De Lucia, “Dynamic
selection of classifiers in bug prediction: An adaptive method,” IEEE
Transactions on Emerging Topics in Computational Intelligence, vol. 1,
no. 3, pp. 202–212, 2017.
[23] J. Petri´c, D. Bowes, T. Hall, B. Christianson, and N. Baddoo, “Building
an ensemble for software defect prediction based on diversity selection,”
Proceedings of the 10th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement, p. 46, 2016.
[24] C. Rosen, B. Grawi, and E. Shihab, “Commit guru: analytics and risk
prediction of software commits,” in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. ACM, 2015, pp.
966–969.
[25] J. R. Quinlan, “Induction of decision trees,” Machine learning, vol. 1,
no. 1, pp. 81–106, 1986.
[26] S. Le Cessie and J. C. Van Houwelingen, “Ridge estimators in logistic
regression,” Applied statistics, pp. 191–201, 1992.
[27] R. Kohavi, “The power of decision tables,” in European conference on
machine learning. Springer, 1995, pp. 174–189.
[28] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support
vector machines,” IEEE Intelligent Systems and their applications, vol. 13,
no. 4, pp. 18–28, 1998.
[29] R. O. Duda and P. E. Hart, Pattern classification and scene analysis, ser.
A Wiley-Interscience publication. Wiley, 1973. [Online]. Available:
http://www.worldcat.org/oclc/00388788
[30] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5–32, 2001.
[31] T. G. Dietterich, “Ensemble methods in machine learning,” International
workshop on multiple classifier systems, pp. 1–15, 2000.
[32] L. Breiman, “Bagging predictors,” Machine learning, pp. 123–140, 1996.
[33] L. Rokach, “Ensemble-based classifiers,” Artificial Intelligence Review,
vol. 33, no. 1, pp. 1–39, 2010.
[34] P. Refaeilzadeh, L. Tang, and H. Liu, “Cross-validation,” in Encyclopedia
of database systems. Springer, 2009, pp. 532–538.
[35] D. Di Nucci, F. Palomba, G. De Rosa, G. Bavota, R. Oliveto, and
A. De Lucia, “A developer centered bug prediction model,” IEEE
Transactions on Software Engineering, 2017.
[36] T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman, F. Shull,
B. Turhan, and T. Zimmermann, “Local versus global lessons for
defect prediction and effort estimation,” IEEE Transactions on software
engineering, vol. 39, no. 6, pp. 822–834, 2013.
[37] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-grained just-in-time
defect prediction,” Journal of Systems and Software, 2018.
[38] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
“An empirical study of just-in-time defect prediction using cross-project
models,” in Proceedings of the 11th Working Conference on Mining
Software Repositories. ACM, 2014, pp. 172–181.
[39] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,” IEEE
Transactions on Software Engineering, vol. 44, no. 5, pp. 412–428,
2018.
[40] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for justin-
time defect prediction,” in Software Quality, Reliability and Security
(QRS), 2015 IEEE International Conference on. IEEE, 2015, pp. 17–26.
[41] J. G. Barnett, C. K. Gathuru, L. S. Soldano, and S. McIntosh, “The
relationship between commit message detail and defect proneness in java
projects on github,” in Proceedings of the 13th International Conference
on Mining Software Repositories. ACM, 2016, pp. 496–499.
[42] X. Yang, D. Lo, X. Xia, and J. Sun, “Tlel: A two-layer ensemble learning
approach for just-in-time defect prediction,” Information and Software
Technology, vol. 87, pp. 206–220, 2017.
[43] X. Chen, Y. Zhao, Q. Wang, and Z. Yuan, “Multi: Multi-objective effortaware
just-in-time software defect prediction,” Information and Software
Technology, vol. 93, pp. 1–13, 2018.
[44] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: simple unsupervised models
could be better than supervised models,” in Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 2016, pp. 157–168.
[45] Q. Huang, X. Xia, and D. Lo, “Supervised vs unsupervised models:
A holistic look at effort-aware just-in-time defect prediction,” in 2017
IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 2017, pp. 159–170.
[46] M. Nayrolles and A. Hamou-Lhadj, “Clever: Combining code metrics
with clone detection for just-in-time fault prevention and resolution in
large industrial projects,” 2018.
[47] L. D’Avanzo, F. Ferrucci, C. Gravino, and P. Salza, “Cosmic functional
measurement of mobile applications and code size estimation,” in
Proceedings of the 30th Annual ACM Symposium on Applied Computing.
ACM, 2015, pp. 1631–1636.
[48] R. Kohavi and G. H. John, “Wrappers for feature subset selection,”
Artificial intelligence, vol. 97, no. 1-2, pp. 273–324, 1997.
[49] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy, “Crossproject
defect prediction: a large scale experiment on data vs. domain
vs. process,” in Proceedings of the the 7th joint meeting of the European
software engineering conference. ACM, 2009, pp. 91–100.
[50] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Software Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[51] S. Watanabe, H. Kaiya, and K. Kaijiri, “Adapting a fault prediction
model to allow inter language reuse,” in Proc. of the 4th international
workshop on Predictor models in software engineering. ACM, 2008,
pp. 19–24.
[52] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in Proceedings of the 28th international conference
on Software engineering. ACM, 2006, pp. 452–461.
[53] M. Jureczko and L. Madeyski, “Towards identifying software project
clusters with regard to defect prediction,” in Proceedings of the 6th
International Conference on Predictive Models in Software Engineering.
ACM, 2010, p. 9.
[54] Z. He, F. Shu, Y. Yang, M. Li, and Q. Wang, “An investigation on
the feasibility of cross-project defect prediction,” Automated Software
Engineering, vol. 19, no. 2, pp. 167–199, 2012.
[55] A. T. Mısırlı, A. B. Bener, and B. Turhan, “An industrial case study
of classifier ensembles for locating software defects,” Software Quality
Journal, vol. 19, no. 3, pp. 515–536, 2011.
[56] Y. Zhang, D. Lo, X. Xia, and J. Sun, “An empirical study of classifier
combination for cross-project defect prediction,” Proceedings of the
IEEE Annual Computer Software and Applications Conference, vol. 2,
pp. 264–269, 2015.
[57] Y. Liu, T. M. Khoshgoftaar, and N. Seliya, “Evolutionary optimization of
software quality modeling with multiple repositories,” IEEE Transactions
on Software Engineering, vol. 36, no. 6, pp. 852–864, Nov 2010.
[58] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in
defect prediction,” Proceedings of International Conference on Software
Engineering, pp. 481–490, 2011.
[59] D. Bowes, T. Hall, and J. Petri´c, “Software defect prediction: do different
classifiers find the same defects?” Software Quality Journal, vol. 26,
no. 2, pp. 525–552, 2018.
[60] G. Catolino, F. Palomba, A. De Lucia, F. Ferrucci, and A. Zaidman,
“Enhancing change prediction models using developer-related factors,”
Journal of Systems and Software, vol. 143, pp. 14–28, 2018.
[61] G. Seni and J. F. Elder, “Ensemble methods in data mining: improving
accuracy through combining predictions,” Synthesis Lectures on Data
Mining and Knowledge Discovery, vol. 2, no. 1, pp. 1–126, 2010.
[62] G. Catolino, D. Di Nucci, and F. Ferrucci. (2019) Cross-project just-intime
bug prediction for mobile app: An empirical assessment - online
appendix https://figshare.com/s/9a075be3e1fb64f76b48.
[63] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[64] R. Moser, W. Pedrycz, and G. Succi, “A comparative analysis of
the efficiency of change metrics and static code attributes for defect
prediction,” in Proceedings of the 30th international conference on
Software engineering. ACM, 2008, pp. 181–190.
[65] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and M. Nakamura,
“An analysis of developer metrics for fault prediction,” in Proceedings
of the 6th International Conference on Predictive Models in Software
Engineering. ACM, 2010, p. 18.
[66] G. Catolino, F. Palomba, A. De Lucia, F. Ferrucci, and A. Zaidman,
“Developer-related factors in change prediction: an empirical assessment,”
in Proceedings of the 25th International Conference on Program
Comprehension. IEEE Press, 2017, pp. 186–195.
[67] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy, “Characterizing
and predicting which bugs get fixed: an empirical study of microsoft
windows,” in Software Engineering, 2010 ACM/IEEE 32nd International
Conference on, vol. 1. IEEE, 2010, pp. 495–504.
[68] R. M. O’brien, “A caution regarding rules of thumb for variance inflation
factors,” Quality & Quantity, vol. 41, no. 5, pp. 673–690, 2007.
[69] H. Liu and H. Motoda, Feature selection for knowledge discovery and
data mining. Springer Science & Business Media, 2012, vol. 454.
[70] K. E. Bennin, J. Keung, P. Phannachitta, A. Monden, and S. Mensah,
“Mahakil: Diversity based oversampling approach to alleviate the class
imbalance issue in software defect prediction,” IEEE Transactions on
Software Engineering, vol. 44, no. 6, pp. 534–550, 2018.
[71] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
“Smote: synthetic minority over-sampling technique,” Journal of artificial
intelligence research, vol. 16, pp. 321–357, 2002.
[72] K. Ba´nczyk, O. Kempa, T. Lasota, and B. Trawi´nski, “Empirical
comparison of bagging ensembles created using weak learners for a
regression problem,” Asian Conference on Intelligent Information and
Database Systems, pp. 312–322, 2011.
[73] L. Reyzin and R. E. Schapire, “How boosting the margin can also boost
classifier complexity,” Proceedings of the 23rd international conference
on Machine learning, pp. 753–760, 2006.
[74] Y. Liu, T. M. Khoshgoftaar, and N. Seliya, “Evolutionary optimization of
software quality modeling with multiple repositories,” IEEE Transactions
on Software Engineering, vol. 36, no. 6, pp. 852–864, 2010.
[75] E. Ceylan, F. O. Kutlubay, and A. B. Bener, “Software defect identification
using machine learning techniques,” in Software Engineering and
Advanced Applications, 2006. SEAA’06. 32nd EUROMICRO Conference
on. IEEE, 2006, pp. 240–247.
[76] A. Okutan and O. T. Yıldız, “Software defect prediction using bayesian
networks,” Empirical Software Engineering, vol. 19, no. 1, pp. 154–181,
2014.
[77] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,”
Journal of Machine Learning Research, vol. 13, no. Feb, pp.
281–305, 2012.
[78] P. Baldi, S. Brunak, Y. Chauvin, C. A. Andersen, and H. Nielsen,
“Assessing the accuracy of prediction algorithms for classification: an
overview,” Bioinformatics, vol. 16, no. 5, pp. 412–424, 2000.
[79] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto, “An
empirical comparison of model validation techniques for defect prediction
models,” IEEE Transanctions on Software Engineering, vol. 43, no. 1,
pp. 1–18, 2017.
[80] A. J. Scott and M. Knott, “A cluster analysis method for grouping means
in the analysis of variance,” Biometrics, vol. 30, pp. 507–512, 1974.
[81] F. Palomba, M. Linares-Vásquez, G. Bavota, R. Oliveto, M. Di Penta,
D. Poshyvanyk, and A. De Lucia, “Crowdsourcing user reviews to support
the evolution of mobile apps,” Journal of Systems and Software, vol.
137, pp. 143–162, 2018.
[82] N. Nagappan and T. Ball, “Use of relative code churn measures to
predict system defect density,” in Proceedings of the 27th international
conference on Software engineering. ACM, 2005, pp. 284–292.
[83] V. N. Inukollu, D. D. Keshamoni, T. Kang, and M. Inukollu, “Factors
influencing quality of mobile apps: Role of mobile app development life
cycle,” arXiv preprint arXiv:1410.4537, 2014.
[84] P. Salza, F. Palomba, D. Di Nucci, C. D’Uva, A. De Lucia, and F. Ferrucci,
“Do developers update third-party libraries in mobile apps?” 2018.
[85] J. Li, T. Stålhane, J. M. Kristiansen, and R. Conradi, “Cost drivers of
software corrective maintenance: An empirical study in two companies,”
in Software Maintenance (ICSM), 2010 IEEE International Conference
on. IEEE, 2010, pp. 1–8.
[86] E. Kocaguneli, T. Menzies, and J. W. Keung, “On the value of ensemble
effort estimation,” IEEE Transactions on Software Engineering, vol. 38,
no. 6, pp. 1403–1416, 2012.
[87] G. Catolino and F. Ferrucci, “Ensemble techniques for software change
prediction: A preliminary investigation,” in Machine Learning Techniques
for Software Quality Evaluation (MaLTeSQuE), 2018 IEEE Workshop
on. IEEE, 2018, pp. 25–30.
[88] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“Automated parameter optimization of classification techniques for defect
prediction models,” in Software Engineering (ICSE), 2016 IEEE/ACM
38th International Conference on. IEEE, 2016, pp. 321–332.
[89] M. Shepperd, Q. Song, Z. Sun, and C. Mair, “Data quality: Some
comments on the nasa software defect datasets,” IEEE Transactions on
Software Engineering, vol. 39, no. 9, pp. 1208–1215, 2013.
[90] T. Wolf, A. Schroter, D. Damian, and T. Nguyen, “Predicting build
failures using social network analysis on developer communication,”
in Proceedings of the 31st International Conference on Software
Engineering. IEEE Computer Society, 2009, pp. 1–11.


>><[N]>Deep Learning for Just-In-Time Defect Prediction
[1] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” TSE, vol. 39, no. 6, pp. 757–773, 2013.
[2] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect prediction from static code features: current results, limitations,
new approaches,” Automated Software Engineering, vol. 17, no. 4, pp.
375–407, 2010.
[3] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in ASE,
2013, pp. 279–289.
[4] M. DAmbros, M. Lanza, and R. Robbes, “Evaluating defect prediction
approaches: a benchmark and an extensive comparison,” Empirical
Software Engineering, vol. 17, no. 4-5, pp. 531–577, 2012.
[5] T. Zimmermann and N. Nagappan, “Predicting defects using network
analysis on dependency graphs,” in ICSE, 2008, pp. 531–540.
[6] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Software Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[7] S. Kim, E. J. Whitehead, and Y. Zhang, “Classifying software changes:
Clean or buggy?” TSE, vol. 34, no. 2, pp. 181–196, 2008.
[8] Y. Kamei, S. Matsumoto, A. Monden, K.-i. Matsumoto, B. Adams,
and A. E. Hassan, “Revisiting common bug prediction findings using
effort-aware models,” in ICSM, 2010, pp. 1–10.
[9] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,
2006.
[10] G. E. Hinton, “Learning multiple layers of representation,” Trends in
cognitive sciences, vol. 11, no. 10, pp. 428–434, 2007.
[11] L. Deng, J. Li, J.-T. Huang, K. Yao, D. Yu, F. Seide, M. Seltzer,
G. Zweig, X. He, J. Williams et al., “Recent advances in deep learning
for speech research at microsoft,” in Acoustics, Speech and Signal
Processing (ICASSP), 2013 IEEE International Conference on, 2013,
pp. 8604–8608.
[12] F. Rahman, D. Posnett, and P. Devanbu, “Recalling the imprecision of
cross-project defect prediction,” in FSE, 2012, p. 61.
[13] F. Rahman and P. Devanbu, “How, and why, process metrics are better,”
in ICSE, 2013, pp. 432–441.
[14] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, “Sample size vs.
bias in defect prediction,” in ESEC/FSE, 2013, pp. 147–157.
[15] E. Arisholm, L. C. Briand, and M. Fuglerud, “Data mining techniques
for building fault-proneness models in telecom java software,” in ISSRE,
2007, pp. 215–224.
[16] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in ICSE,
2013, pp. 382–391.
[17] G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella, and
S. Panichella, “Multi-objective cross-project defect prediction,” in ICST,
2013, pp. 252–261.
[18] J. Han and M. Kamber, Data Mining: Concepts and Techniques, 2006.
[19] A. Mockus and D. M. Weiss, “Predicting risk of software changes,”
Bell Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[20] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in ICSE, 2006, pp. 452–461.
[21] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in ICSE, 2009, pp. 78–88.
[22] N. Nagappan and T. Ball, “Use of relative code churn measures to
predict system defect density,” in ICSE, 2005, pp. 284–292.
[23] A. G. Koru, D. Zhang, K. El Emam, and H. Liu, “An investigation
into the functional form of the size-defect relationship for software
modules,” TSE, vol. 35, no. 2, pp. 293–304, 2009.
[24] R. Purushothaman and D. E. Perry, “Toward understanding the rhetoric
of small source code changes,” TSE, vol. 31, no. 6, pp. 511–526, 2005.
[25] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting fault
incidence using software change history,” TSE, vol. 26, no. 7, pp. 653–
661, 2000.
[26] H. He and E. A. Garcia, “Learning from imbalanced data,” TKDE,
vol. 21, no. 9, pp. 1263–1284, 2009.
[27] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto, and K.-i. Matsumoto,
“The effects of over and under sampling on fault-prone module
detection,” in ESEM. IEEE, 2007, pp. 196–204.
[28] T. M. Khoshgoftaar, X. Yuan, and E. B. Allen, “Balancing misclassification
rates in classification-tree models of software quality,” Empirical
Software Engineering, vol. 5, no. 4, pp. 313–330, 2000.
[29] G. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for
deep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554,
2006.
[30] X. Xia, Y. Feng, D. Lo, Z. Chen, and X. Wang, “Towards more accurate
multi-label software behavior learning,” in CSMR-WCRE, 2014, pp.
134–143.
[31] X. Xia, D. Lo, X. Wang, and B. Zhou, “Tag recommendation in software
information sites,” in MSR, 2013, pp. 287–296.
[32] N. Cliff, Ordinal methods for behavioral data analysis, 2014.
[33] X. X. J. S. Yun Zhang, David Lo, “An empirical study of classifier
combination for cross-project defect prediction,” in COMPSAC. IEEE,
2015.
[34] X. X. Y. T. Xiao Xuan, David Lo, “Evaluating defect prediction
approaches using a massive set of metrics: An empirical study,” in SAC.
IEEE, 2015.
[35] F. Peters, T. Menzies, and A. Marcus, “Better cross company defect
prediction,” in MSR, 2013, pp. 409–418.
[36] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-project defect prediction: a large scale experiment on data vs.
domain vs. process,” in ESEC/FSE, 2009, pp. 91–100.
[37] M. L¨angkvist, L. Karlsson, and A. Loutfi, “A review of unsupervised
feature learning and deep learning for time-series modeling,” Pattern
Recognition Letters, vol. 42, pp. 11–24, 2014.
[38] Y.-l. Boureau, Y. L. Cun et al., “Sparse feature learning for deep belief
networks,” in Advances in neural information processing systems, 2008,
pp. 1185–1192.
[39] A.-r. Mohamed, G. E. Dahl, and G. Hinton, “Acoustic modeling using
deep belief networks,” Audio, Speech, and Language Processing, IEEE
Transactions on, vol. 20, no. 1, pp. 14–22, 2012.
[40] C. Zhu, J. Yin, and Q. Li, “A stock decision support system based on
dbns,” Journal of Computational Information Systems, vol. 10, no. 2,
pp. 883–893, 2014.


>><[N]>DeepJIT: An End-To-End Deep Learning Framework for Just-In-Time Defect Prediction
[1] Y. Kamei and E. Shihab, “Defect prediction: Accomplishments and
future challenges,” in Leaders of Tomorrow Symposium: Future of
Software Engineering, FOSE@SANER 2016, Osaka, Japan, March 14,
2016, 2016, pp. 33–45.
[2] M. D’Ambros, M. Lanza, and R. Robbes, “Evaluating defect prediction
approaches: A benchmark and an extensive comparison,” Empirical
Softw. Engg., vol. 17, no. 4-5, pp. 531–577, Aug. 2012. [Online].
Available: http://dx.doi.org/10.1007/s10664-011-9173-9
[3] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[4] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering, ser. FSE ’12. New York, NY, USA: ACM, 2012, pp. 62:1–
62:11. [Online]. Available: http://doi.acm.org/10.1145/2393596.2393670
[5] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and
K. Matsumoto, “The impact of mislabelling on the performance and
interpretation of defect prediction models,” in Proceedings of the 37th
International Conference on Software Engineering - Volume 1, ser.
ICSE ’15. Piscataway, NJ, USA: IEEE Press, 2015, pp. 812–823.
[Online]. Available: http://dl.acm.org/citation.cfm?id=2818754.2818852
[6] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Trans. Softw. Eng., vol. 39, no. 6, pp. 757–773, Jun.
2013. [Online]. Available: http://dx.doi.org/10.1109/TSE.2012.70
[7] S. Kim, E. J. Whitehead, Jr., and Y. Zhang, “Classifying software
changes: Clean or buggy?” IEEE Trans. Softw. Eng., vol. 34, no. 2,
pp. 181–196, Mar. 2008. [Online]. Available: http://dx.doi.org/10.1109/
TSE.2007.70773
[8] O. Kononenko, O. Baysal, L. Guerrouj, Y. Cao, and M. W. Godfrey,
“Investigating code review quality: Do people and participation matter?”
in Proceedings of the 2015 IEEE International Conference on Software
Maintenance and Evolution (ICSME), ser. ICSME ’15. Washington,
DC, USA: IEEE Computer Society, 2015, pp. 111–120. [Online].
Available: http://dx.doi.org/10.1109/ICSM.2015.7332457
[9] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic
features for defect prediction,” in Proceedings of the 38th International
Conference on Software Engineering, ser. ICSE ’16. New York,
NY, USA: ACM, 2016, pp. 297–308. [Online]. Available: http:
//doi.acm.org/10.1145/2884781.2884804
[10] Z. Tu, Z. Su, and P. Devanbu, “On the localness of software,” in
Proceedings of the 22Nd ACM SIGSOFT International Symposium
on Foundations of Software Engineering, ser. FSE 2014. New
York, NY, USA: ACM, 2014, pp. 269–280. [Online]. Available:
http://doi.acm.org/10.1145/2635868.2635875
[11] A. T. Nguyen and T. N. Nguyen, “Graph-based statistical language
model for code,” in Proceedings of the 37th International Conference
on Software Engineering - Volume 1, ser. ICSE ’15. Piscataway,
NJ, USA: IEEE Press, 2015, pp. 858–868. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2818754.2818858
[12] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, “On the
naturalness of software,” in Proceedings of the 34th International
Conference on Software Engineering, ser. ICSE ’12. Piscataway,
NJ, USA: IEEE Press, 2012, pp. 837–847. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2337223.2337322
[13] Z. Li and Y. Zhou, “Pr-miner: Automatically extracting implicit
programming rules and detecting violations in large software code,” in
Proceedings of the 10th European Software Engineering Conference
Held Jointly with 13th ACM SIGSOFT International Symposium
on Foundations of Software Engineering, ser. ESEC/FSE-13. New
York, NY, USA: ACM, 2005, pp. 306–315. [Online]. Available:
http://doi.acm.org/10.1145/1081706.1081755
[14] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning
for just-in-time defect prediction,” in Proceedings of the 2015 IEEE
International Conference on Software Quality, Reliability and Security,
ser. QRS ’15. Washington, DC, USA: IEEE Computer Society, 2015,
pp. 17–26. [Online]. Available: http://dx.doi.org/10.1109/QRS.2015.14
[15] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,
no. 7553, p. 436, 2015.
[16] Y. Kim, “Convolutional neural networks for sentence classification,”
arXiv preprint arXiv:1408.5882, 2014.
[17] C. dos Santos and M. Gatti, “Deep convolutional neural networks for
sentiment analysis of short texts,” in Proceedings of COLING 2014, the
25th International Conference on Computational Linguistics: Technical
Papers, 2014, pp. 69–78.
[18] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural
network for modelling sentences,” arXiv preprint arXiv:1404.2188,
2014.
[19] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional
networks for text classification,” in Advances in neural information
processing systems, 2015, pp. 649–657.
[20] R. Johnson and T. Zhang, “Effective use of word order for text
categorization with convolutional neural networks,” arXiv preprint
arXiv:1412.1058, 2014.
[21] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving
target?: A longitudinal case study of just-in-time defect prediction,”
in Proceedings of the 40th International Conference on Software
Engineering, ser. ICSE ’18. New York, NY, USA: ACM, 2018, pp. 560–
560. [Online]. Available: http://doi.acm.org/10.1145/3180155.3182514
[22] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and
L. Fei-Fei, “Large-scale video classification with convolutional neural
networks,” in Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition, 2014, pp. 1725–1732.
[23] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, “Face recognition:
A convolutional neural-network approach,” IEEE transactions on neural
networks, vol. 8, no. 1, pp. 98–113, 1997.
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural information
processing systems, 2012, pp. 1097–1105.
[25] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
restoration with neural networks,” IEEE Transactions on Computational
Imaging, vol. 3, no. 1, pp. 47–57, 2017.
[26] G. Tolias, R. Sicre, and H. J´egou, “Particular object retrieval with integral
max-pooling of cnn activations,” arXiv preprint arXiv:1511.05879,
2015.
[27] M. D. Zeiler and R. Fergus, “Stochastic pooling for regularization of
deep convolutional neural networks,” arXiv preprint arXiv:1301.3557,
2013.
[28] L. Bottou, “Large-scale machine learning with stochastic gradient descent,”
in Proceedings of the 19th International Conference on Computational
Statistics (COMPSTAT). Springer, 2010, pp. 177–186.
[29] S. Bird and E. Loper, “Nltk: the natural language toolkit,” in Proceedings
of the ACL 2004 on Interactive poster and demonstration sessions.
Association for Computational Linguistics, 2004, p. 31.
[30] P. Willett, “The porter stemming algorithm: then and now,” Program,
2006.
[31] M. White, C. Vendome, M. Linares-V´asquez, and D. Poshyvanyk,
“Toward deep learning software repositories,” in Proceedings of the 12th
Working Conference on Mining Software Repositories. IEEE Press,
2015, pp. 334–345.
[32] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann
machines,” in Proceedings of the 27th international conference on
machine learning (ICML-10), 2010, pp. 807–814.
[33] G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural
networks for lvcsr using rectified linear units and dropout,” in Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE International
Conference on. IEEE, 2013, pp. 8609–8613.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[35] H. T. Ng and J. Zelle, “Corpus-based approaches to semantic interpretation
in nlp,” AI magazine, vol. 18, no. 4, p. 45, 1997.
[36] N. V. Chawla, N. Japkowicz, and A. Kotcz, “Special issue on learning
from imbalanced data sets,” ACM Sigkdd Explorations Newsletter, vol. 6,
no. 1, pp. 1–6, 2004.
[37] Z.-H. Zhou and X.-Y. Liu, “Training cost-sensitive neural networks with
methods addressing the class imbalance problem,” IEEE Transactions on
Knowledge and Data Engineering, vol. 18, no. 1, pp. 63–77, 2006.
[38] M. Kukar, I. Kononenko et al., “Cost-sensitive learning with neural
networks.” in ECAI, 1998, pp. 445–449.
[39] R. Caruana, S. Lawrence, and C. L. Giles, “Overfitting in neural nets:
Backpropagation, conjugate gradient, and early stopping,” in Advances
in neural information processing systems, 2001, pp. 402–408.
[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
“Dropout: A simple way to prevent neural networks from over-
fitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp.
1929–1958, 2014.
[41] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in Proceedings of 3rd International Conference on Learning Representations
(ICLR), 2015.
[42] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and
S. Mougiakakou, “Lung pattern classification for interstitial lung diseases
using a deep convolutional neural network,” IEEE transactions on
medical imaging, vol. 35, no. 5, pp. 1207–1216, 2016.
[43] S. Arora, N. Cohen, and E. Hazan, “On the optimization of deep
networks: Implicit acceleration by overparameterization,” in 35th International
Conference on Machine Learning (ICML), 2018, pp. 244–253.
[44] M. T. Hagan and M. B. Menhaj, “Training feedforward networks with
the Marquardt algorithm,” IEEE Transactions on Neural Networks,
vol. 5, no. 6, pp. 989–993, 1994.
[45] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in Proceedings of the 28th international conference
on Software engineering. ACM, 2006, pp. 452–461.
[46] Y. Kamei, S. Matsumoto, A. Monden, K. Matsumoto, B. Adams, and
A. E. Hassan, “Revisiting common bug prediction findings using effort
aware models,” in Proc. Int’l Conf. on Software Maintenance (ICSM’10),
2010, pp. 1–10.
[47] M. D’Ambros, M. Lanza, and R. Robbes, “An extensive comparison
of bug prediction approaches,” in Mining Software Repositories (MSR),
2010 7th IEEE Working Conference on. IEEE, 2010, pp. 31–41.
[48] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in ICSE ’09: Proceedings of the 2009 IEEE 31st International Conference
on Software Engineering, 2009, pp. 78–88.
[49] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and M. Nakamura,
“An analysis of developer metrics for fault prediction,” in Proceedings
of the 6th International Conference on Predictive Models in
Software Engineering. ACM, 2010, p. 18.
[50] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting fault
incidence using software change history,” IEEE Trans. Softw. Eng.,
vol. 26, no. 7, pp. 653–661, 2000.
[51] A. Porter, H. Siy, A. Mockus, and L. Votta, “Understanding the sources
of variation in software inspections,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 7, no. 1, pp. 41–79, 1998.
[52] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida, “Investigating
code review practices in defective files: An empirical study of the qt
system,” in Proceedings of the 12th Working Conference on Mining
Software Repositories. IEEE Press, 2015, pp. 168–179.
[53] E. S. Raymond, The Cathedral and the Bazaar: Musings on Linux and
Open Source by an Accidental Revolutionary. Oreilly & Associates
Inc, 2001.
[54] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “The impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects,” in Proceedings of the 11th
Working Conference on Mining Software Repositories. ACM, 2014,
pp. 192–201.
[55] S. Mcintosh, Y. Kamei, B. Adams, and A. E. Hassan, “An empirical
study of the impact of modern code review practices on software
quality,” Empirical Softw. Engg., vol. 21, no. 5, pp. 2146–2189, Oct.
2016. [Online]. Available: http://dx.doi.org/10.1007/s10664-015-9381-9
[56] J. Fox, Applied regression analysis, linear models, and related methods.
Sage Publications, Inc, 1997.
[57] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” science, vol. 313, no. 5786, pp. 504–507,
2006.
[58] N. M. Nasrabadi, “Pattern recognition and machine learning,” Journal
of electronic imaging, vol. 16, no. 4, p. 049901, 2007.
[59] G. H. Nguyen, A. Bouzerdoum, and S. L. Phung, “Learning pattern
classification tasks with imbalanced data sets,” in Pattern recognition.
InTech, 2009.
[60] Q. Gu, Z. Cai, L. Zhu, and B. Huang, “Data mining on imbalanced
data sets,” in Advanced Computer Theory and Engineering, 2008.
ICACTE’08. International Conference on. IEEE, 2008, pp. 1020–1024.
[61] A. Severyn and A. Moschitti, “Learning to rank short text pairs with
convolutional deep neural networks,” in Proceedings of the 38th International
Conference on Research and Development in Information
Retrieval (SIGIR). ACM, 2015, pp. 373–382.
[62] X. Huo, M. Li, and Z.-H. Zhou, “Learning unified features from natural
and programming languages for locating buggy source code.” in IJCAI,
2016, pp. 1606–1612.
[63] X. Huo and M. Li, “Enhancing the unified features to locate buggy files
by exploiting the sequential nature of source code,” in Proceedings of the
26th International Joint Conference on Artificial Intelligence (IJCAI).
AAAI Press, 2017, pp. 1909–1915.
[64] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-adaptation
of feature detectors,” arXiv preprint arXiv:1207.0580, 2012.
[65] L. Prechelt, “Automatic early stopping using cross validation: quantifying
the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–767, 1998.
[66] R. Kohavi et al., “A study of cross-validation and bootstrap for accuracy
estimation and model selection,” in Ijcai, vol. 14, no. 2. Montreal,
Canada, 1995, pp. 1137–1145.
[67] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, “Sample size vs.
bias in defect prediction,” in Proceedings of the 2013 9th joint meeting
on foundations of software engineering. ACM, 2013, pp. 147–157.
[68] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of
classification techniques on the performance of defect prediction models,”
in Proceedings of the 37th International Conference on Software
Engineering-Volume 1. IEEE Press, 2015, pp. 789–800.
[69] B. Korbar, A. M. Olofson, A. P. Miraflor, C. M. Nicka, M. A.
Suriawinata, L. Torresani, A. A. Suriawinata, and S. Hassanpour, “Deep
learning for classification of colorectal polyps on whole-slide images,”
Journal of pathology informatics, vol. 8, 2017.
[70] J. Liu, W.-C. Chang, Y. Wu, and Y. Yang, “Deep learning for extreme
multi-label text classification,” in Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in Information
Retrieval. ACM, 2017, pp. 115–124.
[71] N. A. Gawande, J. A. Daily, C. Siegel, N. R. Tallent, and A. Vishnu,
“Scaling deep learning workloads: Nvidia dgx-1/pascal and intel knights
landing,” Future Generation Computer Systems, 2018.
[72] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The impact of
class rebalancing techniques on the performance and interpretation of
defect prediction models,” 2018.
[73] L. Aversano, L. Cerulo, and C. Del Grosso, “Learning from bugintroducing
changes to prevent fault prone code,” in Proc. Int’l Workshop
on Principles of Software Evolution (IWPSE’07), 2007, pp. 19–26.
[74] S. Kim, E. J. Whitehead, Jr., and Y. Zhang, “Classifying software
changes: Clean or buggy?” IEEE Trans. Softw. Eng., vol. 34, no. 2,
pp. 181–196, 2008.
[75] S. Wang, T. Liu, J. Nam, and L. Tan, “Deep semantic feature learning for
software defect prediction,” IEEE Transactions on Software Engineering,
pp. 1–1, 2018.
[76] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via
convolutional neural network,” in 2017 IEEE International Conference
on Software Quality, Reliability and Security (QRS), July 2017, pp. 318–
328.
[77] H. K. Dam, T. Tran, T. Pham, S. W. Ng, J. Grundy, and
A. Ghose, “Automatic feature learning for predicting vulnerable
software components,” IEEE Transactions on Software Engineering,
2019. [Online]. Available: DOI:10.1109/TSE.2018.2881961.
[78] W. Yin, H. Sch¨utze, B. Xiang, and B. Zhou, “Abcnn: Attention-based
convolutional neural network for modeling sentence pairs,” Transactions
of the Association for Computational Linguistics, vol. 4, pp. 259–272,
2016.


>><[N]>Effort-Aware Just-in-Time Defect Identification in Practice: A Case Study at Alibaba
[1] Hervé Abdi. 2007. Bonferroni and Šidák corrections for multiple comparisons.
Encyclopedia of measurement and statistics 3 (2007), 103–107.
[2] Benjamin M Bolker, Mollie E Brooks, Connie J Clark, Shane W Geange, John R
Poulsen, M Henry H Stevens, and Jada-Simone S White. 2009. Generalized linear
mixed models: a practical guide for ecology and evolution. Trends in ecology &
evolution 24, 3 (2009), 127–135.
[3] Andrew P Bradley. 1997. The use of the area under the ROC curve in the
evaluation of machine learning algorithms. Pattern recognition 30, 7 (1997),
1145–1159.
[4] Norman Cliff. 2014. Ordinal methods for behavioral data analysis. Psychology
Press.
[5] Jacek Czerwonka, Rajiv Das, Nachiappan Nagappan, Alex Tarvo, and Alex Teterev.
2011. Crane: Failure prediction, change analysis and test prioritization in practice–
experiences from windows. In Software Testing, Verification and Validation (ICST),
2011 IEEE Fourth International Conference on. IEEE, 357–366.
[6] Yuanrui Fan, Xin Xia, David Lo, and Ahmed E Hassan. 2018. Chaff from the
Wheat: Characterizing and Determining Valid Bug Reports. IEEE Transactions on
Software Engineering (2018).
[7] Wei Fu and Tim Menzies. 2017. Revisiting unsupervised learning for defect
prediction. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering. ACM, 72–83.
[8] Takafumi Fukushima, Yasutaka Kamei, Shane McIntosh, Kazuhiro Yamashita, and
Naoyasu Ubayashi. 2014. An empirical study of just-in-time defect prediction
using cross-project models. In Proceedings of the 11th Working Conference on
Mining Software Repositories. ACM, 172–181.
[9] Frank E Harrell. 2001. Regression modeling strategies: with applications to linear
models, logistic regression, and survival analysis. Springer.
[10] Safwat Hassan, Chakkrit Tantithamthavorn, Cor-Paul Bezemer, and Ahmed E
Hassan. 2018. Studying the dialogue between users and developers of free apps
in the google play store. Empirical Software Engineering 23, 3 (2018), 1275–1312.
[11] Hideaki Hata, Osamu Mizuno, and Tohru Kikuno. 2012. Bug prediction based on
fine-grained module histories. In Proceedings of the 34th International Conference
on Software Engineering. IEEE Press, 200–210.
[12] Qiao Huang, Xin Xia, and David Lo. 2017. Supervised vs unsupervised models:
A holistic look at effort-aware just-in-time defect prediction. In Software
Maintenance and Evolution (ICSME), 2017 IEEE International Conference on. IEEE,
159–170.
[13] Qiao Huang, Xin Xia, and David Lo. 2018. Revisiting Supervised and Unsupervised
Models for Effort-Aware Just-in-Time Defect Prediction. Empirical Software
Engineering (2018), In press.
[14] Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction. In
Proceedings of the 28th IEEE/ACM International Conference on Automated Software
Engineering. IEEE Press, 279–289.
[15] Paul CD Johnson. 2014. Extension of Nakagawa & Schielzeth’s R2GLMM to
random slopes models. Methods in Ecology and Evolution 5, 9 (2014), 944–946.
[16] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072–2106.
[17] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Aloka Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. Software Engineering, IEEE Transactions on 39, 6
(2013), 757–773.
[18] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering 34, 2 (2008),
181–196.
[19] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking classification models for software defect prediction: A proposed
framework and novel findings. IEEE Transactions on Software Engineering 34, 4
(2008), 485–496.
[20] Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and
E James Whitehead Jr. 2013. Does bug prediction support human developers?
findings from a google case study. In Proceedings of the 2013 International Conference
on Software Engineering. IEEE Press, 372–381.
[21] Heng Li, Weiyi Shang, Ying Zou, and Ahmed E Hassan. 2017. Towards just-intime
suggestions for log changes. Empirical Software Engineering 22, 4 (2017),
1831–1865.
[22] Chao Liu, Dan Yang, Xin Xia, Meng Yan, and Xiaohong Zhang. 2019. A twophase
transfer learning model for cross-project defect prediction. Information
and Software Technology 107 (2019), 125–136.
[23] Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. 2017.
Code churn: A neglected metric in effort-aware just-in-time defect prediction. In
Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software
Engineering and Measurement. IEEE Press, 11–19.
[24] Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction. IEEE
Transactions on Software Engineering (2017).
[25] Thilo Mende and Rainer Koschke. 2010. Effort-aware defect prediction models. In
Software Maintenance and Reengineering (CSMR), 2010 14th European Conference
on. IEEE, 107–116.
[26] Tim Menzies, Jeremy Greenwald, and Art Frank. 2007. Data mining static code
attributes to learn defect predictors. IEEE transactions on software engineering 1
(2007), 2–13.
[27] André N Meyer, Thomas Fritz, Gail C Murphy, and Thomas Zimmermann. 2014.
Software developers’ perceptions of productivity. In Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of Software Engineering. ACM,
19–29.
[28] Osamu Mizuno and Tohru Kikuno. 2007. Training on errors experiment to detect
fault-prone software modules by spam filter. In Proceedings of the the 6th joint
meeting of the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering. ACM, 405–414.
[29] Audris Mockus and David M Weiss. 2000. Predicting risk of software changes.
Bell Labs Technical Journal 5, 2 (2000), 169–180.
[30] Shinichi Nakagawa and Holger Schielzeth. 2013. A general and simple method for
obtaining R2 from generalized linear mixed-effects models. Methods in Ecology
and Evolution 4, 2 (2013), 133–142.
[31] Mathieu Nayrolles and Abdelwahab Hamou-Lhadj. 2018. CLEVER: combining
code metrics with clone detection for just-in-time fault prevention and resolution
in large industrial projects. In Proceedings of the 15th International Conference on
Mining Software Repositories. ACM, 153–164.
[32] Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques
actually helping programmers?. In Proceedings of the 2011 international symposium
on software testing and analysis. ACM, 199–209.
[33] Gopi Krishnan Rajbahadur, ShaoweiWang, Yasutaka Kamei, and Ahmed E Hassan.
2017. The impact of using regression models to build defect classifiers. In 2017
IEEE/ACM 14th International Conference on Mining Software Repositories (MSR).
IEEE, 135–145.
[34] Alastair J Scott and M Knott. 1974. A cluster analysis method for grouping means
in the analysis of variance. Biometrics (1974), 507–512.
[35] Martin Shepperd, David Bowes, and Tracy Hall. 2014. Researcher bias: The use
of machine learning in software defect prediction. IEEE Transactions on Software
Engineering 40, 6 (2014), 603–616.
[36] Emad Shihab, Ahmed E Hassan, Bram Adams, and Zhen Ming Jiang. 2012. An
industrial study on the risk of software changes. In Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software Engineering.
ACM, 62.
[37] Jacek Śliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When do
changes induce fixes?. In ACM sigsoft software engineering notes, Vol. 30. ACM,
1–5.
[38] Tom AB Snijders. 2005. Fixed and random effects. Encyclopedia of statistics in
behavioral science (2005).
[39] Ming Tan, Lin Tan, Sashank Dara, and Caleb Mayeux. 2015. Online defect
prediction for imbalanced data. In Software Engineering (ICSE), 2015 IEEE/ACM
37th IEEE International Conference on, Vol. 2. IEEE, 99–108.
[40] Chakkrit Tantithamthavorn and Ahmed E Hassan. 2018. An experience report
on defect modelling in practice: Pitfalls and challenges. In Proceedings of the 40th
International Conference on Software Engineering: Software Engineering in Practice.
ACM, 286–295.
[41] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2016. Automated parameter optimization of classification techniques
for defect prediction models. In Software Engineering (ICSE), 2016 IEEE/ACM 38th
International Conference on. IEEE, 321–332.
[42] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2017. An empirical comparison of model validation techniques for
defect prediction models. IEEE Transactions on Software Engineering 43, 1 (2017),
1–18.
[43] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2018. The impact of automated parameter optimization on defect
prediction models. IEEE Transactions on Software Engineering (2018).
[44] AB Tom, Tom AB Snijders Roel J Bosker, and Roel J Bosker. 1999. Multilevel
analysis: an introduction to basic and advanced multilevel modeling. Sage.
[45] Zhiyuan Wan, Xin Xia, Ahmed E Hassan, David Lo, Jianwei Yin, and Xiaohu
Yang. 2018. Perceptions, Expectations, and Challenges in Defect Prediction. IEEE
Transactions on Software Engineering (2018).
[46] Wikipedia. [n.d.]. https://en.wikipedia.org/wiki/Alibaba_Group. ([n. d.]).
[47] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. Breakthroughs
in Statistics (1992), 196–202.
[48] Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, and
Zhenchang Xing. 2017. What do developers search for on the web? Empirical
Software Engineering 22, 6 (2017), 3149–3185.
[49] Xin Xia, LO David, Sinno Jialin Pan, Nachiappan Nagappan, and Xinyu Wang.
2016. Hydra: Massively compositional model for cross-project defect prediction.
IEEE Transactions on software Engineering 42, 10 (2016), 977.
[50] Meng Yan, Yicheng Fang, David Lo, Xin Xia, and Xiaohong Zhang. 2017. Filelevel
defect prediction: Unsupervised vs. supervised models. In 2017 ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM). IEEE, 344–353.
[51] Meng Yan, Xin Xia, David Lo, Ahmed E Hassan, and Shanping Li. 2019. Characterizing
and identifying reverted commits. Empirical Software Engineering 24, 4
(2019), 2171–2208.
[52] Meng Yan, Xin Xia, Emad Shihab, David Lo, Jianwei Yin, and Xiaohu Yang.
2018. Automating change-level self-admitted technical debt determination. IEEE
Transactions on Software Engineering 45, 12 (2018), 1211–1229.
[53] Xinli Yang, David Lo, Xin Xia, and Jianling Sun. 2017. TLEL: A two-layer ensemble
learning approach for just-in-time defect prediction. Information and Software
Technology 87 (2017), 206–220.
[54] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning
for just-in-time defect prediction. In Software Quality, Reliability and Security
(QRS), 2015 IEEE International Conference on. IEEE, 17–26.
[55] Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei Xu,
Baowen Xu, and Hareton Leung. 2016. Effort-aware just-in-time defect prediction:
simple unsupervised models could be better than supervised models. In Proceedings
of the 2016 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering. ACM, 157–168.
[56] Zuoning Yin, Ding Yuan, Yuanyuan Zhou, Shankar Pasupathy, and Lakshmi
Bairavasundaram. 2011. How do fixes become bugs?. In Proceedings of the 19th
ACM SIGSOFT symposium and the 13th European conference on Foundations of
software engineering. ACM, 26–36.
[57] Feng Zhang, Audris Mockus, Iman Keivanloo, and Ying Zou. 2014. Towards
building a universal defect prediction model. In Proceedings of the 11th Working
Conference on Mining Software Repositories. ACM, 182–191.
[58] Feng Zhang, Audris Mockus, Ying Zou, Foutse Khomh, and Ahmed E Hassan. 2013.
How does context affect the distribution of software maintainability metrics?. In
2013 IEEE International Conference on Software Maintenance. IEEE, 350–359.
[59] Yuming Zhou, Yibiao Yang, Hongmin Lu, Lin Chen, Yanhui Li, Yangyang Zhao,
Junyan Qian, and Baowen Xu. 2018. How FarWe Have Progressed in the Journey?
An Examination of Cross-Project Defect Prediction. ACMTransactions on Software
Engineering and Methodology (TOSEM) 27, 1 (2018), 1.
[60] Thomas Zimmermann, Nachiappan Nagappan, Harald Gall, Emanuel Giger, and
Brendan Murphy. 2009. Cross-project defect prediction: a large scale experiment
on data vs. domain vs. process. In Proceedings of the the 7th joint meeting of the
European software engineering conference and the ACM SIGSOFT symposium on
The foundations of software engineering. ACM, 91–100.


>><[N]>Effort-Aware Just-in-Time Defect Prediction: Simple Unsupervised Models Could Be Better Than Supervised Models
[1] E. Arisholm, L. C. Briand, and E. B. Johannessen. A
systematic and comprehensive investigation of methods
to build and evaluate fault prediction models. Journal
of Systems and Software, 83(1):2{17, Jan. 2010.
[2] Y. Benjamini and Y. Hochberg. Controlling the False
Discovery Rate: A Practical and Powerful Approach to
Multiple Testing. Journal of the Royal Statistical
Society. Series B (Methodological), 57(1):289{300, Jan.
1995.
[3] P. Bishnu and V. Bhattacherjee. Software Fault
Prediction Using Quad Tree-Based K-Means Clustering
Algorithm. IEEE Transactions on Knowledge and Data
Engineering, 24(6):1146{1150, June 2012.
[4] M. D'Ambros, M. Lanza, and R. Robbes. An extensive
comparison of bug prediction approaches. In 2010 7th
IEEE Working Conference on Mining Software
Repositories (MSR), pages 31{41, May 2010.
[5] J. Ekanayake, J. Tappolet, H. C. Gall, and
A. Bernstein. Time variance and defect prediction in
software projects. Empirical Software Engineering,
17(4-5):348{389, Nov. 2011.
[6] J. Eyolfson, L. Tan, and P. Lam. Do Time of Day and
Developer Experience Affect Commit Bugginess? MSR
'11, pages 153{162, New York, NY, USA, 2011. ACM.
[7] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita,
and N. Ubayashi. An Empirical Study of Just-in-time
Defect Prediction Using Cross-project Models. In
Proceedings of the 11th Working Conference on Mining
Software Repositories, MSR 2014, pages 172{181, New
York, NY, USA, 2014. ACM.
[8] B. Ghotra, S. McIntosh, and A. E. Hassan. Revisiting
the Impact of Classification Techniques on the
Performance of Defect Prediction Models. In 2015
IEEE/ACM 37th IEEE International Conference on
Software Engineering (ICSE), volume 1, pages 789{800,
May 2015.
[9] T. Graves, A. Karr, J. Marron, and H. Siy. Predicting
fault incidence using software change history. IEEE
Transactions on Software Engineering, 26(7):653{661,
July 2000.
[10] T. Hall, S. Beecham, D. Bowes, D. Gray, and
S. Counsell. A Systematic Literature Review on Fault
Prediction Performance in Software Engineering. IEEE
Transactions on Software Engineering, 38(6):1276{1304,
Nov. 2012.
[11] A. E. Hassan. Predicting Faults Using the Complexity
of Code Changes. In Proceedings of the 31st
International Conference on Software Engineering,
ICSE '09, pages 78{88, Washington, DC, USA, 2009.
IEEE Computer Society.
[12] Y. Kamei, S. Matsumoto, A. Monden, K.-i. Matsumoto,
B. Adams, and A. E. Hassan. Revisiting Common Bug
Prediction Findings Using Effort-aware Models. In
Proceedings of the 2010 IEEE International Conference
on Software Maintenance, ICSM '10, pages 1{10,
Washington, DC, USA, 2010. IEEE Computer Society.
[13] Y. Kamei, E. Shihab, B. Adams, A. Hassan,
A. Mockus, A. Sinha, and N. Ubayashi. A large-scale
empirical study of just-in-time quality assurance. IEEE
Transactions on Software Engineering, 39(6):757{773,
June 2013.
[14] H. Khalid, M. Nagappan, E. Shihab, and A. E. Hassan.
Prioritizing the Devices to Test Your App on: A Case
Study of Android Game Apps. In Proceedings of the
22Nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2014, pages
610{620, New York, NY, USA, 2014. ACM.
[15] S. Kim, E. Whitehead, and Y. Zhang. Classifying
Software Changes: Clean or Buggy? IEEE
Transactions on Software Engineering, 34(2):181{196,
Mar. 2008.
[16] A. Koru, D. Zhang, K. El Emam, and H. Liu. An
Investigation into the Functional Form of the
Size-Defect Relationship for Software Modules. IEEE
Transactions on Software Engineering, 35(2):293{304,
Mar. 2009.
[17] A. Koru, D. Zhang, and H. Liu. Modeling the Effect of
Size on Defect Proneness for Open-Source Software.
pages 115{124, May 2007.
[18] A. G. Koru, K. E. Emam, D. Zhang, H. Liu, and
D. Mathew. Theory of relative defect proneness.
Empirical Software Engineering, 13(5):473{498, Oct.
2008.
[19] G. Koru, H. Liu, D. Zhang, and K. E. Emam. Testing
the theory of relative defect proneness for closed-source
software. Empirical Software Engineering,
15(6):577{598, Dec. 2010.
[20] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch.
Benchmarking Classification Models for Software
Defect Prediction: A Proposed Framework and Novel
Findings. IEEE Transactions on Software Engineering,
34(4):485{496, July 2008.
[21] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J.
Whitehead Jr. Does Bug Prediction Support Human
Developers? Findings from a Google Case Study. In
Proceedings of the 2013 International Conference on
Software Engineering, ICSE '13, pages 372{381,
Piscataway, NJ, USA, 2013. IEEE Press.
[22] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto,
and M. Nakamura. An analysis of developer metrics for
fault prediction. In Proceedings of the 6th International
Conference on Predictive Models in Software
Engineering, PROMISE '10, pages 18:1{18:9, New
York, NY, USA, 2010. ACM.
[23] T. Mende and R. Koschke. Revisiting the Evaluation of
Defect Prediction Models. In Proceedings of the 5th
International Conference on Predictor Models in
Software Engineering, PROMISE '09, pages 7:1{7:10,
New York, NY, USA, 2009. ACM.
[24] T. Mende and R. Koschke. Effort-aware defect
prediction models. In Proceedings of the 2010 14th
European Conference on Software Maintenance and
Reengineering, CSMR '10, pages 107{116, Washington,
DC, USA, 2010. IEEE Computer Society.
[25] T. Menzies, J. Greenwald, and A. Frank. Data Mining
Static Code Attributes to Learn Defect Predictors.
IEEE Transactions on Software Engineering,
33(1):2{13, Jan. 2007.
[26] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang,
and A. Bener. Defect prediction from static code
features: current results, limitations, new approaches.
Automated Software Engineering, 17(4):375{407, May
2010.
[27] N. Mittas and L. Angelis. Ranking and Clustering
Software Cost Estimation Models through a Multiple
Comparisons Algorithm. IEEE Transactions on
Software Engineering, 39(4):537{551, Apr. 2013.
[28] A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal, 5(2):169{180,
Apr. 2000.
[29] A. Monden, T. Hayashi, S. Shinoda, K. Shirai,
J. Yoshida, M. Barker, and K. Matsumoto. Assessing
the Cost Effectiveness of Fault Prediction in
Acceptance Testing. IEEE Transactions on Software
Engineering, 39(10):1345{1357, Oct. 2013.
[30] N. Nagappan and T. Ball. Use of relative code churn
measures to predict system defect density. In
Proceedings of the 27th International Conference on
Software Engineering, ICSE '05, pages 284{292, New
York, NY, USA, 2005. ACM.
[31] A. T. Nguyen, T. T. Nguyen, H. A. Nguyen, and T. N.
Nguyen. Multi-layered Approach for Recovering Links
Between Bug Reports and Fixes. In Proceedings of the
ACM SIGSOFT 20th International Symposium on the
Foundations of Software Engineering, FSE '12, pages
63:1{63:11, New York, NY, USA, 2012. ACM.
[32] F. Rahman and P. Devanbu. How, and why, process
metrics are better. In Proceedings of the 2013
International Conference on Software Engineering,
ICSE '13, pages 432{441, Piscataway, NJ, USA, 2013.
IEEE Press.
[33] F. Rahman, D. Posnett, and P. Devanbu. Recalling the
"imprecision" of cross-project defect prediction. In
Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software
Engineering, FSE '12, pages 61:1{61:11, New York, NY,
USA, 2012. ACM.
[34] F. Rahman, D. Posnett, A. Hindle, E. Barr, and
P. Devanbu. BugCache for Inspections: Hit or Miss? In
Proceedings of the 19th ACM SIGSOFT Symposium
and the 13th European Conference on Foundations of
Software Engineering, ESEC/FSE '11, pages 322{331,
New York, NY, USA, 2011. ACM.
[35] J. Romano, J. D. Kromrey, J. Coraggio, and
J. Skowronek. Appropriate statistics for ordinal level
data: Should we really be using t-test and cohen's d for
evaluating group differences on the nsse and other
surveys. In annual meeting of the Florida Association
of Institutional Research, pages 1{33, 2006.
[36] Y. Shin, A. Meneely, L. Williams, and J. Osborne.
Evaluating Complexity, Code Churn, and Developer
Activity Metrics as Indicators of Software
Vulnerabilities. IEEE Transactions on Software
Engineering, 37(6):772{787, Nov. 2011.
[37] J. Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce fixes? In Proceedings of the 2005
International Workshop on Mining Software
Repositories, MSR 2005, Saint Louis, Missouri, USA,
May 17, 2005. ACM, 2005.
[38] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung. ReLink:
Recovering Links Between Bugs and Changes. In
Proceedings of the 19th ACM SIGSOFT Symposium
and the 13th European Conference on Foundations of
Software Engineering, ESEC/FSE '11, pages 15{25,
New York, NY, USA, 2011. ACM.
[39] Y. Yang, Y. Zhou, H. Lu, L. Chen, Z. Chen, B. Xu,
H. Leung, and Z. Zhang. Are Slice-Based Cohesion
Metrics Actually Useful in Effort-Aware Post-Release
Fault-Proneness Prediction? An Empirical Study.
IEEE Transactions on Software Engineering,
41(4):331{357, Apr. 2015.
[40] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and
L. Bairavasundaram. How Do Fixes Become Bugs? In
Proceedings of the 19th ACM SIGSOFT Symposium
and the 13th European Conference on Foundations of
Software Engineering, ESEC/FSE '11, pages 26{36,
New York, NY, USA, 2011. ACM.
[41] S. Zhong, T. M. Khoshgoftaar, and N. Seliya.
Unsupervised learning for expert-based software quality
estimation. In Proceedings of the Eighth IEEE
International Conference on High Assurance Systems
Engineering, HASE'04, pages 149{155, Washington,
DC, USA, 2004. IEEE Computer Society.
[42] Y. Zhou, B. Xu, H. Leung, and L. Chen. An in-depth
study of the potentially confounding effect of class size
in fault prediction. ACM Trans. Softw. Eng. Methodol.,
23(1):10:1{10:51, Feb. 2014.
[43] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project Defect Prediction: A Large
Scale Experiment on Data vs. Domain vs. Process. In
Proceedings of the the 7th Joint Meeting of the
European Software Engineering Conference and the
ACM SIGSOFT Symposium on The Foundations of
Software Engineering, ESEC/FSE '09, pages 91{100,
New York, NY, USA, 2009. ACM.
[44] T. Zimmermann, R. Premraj, and A. Zeller. Predicting
defects for eclipse. In Proceedings of the Third
International Workshop on Predictor Models in
Software Engineering, PROMISE '07, pages 9{,
Washington, DC, USA, 2007. IEEE Computer Society.




>><N.>Effort-Aware Tri-Training for Semi-supervised Just-in-Time Defect Prediction
1. Angluin, D., Laird, P.D.: Learning from noisy examples. Mach. Learn. 2(4), 343–
370 (1987)
2. Arshad, A., Riaz, S., Jiao, L., Murthy, A.: Semi-supervised deep fuzzy c-mean
clustering for software fault prediction. IEEE Access 6, 25675–25685 (2018)
3. Blum, A., Mitchell, T.M.: Combining labeled and unlabeled data with co-training.
In: Proceedings of COLT, pp. 92–100 (1998)
4. Chapelle, O., Scholkopf, B., Zien, A.: Semi-supervised learning. IEEE Trans. Neural
Netw. 20(3), 542–542 (2006)
5. Chen, X., Zhao, Y., Wang, Q., Yuan, Z.: MULTI: multi-objective effort-aware justin-
time software defect prediction. Inf. Softw. Tech. 93, 1–13 (2018)
6. Fu, W., Menzies, T.: Revisiting unsupervised learning for defect prediction. In:
ESEC/FSE, pp. 72–83 (2017)
7. Hata, H., Mizuno, O., Kikuno, T.: Bug prediction based on fine-grained module
histories. In: ICSE, pp. 200–210 (2012)
8. Huang, Q., Xia, X., Lo, D.: Supervised vs unsupervised models: a holistic look at
effort-aware just-in-time defect prediction. In: ICSME, pp. 159–170 (2017)
9. Romano, J., Kromrey, J.D., Coraggio, J., Skowronek, J., Devine, L.: Exploring
methods for evaluating group differences on the NSSE and other surveys: are the
t-test and Cohen’s d indices the most appropriate choices. In: Annual Meeting of
the Southern Association for Institutional Research (2006)
10. Jiang, Y., Li, M., Zhou, Z.: Software defect detection with rocus. J. Comput. Sci.
Technol. 26(2), 328–342 (2011)
11. Kamei, Y., Fukushima, T., McIntosh, S., Yamashita, K., Ubayashi, N., Hassan,
A.E.: Studying just-in-time defect prediction using cross-project models. Empir.
Softw. Eng. 21(5), 2072–2106 (2016)
12. Kamei, Y., et al.: A large-scale empirical study of just-in-time quality assurance.
IEEE Trans. Softw. Eng. 39(6), 757–773 (2013)
13. Li, M., Zhang, H., Wu, R., Zhou, Z.: Sample-based software defect prediction with
active and semi-supervised learning. Autom. Softw. Eng. 19(2), 201–230 (2012)
14. Li, W., Huang, Z., Li, Q.: Three-way decisions based software defect prediction.
Knowl.-Based Syst. 91, 263–274 (2016)
15. Li, Z., Jing, X., Zhu, X.: Progress on approaches to software defect prediction. IET
Softw. 12(3), 161–175 (2018)
16. Liu, J., Zhou, Y., Yang, Y., Lu, H., Xu, B.: Code churn: a neglected metric in
effort-aware just-in-time defect prediction. In: ESEM, pp. 11–19 (2017)
17. Lu, H., Cukic, B., Culp, M.V.: An iterative semi-supervised approach to software
fault prediction. In: PROMISE, pp. 15:1–15:10 (2011)
18. Lu, H., Cukic, B., Culp, M.V.: Software defect prediction using semi-supervised
learning with dimension reduction. In: ASE, pp. 314–317 (2012)
19. Mockus, A., Weiss, D.M.: Predicting risk of software changes. Bell Labs Tech. J.
5(2), 169–180 (2000)
20. Song, Q., Jia, Z., Shepperd, M.J., Ying, S., Liu, J.: A general software defectproneness
prediction framework. IEEE Trans. Softw. Eng. 37(3), 356–370 (2011)
21. Yang, X., Lo, D., Xia, X., Sun, J.: TLEL: a two-layer ensemble learning approach
for just-in-time defect prediction. Inf. Softw. Tech. 87, 206–220 (2017)
22. Yang, X., Lo, D., Xia, X., Zhang, Y., Sun, J.: Deep learning for just-in-time defect
prediction. In: QRS, pp. 17–26 (2015)
23. Yang, Y., et al.: Effort-aware just-in-time defect prediction: simple unsupervised
models could be better than supervised models. In: FSE, pp. 157–168 (2016)
24. Zhang, Z., Jing, X., Wang, T.: Label propagation based semi-supervised learning
for software defect prediction. Autom. Softw. Eng. 24(1), 47–69 (2017)
25. Zhou, Z., Li, M.: Tri-training: exploiting unlabeled data using three classifiers.
IEEE Trans. Knowl. Data Eng. 17(11), 1529–1541 (2005)
26. Zhou, Z., Li, M.: Semi-supervised learning by disagreement. Knowl. Inf. Syst.
24(3), 415–439 (2010)
27. Zhu, X.: Semi-supervised learning. In: Encyclopedia of Machine Learning and Data
Mining, pp. 1142–1147 (2017)





>><[N]>How Well Just-In-Time Defect Prediction Techniques Enhance Software Reliability?
[1] X. Li, Y. F. Li, M. Xie, and S. H. Ng, “Reliability analysis and optimal
version-updating for open source software,” Information and Software
Technology, vol. 53, no. 9, pp. 929–936, 2011.
[2] Y. Tian, J. Tian, and N. Li, “Reliability assessment and prediction with
testing efficiency growth for open source software,” in International
Conference on Software Engineering and Data Engineering, SEDE
2017, San Diego, CA, October 2-4, 2017. ACM, 2017, pp. 72–83.
[3] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[4] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: simple unsupervised models
could be better than supervised models,” in ACM SIGSOFT International
Symposium on Foundations of Software Engineering, FSE 2016, Seattle,
WA, USA, November 13-18, 2016, T. Zimmermann, J. Cleland-Huang,
and Z. Su, Eds. ACM, 2016, pp. 157–168.
[5] W. Fu and T. Menzies, “Revisiting unsupervised learning for defect
prediction,” in ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2017, Paderborn, Germany, September 4-
8, 2017, E. Bodden, W. Sch¨afer, A. van Deursen, and A. Zisman, Eds.
ACM, 2017, pp. 72–83.
[6] X. Chen, Y. Zhao, Q. Wang, and Z. Yuan, “MULTI: multi-objective
effort-aware just-in-time software defect prediction,” Information and
Software Technology, vol. 93, pp. 1–13, 2018.
[7] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Software
Engineering, vol. 24, no. 5, pp. 2823–2862, 2019.
[8] J. Tian, “Reliability measurement, analysis, and improvement for large
software systems,” Advances in Computers, vol. 46, pp. 159–235, 1998.
[9] M. R. Lyu, editor, Handbook of software reliability engineering.
McGraw-Hill, New York, 1995.
[10] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[11] S. Kim, E. J. W. Jr., and Y. Zhang, “Classifying software changes: Clean
or buggy?” IEEE Transactions on Software Engineering, vol. 34, no. 2,
pp. 181–196, 2008.
[12] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and L. N. Bairavasundaram,
“How do fixes become bugs?” in ACM SIGSOFT International Symposium
on Foundations of Software Engineering, FSE 2011, Szeged,
Hungary, September 5-9, 2011, T. Gyim´othy and A. Zeller, Eds. ACM,
2011, pp. 26–36.
[13] J. Liu, Y. Zhou, Y. Yang, H. Lu, and B. Xu, “Code churn: A neglected
metric in effort-aware just-in-time defect prediction,” in ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement,
ESEM 2017, Toronto, ON, Canada, November 9-10, 2017,
A. Bener, B. Turhan, and S. Biffl, Eds. IEEE Computer Society, 2017,
pp. 11–19.
[14] G. Koru, H. Liu, D. Zhang, and K. E. Emam, “Testing the theory
of relative defect proneness for closed-source software,” Empirical
Software Engineering, vol. 15, no. 6, pp. 577–598, 2010.
[15] Q. Song, Y. Guo, and M. Shepperd, “A comprehensive investigation of
the role of imbalanced learning for software defect prediction,” IEEE
Transactions on Software Engineering, vol. 45, no. 12, pp. 1253–1269,
2018.
[16] M. Borg, O. Svensson, K. Berg, and D. Hansson, “SZZ unleashed: an
open implementation of the SZZ algorithm - featuring example usage in
a study of just-in-time bug prediction for the jenkins project,” in ACM
SIGSOFT International Workshop on Machine Learning Techniques for
Software Quality Evaluation, MaLTeSQuE@FSE/ESEC 2019, Tallinn,
Estonia, August 27, 2019. ACM, 2019, pp. 7–12.
[17] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Predicting the location
and number of faults in large software systems,” IEEE Transactions on
Software Engineering, vol. 31, no. 4, pp. 340–355, 2005.
[18] Y. Tian, J. Tian, and N. Li, “Cloud reliability and efficiency improvement
via failure risk based proactive actions,” Journal of Systems and
Software, vol. 163, p. 110524, 2020.
[19] N. Li, M. Shepperd, and Y. Guo, “A systematic review of unsupervised
learning techniques for software defect prediction,” Information and
Software Technology, vol. 122, p. 106287, 2020.
[20] J. Nam and S. Kim, “CLAMI: defect prediction on unlabeled datasets
(T),” in IEEE/ACM International Conference on Automated Software
Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015, M. B.
Cohen, L. Grunske, and M. Whalen, Eds. IEEE Computer Society,
2015, pp. 452–463.
[21] J. Yang and H. Qian, “Defect prediction on unlabeled datasets by using
unsupervised clustering,” in IEEE International Conference on High
Performance Computing and Communications, HPCC 2016, Sydney,
Australia, December 12-14, 2016. IEEE, 2016, pp. 465–472.
[22] M. Yan, Y. Fang, D. Lo, X. Xia, and X. Zhang, “File-level defect
prediction: Unsupervised vs. supervised models,” in ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement,
ESEM 2017, Toronto, ON, Canada, November 9-10, 2017, A. Bener,
B. Turhan, and S. Biffl, Eds. IEEE Computer Society, 2017, pp. 344–
353.
[23] M. Yan, X. Zhang, C. Liu, L. Xu, M. Yang, and D. Yang, “Automated
change-prone class prediction on unlabeled dataset using unsupervised
method,” Information and Software Technology, vol. 92, pp. 1–16, 2017.



>><N.>Improving the Quality of Software by Quantifying the Code Change Metric and Predicting the Bugs
1. Arisholm, E., Briand, L.C.: Predicting fault prone components in a java legacy system. In:
Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering,
pp. 8–17. ACM (2006)
2. D’Ambros, M., Lanza, M., Robbes, R.: An extensive comparison of bug prediction approaches.
In: MSR 2010: Proceedings of the 7th International Working Conference on
Mining Software Repositories, pp. 31–41 (2010)
3. D’Ambros, M., Lanza, M., Robbes, R.: Evaluating defect prediction approaches: A
benchmark and an extensive comparison. Empirical Software Engineering 17(4-5), 537–
577 (2012)
4. Elish, K.O., Elish, M.O.: Predicting defect-prone software modules using support vector
machines. The Journal of Systems and Software 81, 649–660 (2008)
5. Fenton, N.E., Ohlsson, N.: Quantitative analysis of faults and failures in a complex software
system. IEEE Transactions on Software Engineering 26(8), 797–814 (2000)
6. Goel, A.L., Okumoto, K.: Time dependent error detection rate model for software reliability
and other performance measures. IEEE Transactions on Reliability R-28(3), 206–211
(1979)
7. Graves, T.L., Karr, A.F., Marron, J.S., Siy, H.P.: Predicting fault incidence using software
change history. IEEE Transactions on Software Engineering 26(7), 653–661 (2000)
8. Gyimóthy, T., Ferenc, R., Siket, I.: Empirical validation of object-oriented metrics on open
source software for fault prediction. IEEE Transactions on Software Engineering 31(10),
897–910 (2005)
9. Hassan, A.E.: Predicting Faults based on complexity of code change. In: The Proceedings
of 31st Intl. Conf. on Software Engineering, pp. 78–88 (2009)
10. Hassan, A.E., Holt, R.C.: Studying the chaos in code development. In: Proceedings of 10th
Working Conference on Reverse Engineering (November 2003)
11. Hassan, A.E., Holt, R.C.: The chaos of software development. In: Proceedings of the 6th
IEEE International Workshop on Principles of Software Evolution (September 2003)
12. Hassan, A.E., Holt, R.C.: The top ten list: Dynamic fault prediction. In: Proceedings of
ICSM 2005, pp. 263–272 (2005)
13. Herraiz, I., Gonzalez-Barahona, J.M., Robles, G.: Towards a theoretical model for software
growth. In: Proceedings of the 4th International Workshop on Mining Software Repositories,
Minnesotta, USA (2007)
14. Kapur, P.K., Garg, R.B., Kumar, S.: Contributions to Hardware and Software Reliability.
World Scientific Publishing Co. Ltd., Singapore (1999)
15. Kapur, P.K., Garg, R.B.: A software reliability growth model for an error removal phenomenon.
Software Engineering Journal 7, 291–294 (1992)
16. Kim, S., Zimmermann, T., Whitehead, J., Zeller, A.: Predicting faults from cached history.
In: Proceedings of ICSE 2007, pp. 489–498. IEEE CS (2007)
17. Khoshgoftaar, T.M., Allen, E.B., Jones, W.D., Hudepohl, J.P.: Data mining for predictors
of software quality. International Journal of Software Engineering and Knowledge Engineering
9(5), 547–563 (1999)
18. Leszak, M., Perry, D.E., Stoll, D.: Classification and evaluation of defects in a project retrospective.
The Journal of Systems and Software 61(3), 173–187 (2002)
19. Lyu, M.R.: Handbook of Software Reliability Engineering. McGraw-Hill (1996)
20. Musa, J.D., Iannino, A., Okumoto, K.: Software Reliability, Measurement, Prediction and
Application. McGraw-Hill (1987)
21. Nagappan, N., Ball, T.: Use of relative code churn measures to predict system defect density.
In: Proceedings of the 27th International Conference on Software Engineering, pp.
284–292 (2005)
22. Nagappan, N., Ball, T.: Static analysis tools as early indicators of pre-release defect density.
In: Proceedings of ICSE 2005, pp. 580–586. ACM (2005)
23. Nagappan, N., Ball, T., Zeller, A.: Mining metrics to predict component failures. In: Proceedings
of ICSE 2006, pp. 452–461 ACM (2006)
24. Ohba, M.: Inflection S-shaped software reliability growth model. In: Osaki, S., Hotoyama,
Y. (eds.) Stochastic Models in Reliability Theory. LNEMS, vol. 235, pp. 144–162. Springer,
Heidelberg (1984)
25. Ostrand, T.J., Weyuker, E.J., Bell, R.M.: Predicting the location and number of faults in
large complex systems. IEEE Transactions on Software Engineering 31(4), 340–355
(2005)
26. Shannon, C.E.: A Mathematical Theory of Communication. The Bell System Technical
Journal 27, 379–423, 623–656 (1948)
27. Singh, V.B.: A Study on Software Reliability Growth Modeling using Change Point and
Fault Dependency. University of Delhi (2008)
28. The bugZilla project (2012), http://www.bugzilla.org
29. The Mozilla project (2012), http://www.mozilla.org
30. Weisberg, S.: Applied Linear Regression. John Wiley and Sons (1980)
31. Yamada, S., Ohba, M., Osaki, S.: S-shaped software reliability growth modelling for software
error detection. IEEE Trans. on Reliability R-32 (5), 475–484 (1983)
32. Vapnik, V.: The Nature of Statistical Learning Theory. Springer, New York (1995)
33. Xing, F., Guo, P.: Support vector regression for software reliability growth modeling and
prediction. In: Wang, J., Liao, X.-F., Yi, Z. (eds.) ISNN 2005. LNCS, vol. 3496, pp. 925–930. Springer, Heidelberg (2005)
34. Singh, V.B., Chaturvedi, K.K.: Entropy based bug prediction using support vector regression.
In: Proceedings ISDA 2012 - 12th International Conference on Intelligent System
Design and Applications, Kochi, India, November 27-29, pp. 746–751. IEEE Xplore, USA
(2012)



>><[N]>JITBot: An Explainable Just-In-Time Defect Prediction Bot
[1] Hoa Khanh Dam, Truyen Tran, and Aditya Ghose. 2018. Explainable Software
Analytics. In Proceedings of the International Conference on Software Engineering:
New Ideas and Emerging Results (ICSE-NIER). 53–56.
[2] Ahmed E. Hassan. 2009. Predicting Faults using the Complexity of Code Changes.
In ICSE ’09. 78–88.
[3] Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Hoa Khanh Dam, and John
Grundy. 2020. An Empirical Study of Model-Agnostics Techniques for Defect
Prediction Models. IEEE Transactions on Software Engineering (TSE) (2020).
[4] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E. Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072–2106.
[5] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-Scale Empirical Study of
Just-In-Time Quality Assurance. IEEE Transactions on Software Engineering (TSE)
39, 6 (2013), 757–773.
[6] Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and
E James Whitehead Jr. 2013. Does Bug Prediction Support Human Developers?
Findings from a Google Case Study. In ICSE ’13. 372–381.
[7] Nachiappan Nagappan and Thomas Ball. 2005. Use of Relative Code Churn
Measures to Predict System Defect Density. In ICSE ’05. 284–292.
[8] Luca Pascarella, Fabio Palomba, and Alberto Bacchelli. 2019. Fine-grained just-intime
defect prediction. Journal of Systems and Software (JSS) 150 (2019), 22–36.
[9] Christoffer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit Guru: Analytics
and Risk Prediction of Software Commits. In FSE ’15. 966–969.
[10] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi Matsumoto.
2016. Automated Parameter Optimization of Classification Techniques
for Defect Prediction Models. In ICSE ’16. 321–332.
[11] Zhiyuan Wan, Xin Xia, Ahmed E Hassan, David Lo, Jianwei Yin, and Xiaohu
Yang. 2018. Perceptions, expectations, and challenges in defect prediction. IEEE
Transactions on Software Engineering (TSE) (2018).





>><[N]>Online Defect Prediction for Imbalanced Data
[1] S. Kim, E. Whitehead, and Y. Zhang, “Classifying software changes:
Clean or buggy?” TSE’08, vol. 34, no. 2, pp. 181–196.
[2] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in ICSE ’09, pp. 78–88.
[3] A. Meneely, L. Williams, W. Snipes, and J. Osborne, “Predicting failures
with developer networks and social network analysis,” in SIGSOFT
’08/FSE-16, pp. 13–23.
[4] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect prediction from static code features: Current results, limitations,
new approaches,” ASE’10, vol. 17, no. 4, pp. 375–407.
[5] T. Zimmermann and N. Nagappan, “Predicting defects using network
analysis on dependency graphs,” in ICSE ’08, pp. 531–540.
[6] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting defects for
eclipse,” in PROMISE ’07, pp. 9–9.
[7] N. Bettenburg, M. Nagappan, and A. E. Hassan, “Think locally, act
globally: Improving defect and effort prediction models,” in MSR’12,
2012, pp. 60–69.
[8] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting fault
incidence using software change history,” TSE’00, vol. 26, no. 7, pp.
653–661.
[9] I. Herraiz, J. Gonzalez-Barahona, G. Robles, and D. German, “On the
prediction of the evolution of libre software projects,” in ICSM’07, pp.
405–414.
[10] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in ICSE ’06, pp. 452–461.
[11] L. Cheung, R. Roshandel, N. Medvidovic, and L. Golubchik, “Early
prediction of software component reliability,” in ICSE ’08, pp. 111–120.
[12] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller, “Predicting
faults from cached history,” in ICSE’07, pp. 489–498.
[13] E. Giger, M. D’Ambros, M. Pinzger, and H. C. Gall, “Method-level bug
prediction,” in ESEM ’12, pp. 171–180.
[14] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in defect
prediction,” in ICSE’11, pp. 481–490.
[15] L. Prechelt and A. Pepper, “Why software repositories are not used for
defect-insertion circumstance analysis more often: A case study,” Inf.
Softw. Technol.(2014), vol. 56, no. 10, pp. 1377–1389.
[16] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J. Whitehead Jr.,
“Does bug prediction support human developers? Findings from a google
case study,” in ICSE ’13, pp. 372–381.
[17] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in
ASE’13, pp. 279–289.
[18] L. Pelayo and S. Dick, “Applying novel resampling strategies to software
defect prediction,” in NAFIPS ’07, pp. 69–72.
[19] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Predicting the location
and number of faults in large software systems,” TSE’05, vol. 31, no. 4,
pp. 340–355.
[20] J. Eyolfson, L. Tan, and P. Lam, “Correlations between bugginess and
time-based commit characteristics,” EMSE’14, vol. 19, no. 4, pp. 1009–
1039.
[21] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” in MSR ’05, pp. 1–5.
[22] K. Herzig, S. Just, and A. Zeller, “It’s not a bug, it’s a feature: How
misclassification impacts bug prediction,” in ICSE ’13, pp. 392–401.
[23] M. F. Porter, “Snowball: A language for stemming algorithms,” 2001.
[Online]. Available: http://snowball.tartarus.org/texts/introduction.html
[24] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten, “The WEKA data mining software: An update,” SIGKDD’09,
vol. 11, no. 1, pp. 10–18.
[25] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “DECKARD: Scalable
and accurate tree-based detection of code clones,” in ICSE’07, pp. 96–
105.
[26] Y. Kamei, E. Shihab, B. Adams, A. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” TSE’13, vol. 39, no. 6, pp. 757–773.
[27] J. Eyolfson, L. Tan, and P. Lam, “Do time of day and developer
experience affect commit bugginess?” in MSR ’11, pp. 153–162.
[28] S. Kim and E. J. Whitehead, Jr., “How long did it take to fix bugs?” in
MSR ’06, pp. 173–174.
[29] T.-H. Chen, M. Nagappan, E. Shihab, and A. E. Hassan, “An empirical
study of dormant bugs,” in MSR’14, pp. 82–91.
[30] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
“SMOTE: Synthetic minority over-sampling technique,” J. Artif. Int.
Res.(2002), vol. 16, no. 1, pp. 321–357.
[31] G. H. John and P. Langley, “Estimating continuous distributions in
Bayesian classifiers,” in UAI’95, pp. 338–345.
[32] D. W. Aha, D. Kibler, and M. K. Albert, “Instance-based learning
algorithms,” Mach. Learn.(1991), vol. 6, no. 1, pp. 37–66.
[33] J. G. Cleary and L. E. Trigg, “K*: An instance-based learner using an
entropic distance measure,” in ICML’95, pp. 108–114.
[34] J. Friedman, T. Hastie, and R. Tibshirani, “Additive logistic regression:
a statistical view of boosting,” pp. 337–407.
[35] C. G. Atkeson, A. W. Moore, and S. Schaal, “Locally weighted learning,”
Artif. Intell. Rev.(1997), vol. 11, no. 1-5, pp. 11–73.
[36] B. Martin, “Instance-based learning: Nearest neighbor with generalization,”
Master’s thesis, University of Waikato, 1995.
[37] S. Shalev-Shwartz, Y. Singer, and N. Srebro, “Pegasos: Primal estimated
sub-gradient solver for svm,” in ICML ’07, pp. 807–814.
[38] S. Wang and X. Yao, “Using class imbalance learning for software defect
prediction,” R’13, vol. 62, no. 2, pp. 434–443.
[39] Y. Freund and L. Mason, “The alternating decision tree learning algorithm,”
in ICML ’99, pp. 124–133.
[40] M. Shepperd, D. Bowes, and T. Hall, “Researcher bias: The use of
machine learning in software defect prediction,” SE’14, vol. 40, no. 6,
pp. 603–616.
[41] G. C¸ alikli and A. Bener, “Influence of confirmation biases of developers
on software quality: an empirical study,” Software Quality Journal
(2013), vol. 21, no. 2, pp. 377–416.
[42] T. J. Ostrand and E. J. Weyuker, “An industrial research program in
software fault prediction,” in SE’07, pp. 21–28.
[43] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in FSE’12, pp. 62:1–62:11.
[44] R. M. Bell, E. J. Weyuker, and T. J. Ostrand, “Assessing the impact of
using fault prediction in industry,” in ICSTW’11, pp. 561–565.
[45] N. Littlestone, “Learning quickly when irrelevant attributes abound: A
new linear-threshold algorithm,” Mach. Learn.(1988), vol. 2, no. 4, pp.
285–318.
[46] Z. Sun, Q. Song, and X. Zhu, “Using coding-based ensemble learning to
improve software defect prediction,” SMC’12, vol. 42, no. 6, pp. 1806–
1817.
[47] J. Wang, B. Shen, and Y. Chen, “Compressed c4.5 models for software
defect prediction,” in QSIC’12, pp. 13–16.
[48] J. Zheng, “Cost-sensitive boosting neural networks for software defect
prediction,” Expert Systems with Applications (2000), vol. 37, no. 6, pp.
4537–4543.
[49] X. Y. Jing, S. Ying, Z. W. Zhang, S. S. Wu, and J. Liu, “Dictionary
learning based software defect prediction,” in ICSE’14, pp. 414–423.
[50] R. Vivanco, Y. Kamei, A. Monden, K. Matsumoto, and D. Jin, “Using
search-based metric selection and oversampling to predict fault prone
modules,” in CCECE’10, pp. 1–6.
[51] N. Chawla, A. Lazarevic, L. Hall, and K. Bowyer, “SMOTEBoost:
Improving prediction of the minority class in boosting,” in PKDD’03,
vol. 2838, pp. 107–119.
[52] S. Wang, H. Chen, and X. Yao, “Negative correlation learning for
classification ensembles,” in IJCNN’10, pp. 1–8.
[53] D. Gray, D. Bowes, N. Davey, Y. Sun, and B. Christianson, “The misuse
of the NASA metrics data program data sets for automated software
defect prediction,” in EASE’11, pp. 96–103.



>><[N]>Personalized Defect Prediction
[1] S. Kim, J. E. James Whitehead, and Y. Zhang, “Classifying software
changes: Clean or buggy?” TSE, vol. 34, pp. 181–196, 2008.
[2] A. Hassan, “Predicting faults using the complexity of code changes,” in
ICSE, 2009, pp. 78–88.
[3] A. Meneely, L. Williams, W. Snipes, and J. Osborne, “Predicting failures
with developer networks and social network analysis,” in FSE, 2008, pp.
13–23.
[4] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. B. Bener,
“Defect prediction from static code features: Current results, limitations,
new approaches,” ASE, 2010.
[5] T. Zimmermann and N. Nagappan, “Predicting defects using network
analysis on dependency graphs,” in ICSE, 2008.
[6] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting Defects for
Eclipse,” in PROMISE, 2007.
[7] N. Bettenburg, M. Nagappan, and A. E. Hassan, “Think locally, act
globally: Improving defect and effort prediction models,” in MSR, 2012,
pp. 60–69.
[8] T. Graves, A. Karr, J. Marron, and H. Siy, “Predicting fault incidence
using software change history,” TSE, vol. 26, no. 7, pp. 653–661, 2000.
[9] I. Herraiz, J. Gonz´alez-Barahona, G. Robles, and D. Germ´an, “On the
prediction of the evolution of libre software projects,” in ICSM, 2007,
pp. 405–414.
[10] A. Mockus, D. Weiss, and P. Zhang, “Understanding and predicting
effort in software projects,” in ICSE, 2003.
[11] T. J. Ostrand, E. J. Weyuker, R. M. Bell, P. Avenue, and F. Park,
“Programmer-based fault prediction,” in PROMISE, 2010, pp. 1–10.
[12] Google Official Blog, “Personalized search for everyone,”
http://googleblog.blogspot.ca/2009/12/personalized-search-for-everyone
.html, 2009.
[13] Y. Freund and L. Mason, “The alternating decision tree learning algorithm,”
in ICML, 1999, pp. 124–133.
[14] J. H. Friedman, “Multivariate adaptive regression splines,” The Annals
of Statistics, vol. 19, no. 1, pp. 1–67, 1991.
[15] F. Rahman, D. Posnett, and P. Devanbu, “Recalling the “imprecision”
of cross-project defect prediction,” in FSE, 2012.
[16] F. Rahman and P. Devanbu, “How, and Why, process metrics are better,”
in ICSE, 2013.
[17] E. Arisholm, L. C. Briand, and M. Fuglerud, “Data mining techniques
for building fault-proneness models in telecom java software,” 2007, pp.
215–224.
[18] J. Eyolfson, L. Tan, and P. Lam, “Correlations between bugginess and
time-based commit characteristics,” EMSE, pp. 1–31, 2013.
[19] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do Changes Induce
Fixes?” in MSR, 2005, pp. 24–28.
[20] K. Herzig, S. Just, and A. Zeller, “It’s not a bug, it’s a feature: How
misclassification impacts bug prediction,” in ICSE, 2013.
[21] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “DECKARD: Scalable
and accurate tree-based detection of code clones,” in ICSE, 2007, pp.
96–105.
[22] B. Raskutti, H. Ferra, and A. Kowalczyk, “Second-order features for
maximizing text classification performance,” ECML, pp. 419–430, 2001.
[23] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten, “The WEKA data mining software: An update,” SIGKDD
Explor. Newsl., vol. 11, no. 1, pp. 10–18, 2009.
[24] “Snowball,” http://snowball.tartarus.org/.
[25] Z. Li, L. Tan, X. Wang, S. Lu, Y. Zhou, and C. Zhai, “Have things
changed now? An empirical study of bug characteristics in modern open
source software,” in ASID, October 2006.
[26] G. Bougie, C. Treude, D. Germ´an, and M. Storey, “A comparative
exploration of FreeBSD bug lifetimes,” in MSR, 2010, pp. 106–109.
[27] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-project defect prediction: A large scale experiment on data vs.
domain vs. process,” in FSE, 2009, pp. 91–100.
[28] G. H. John and P. Langley, “Estimating continuous distributions in
bayesian classifiers,” in UAI. San Mateo: Morgan Kaufmann, 1995,
pp. 338–345.
[29] N. Landwehr, M. Hall, and E. Frank, “Logistic model trees,” vol. 95,
no. 1-2, pp. 161–205, 2005.
[30] T. Curk, J. Demar, Q. Xu, G. Leban, U. Petrovi, I. Bratko, G. Shaulsky,
and B. Zupan, “Microarray data mining with visual programming,”
Bioinformatics, vol. 21, pp. 396–398, Feb. 2005.
[31] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf, “Bugs as
deviant behavior: A general approach to inferring errors in systems
code,” in SOSP, 2001, pp. 57–72.
[32] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical
study of operating systems errors,” in SOSP, 2001, pp. 73–88.
[33] S. Kim and E. J. Whitehead, Jr., “How long did it take to fix bugs?” in
MSR, 2006, pp. 173–174.
[34] A. Hassan and R. Holt, “The top ten list: Dynamic fault prediction,” in
ICSM, 2005, pp. 263–272.
[35] A. W. Moore, “Cross-validation,” http://www.autonlab.org/tutorials/
overfit.html, 2008.
[36] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in defect
prediction,” in ICSE, New York, NY, USA, 2011, pp. 481–490.
[37] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Predicting the location
and number of faults in large software systems,” TSE, vol. 31, no. 4,
pp. 340–355, Apr. 2005.
[38] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J. Whitehead Jr,
“Does bug prediction support human developers? findings from a google
case study,” in ICSE, 2013, pp. 372–381.
[39] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein, V. Filkov, and
P. Devanbu, “Fair and balanced?: Bias in bug-fix datasets,” in FSE, 2009,
pp. 121–130.
[40] R. Wu, H. Zhang, S. Kim, and S. Cheung, “Relink: Recovering links
between bugs and changes,” in FSE, 2011, pp. 15–25.
[41] S. Kim, T. Zimmermann, K. Pan, and J. J. Whitehead, “Automatic
identification of bug-introducing changes,” in ASE, 2006, pp. 81–90.
[42] PostgreSQL Community, “Committing with Git – PostgreSQL Wiki,”
http://wiki.postgresql.org/wiki/Committing with Git, 2012.
[43] J. Anvik, L. Hiew, and G. C. Murphy, “Who should fix this bug?” in
ICSE, 2006, pp. 361–370.
[44] D. Matter, A. Kuhn, and O. Nierstrasz, “Assigning bug reports using
a vocabulary-based expertise model of developers,” in MSR, 2009, pp.
131–140.
[45] J. Park, M. Lee, J. Kim, S. Hwang, and S. Kim, “CosTriage: A costaware
triage algorithm for bug reporting systems,” in AAAI, 2011.
[46] G. Jeong, S. Kim, and T. Zimmermann, “Improving bug triage with bug
tossing graphs,” in FSE, 2009, pp. 111–120.
[47] M. Lumpe, R. Vasa, T. Menzies, R. Rush, and B. Turhan, “Learning
better inspection optimization policies,” IJSEKE, vol. 22, no. 5, pp. 621–
644, 2012.
[48] S. Shivaji, E. J. W. Jr., R. Akella, and S. Kim, “Reducing features to
improve code change-based bug prediction,” TSE, vol. 39, no. 4, pp.
552–569, 2013.
[49] C. Tucker, “Social Networks, Personalized Advertising, and Privacy
Controls,” in WEIS, 2011.
[50] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and M. Nakamura,
“An analysis of developer metrics for fault prediction,” in
PROMISE, 2010, pp. 18:1–18:9.
[51] F. Rahman and P. Devanbu, “Ownership, experience and defects: A finegrained
study of authorship,” in ICSE, 2011, pp. 491–500.
[52] D. Posnett, R. DSouza, P. Devanbu, and V. Filkov, “Dual ecological
measures of focus in software development,” in ICSE, 2013.
[53] A. Hindle, E. Barr, Z. Su, M. Gabel, and P. Devanbu, “On the naturalness
of software,” in ICSE, 2012, pp. 837–847.
[54] K. Muslu, Y. Brun, R. Holmes, M. D. Ernst, and D. Notkin, “Improving
ide recommendations by considering global implications of existing
recommendations,” in ICSE, 2012, pp. 1349–1352.
[55] Y. Ye and G. Fischer, “Supporting reuse by delivering task-relevant and
personalized information,” in ICSE, 2002, pp. 513–523.
[56] D. Kim, X. Wang, S. Kim, A. Zeller, S. Cheung, and S. Park, “Which
crashes should i fix first?: Predicting top crashes at an early stage to
prioritize debugging efforts,” TSE, vol. 37, no. 3, pp. 430–447, 2011.
[57] S. S. Kolesnikov, S. Apel, N. Siegmund, S. Sobernig, C. K¨astner, and
S. Senkaya, “Predicting quality attributes of software product lines using
software and network measures and sampling,” in VaMoS, 2013, p. 6.
[58] Y. Tian, J. Lawall, and D. Lo, “Identifying Linux bug fixing patches,”
in ICSE, 2012, pp. 386–396.
[59] Y. Shin and L. Williams, “Can traditional fault prediction models be
used for vulnerability prediction?” EMSE, vol. 18, no. 1, pp. 25–59,
2013.







>><N.>Predicting Risk of Software Changes
1. K. E. Martersteck and A. E. Spencer, Jr., " The 5ESS™ Switching System: Introduction," AT&T Tech. J., Vol. 64, No. 6, Part 2, July-Aug. 1985, pp.1305-1314.
2. K. H. An, D. A. Gustafson, and A. C. Melton, "A model for software maintenance, " Proc. of the Conf. on Soft. Maintenance, Austin, Texas, Sept. 21-24, 1987, IEEE Comp. Soc. Press, pp.57-62.
3. V. R. Basili and B. T. Perricone, "Software Errors and Complexity: An Empirical Investigation," Commun. of the ACM, Vol. 27, No. 1, Jan. 1984, pp. 42-52.
4. L. Hatton, "Reexamining the Fault Density-Component Size Connection," IEEE Soft., Vol. 14, No. 2, Mar./Apr. 1997, pp. 89-97.
5. T. J. McCabe, "Complexity Measure," IEEE Trans. on Soft. Eng., Vol. SE-2, No. 4, July 1976, pp. 308-320.
6. M. H. Halstead, Elements of Software Science, Elsevier North-Holland, New York, 1977.
7. N. F. Schneidewind and H.-M. Hoffman, "An Experiment in Software Error Data Collection and Analysis," IEEE Trans. on Soft. Eng., Vol. SE-5, No. 3, May 1979, pp. 276-286.
8. N. Ohlsson and H. Alberg, "Predicting Fault-Prone Software Modules in Telephone Switches," IEEE Trans. on Soft. Eng., Vol. 22, No. 12, Dec. 1996, pp. 886-894.
9. V. Y. Shen, T.-J. Yu, S. M. Thebaut, and
L.
R. Paulsen, "Identifying Error-Prone Soft-ware—An Empirical Study," IEEE Trans. on Soft. Eng., Vol. SE-11, No. 4, Apr. 1985, pp. 317-325.
10. J. C. Munson and T. M. Khoshgoftaar, "Regression modeling of software quality:
Empirical investigation," Information and Soft. Tech., Vol. 32, No. 2, Feb. 1990, pp. 106-114.
11. T.-J. Yu, V. Y. Shen, and H. E. Dunsmore,
"An Analysis of Several Software Defect Models," IEEE Trans. on Soft. Eng., Vol. 14, No. 9, Sept. 1988, pp. 1261-1270.
12. T. L. Graves, A. F. Karr, J. S. Marron, and
H. Siy, "Predicting Fault Incidence Using Soft-ware Change History," IEEE Trans. on Soft. Eng., Vol. 26, No. 2 (forthcoming 2000).
13. J. D. Musa, A. Iannino, and K. Okumoto, Software Reliability: Measurement, Prediction, and Application, McGraw-Hill, New York, 1990.
14. J. Jelinski and P. B. Moranda, "Software relia-bility research," Probabilistic Models for Software, edited by W. Freiberger, Academic Press, New York, 1972, pp. 485-502.
15. G. J. Schick and R. W. Wolverton, "An Analysis of Competing Software Reliability Models," IEEE Trans. on Soft. Eng., Vol. SE-4, No. 2,
Mar. 1978, pp. 104-120.
16. S. N. Mohanty, "Models and Measurements for Quality Assessment of Software," ACM Computing Surveys, Vol. 11, No. 3, Sept. 1979, pp. 257-275.
17. S. G. Eick, C. R. Loader, M. D. Long, L. G. Votta, and S. Vander Wiel, "Estimating software fault content before coding," Proc. of the 14th Intl. Conf. on Soft. Eng., Melbourne, Australia, May 11-15, 1992, pp.59-65.
18. D. A. Christenson and S. T. Huang, "Estimating the Fault Content of Software Using the Fix-on-Fix Model," Bell Labs Tech. J, Vol. 1, No. 1, Summer 1996, pp. 130-137.
19. A. L. Goel, "Software Reliability Models: Assumptions, Limitations, and Applicability," IEEE Trans. on Soft. Eng., Vol. SE-11, No. 12, Dec. 1985, pp. 1411-1423.
20. P. B. Moranda, "Software quality technology," IEEE Comp., Vol. 11, No. 11, Nov. 1978,
pp.72-78.
21. A. K. Midha, "Software Configuration Manage-ment for the 21st Century," Bell Labs Tech. J., Vol. 2, No. 1, Winter 1997, pp. 154-165.
22. M. J. Rochkind, "The Source Code Control System," IEEE Trans. on Soft. Eng., Vol. SE-1, No. 4, Dec. 1975, pp. 364-370.
23. S. R. Chidamber and C. F. Kemerer, "A Metrics Suite for Object Oriented Design," IEEE Trans. on Soit:. Eng, Vol. 20, No. 6, June 1994, pp. 476-493.
24. A. J. Albrecht and J. E. Gaffney, Jr., "Soft-ware Function, Source Lines of Code, and Development Effort Prediction: A Software Science Validation," IEEE Trans. on Soft. Eng., Vol. SE-9, No. 6, Nov. 1983, pp. 639-648.
25. L. A. Belady and M. M. Lehman, "A model of large program development," IBMSys. J.,
Vol. 15, No. 3, 1976, pp. 225-253.
26. V. R. Basili and D. M. Weiss, "Evaluating Soft-ware Development by Analysis of Changes, " IEEE Trans. on Soft. Eng., Vol. SE-11, No. 2, Feb. 1985, pp. 157-168.
27. A. Mockus, S. G. Eick, T. L. Graves, and A. F. Karr,
"On Measurement and Analysis of Software Changes," Doc. No. ITD-99-36760F, BL0113590-990401-06TM, Lucent Techno-logies, Murray Hill, N. J., Apr. 1999.
28. P. McCullagh and J. A. Nelder, Generalized Linear Models, 2nd ed., Chapman and Hall, New York, 1989.
29. J. M. Chambers and T. J. Hastie, eds., Statistical Models in S, Wadsworth & Brooks, Pacific Grove, Calif., 1992.
30. A. J. Miller, Subset Selection in Regression, Chapman and Hall, London, 1990.
31. C. L. Mallows, "Some Comments on Cp," Technometrics, Vol. 15, No. 4, Nov. 1973, pp.661-667.
32. S. G. Eick, T. L. Graves, A. F. Karr, J. S. Marron, and A. Mockus, "Does Code Decay? Assessing the Evidence from Change Management Data," IEEE Trans. on Soft. Eng. (forthcoming 2000).
33. A. Mockus and L. G. Votta, "Identifying reasons for software changes using historic databases," Proc. Intl. Conf. on Software Maintenance, San Jose, Calif., Oct. 11-14, 2000.
34. D. Atkins, T. Ball, T. Graves, and A. Mockus, "Using version control data to evaluate the impact of software tools," Proc. 21st Intl. Conf. on Soft. Eng., Los Angeles, Calif., May 16-22, 1999, ACM Press, pp. 324-333.
35. H. Siy and A. Mockus, "Measuring domain engineering effects on software change cost," Metrics '99: Sixth Intl. Symp. on Soft. Metrics, Boca Raton, Fla., Nov. 4-6, 1999, pp. 304-311.



>><A(Y)>Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction
Abdi H (2007) Bonferroni and ˇsid´ak corrections for multiple comparisons. Enc Measur Stat 3:103–107
Agrawal A, Menzies T (2018) Is better data better than better data miners?: on the benefits of tuning smote for defect prediction. In: Proceedings of the 40th International Conference on Software Engineering, ACM, pp 1050–1061
Arisholm E, Briand LC, Fuglerud M (2007) Data mining techniques for building fault-proneness models in telecom java software. In: The 18th IEEE International Symposium on Software Reliability (ISSRE’07), IEEE, pp 215–224
Arisholm E, Briand LC, Johannessen EB (2010) A systematic and comprehensive investigation of methods to build and evaluate fault prediction models. J Syst Softw 83(1):2–17
Cliff N (1996) Ordinal methods for behavioral data analysis. Lawrence Erlbaum Associates
da Costa DA, McIntosh S, Shang W, Kulesza U, Coelho R, Hassan AE (2017) A framework for evaluating the results of the szz approach for identifying bug-introducing changes. IEEE Trans Softw Eng 43(7):641–657
D’Ambros M, Lanza M, Robbes R (2010) An extensive comparison of bug prediction approaches. In: 2010 7th IEEE working conference on mining software repositories (MSR), IEEE, pp 31–41
Fu W, Menzies T (2017) Revisiting unsupervised learning for defect prediction. In: Proceedings of the 2017 25th ACM SIGSOFT International Symposium on Foundations of Software Engineering, ACM, p to appear
Ghotra B, McIntosh S, Hassan AE (2015) Revisiting the impact of classification techniques on the performance of defect prediction models. In: Proceedings of the 37th international conference on software engineering-volume 1, IEEE Press, pp 789–800 
Graves TL, Karr AF, Marron JS, Siy H (2000) Predicting fault incidence using software change history. IEEE Trans Softw Eng 26(7):653–661
Guo PJ, Zimmermann T, Nagappan N, Murphy B (2010) Characterizing and predicting which bugs get fixed: an empirical study of microsoft windows. In: 2010 ACM/IEEE 32nd international conference on software engineering, IEEE, vol 1, pp 495–504
Gyimothy T, Ferenc R, Siket I (2005) Empirical validation of object-orientedmetrics on open source software for fault prediction. IEEE Trans Softw Eng 31(10):897–910
Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH (2009) The weka data mining software: an update. ACM SIGKDD explorations newsletter 11(1):10–18
Hall T, Beecham S, Bowes D, Gray D, Counsell S (2012) A systematic literature review on fault prediction performance in software engineering. IEEE Trans Softw Eng 38(6):1276–1304
Hamill M, Goseva-Popstojanova K (2009) Common trends in software fault and failure data. IEEE Trans Softw Eng 35(4):484–496
Han J, Pei J, Kamber M (2011) Data mining: concepts and techniques. Elsevier, Amsterdam
Hassan AE (2009) Predicting faults using the complexity of code changes. In: Proceedings of the 31st international conference on software engineering, IEEE computer society, pp 78–88
Hintze JL, Nelson RD (1998) Violin plots: a box plot-density trace synergism. The American Statistician 52(2):181–184
Huang Q, Xia X, Lo D (2017) Supervised vs unsupervised models: a holistic look at effort-aware just-in-time defect prediction. In: IEEE International Conference on Software maintenance and evolution (ICSME), IEEE
Huang Q, Shihab E, Xia X, Lo D, Li S (2018) Identifying self-admitted technical debt in open source projects using text mining. Empir Softw Eng 23(1):418–451
Jiang T, Tan L, Kim S (2013) Personalized defect prediction. In: 2013 IEEE/ACM 28th International conference on automated software engineering (ASE), IEEE, pp 279–289
Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A large-scale empirical study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773
Kim S, Zimmermann T, Pan K, James E Jr et al (2006) Automatic identification of bug-introducing changes.  In: null, IEEE, pp 81–90
Kim S, Whitehead EJ Jr, Zhang Y (2008) Classifying software changes: Clean or buggy? IEEE Trans Softw Eng 34(2):181–196
Kochhar PS, Xia X, Lo D, Li S (2016) Practitioners’ expectations on automated fault localization. In: Proceedings of the 25th International Symposium on Software Testing and Analysis, ACM, pp 165– 176
Koru AG, Zhang D, El Emam K, Liu H (2009) An investigation into the functional form of the size-defect relationship for software modules. IEEE Trans Softw Eng 35(2):293–304
Koru G, Liu H, Zhang D, El Emam K (2010) Testing the theory of relative defect proneness for closed-source software. Empir Softw Eng 15(6):577–598
Li PL, Herbsleb J, Shaw M, Robinson B (2006) Experiences and results from initiating field defect prediction and product test prioritization efforts at abb inc. In: Proceedings of the 28th international conference on Software engineering, ACM, pp 413–422
Matsumoto S, Kamei Y, Monden A, Matsumoto K, NakamuraM(2010) An analysis of developer metrics for fault prediction. In: Proceedings of the 6th International Conference on Predictive Models in Software Engineering, ACM, p 18
Mende T, Koschke R (2010) Effort-aware defect prediction models. In: 2010 14th European conference on software maintenance and reengineering (CSMR), IEEE, pp 107–116
Menzies T, Di Stefano JS (2004) How good is your blind spot sampling policy. In: Proceedings 8th IEEE International symposium on high assurance systems engineering, 2004, IEEE, pp 129–138
Menzies T, Milton Z, Turhan B, Cukic B, Jiang Y, Bener A (2010) Defect prediction from static code features: current results, limitations, new approaches. Autom Softw Eng 17(4):375–407
Meyer AN, Fritz T, Murphy GC, Zimmermann T (2014) Software developers’ perceptions of productivity.  In: Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, ACM, pp 19–29
Mockus A, Weiss DM (2000) Predicting risk of software changes. Bell Labs Tech J 5(2):169–180
Moser R, Pedrycz W, Succi G (2008) A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction. In: Proceedings of the 30th international conference on Software engineering, ACM, pp 181–190
Munson JC, Khoshgoftaar TM (1992) The detection of fault-prone programs. IEEE Trans Softw Eng 18(5):423–433
Nagappan N, Ball T (2005) Use of relative code churn measures to predict system defect density. In: Proceedings 27th International conference on software engineering, 2005. ICSE 2005. IEEE, pp 284– 292
Nagappan N, Ball T, Murphy B (2006a) Using historical in-process and product metrics for early estimation of software failures. In: 17th International symposium on software reliability engineering, 2006.  ISSRE’06. IEEE, pp 62–74
Nagappan N, Ball T, Zeller A (2006b) Mining metrics to predict component failures. In: Proceedings of the 28th international conference on Software engineering, ACM, pp 452–461
Nam J, Kim S (2015) Clami: Defect prediction on unlabeled datasets (t). In: 2015 30th IEEE/ACM International conference on automated software engineering (ASE), IEEE, pp 452–463
Neto EC, da Costa DA, Kulesza U (2018) The impact of refactoring changes on the szz algorithm: An empirical study. In: 2018 IEEE 25Th international conference on software analysis, evolution and reengineering (SANER), IEEE, pp 380–390
Ostrand TJ, Weyuker EJ, Bell RM (2004) Where the bugs are. In: ACM SIGSOFT Software engineering notes, ACM, vol 29, pp 86–96
Parnin C, Orsom A (2011) Are automated debugging techniques actually helping programmers? In: Proceedings of the 2011 international symposium on software testing and analysis, ACM, pp 199–209
Purushothaman R, Perry DE (2005) Toward understanding the rhetoric of small source code changes. IEEE Trans Softw Eng 31(6):511–526
Rahman F, Devanbu P (2013) How, and why, process metrics are better. In: Proceedings of the 2013 International conference on software engineering, IEEE Press, pp 432–441
Rahman F, Posnett D, Devanbu P (2012) Recalling the imprecision of cross-project defect prediction. In: Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, ACM, p 61
Shihab E, Hassan AE, Adams B, Jiang ZM (2012) An industrial study on the risk of software changes.  In: Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, ACM, p 62
Shihab E, Ihara A, Kamei Y, Ibrahim WM, Ohira M, Adams B, Hassan AE, Matsumoto K (2013) Studying re-opened bugs in open source software. Empir Softw Eng 18(5):1005–1042
Sliwerski J, Zimmermann T, Zeller A (2005) When do changes induce fixes? In: ACM Sigsoft software engineering notes, ACM, vol 30, pp 1–5
Tantithamthavorn C, McIntosh S, Hassan AE, Matsumoto K (2016) Automated parameter optimization of classification techniques for defect prediction models. In: Proceedings of the 38th International Conference on Software Engineering, ACM, pp 321–332
Thongmak M, Muenchaisri P (2003) Predicting faulty classes using design metrics with discriminant analysis. In: Software engineering research and practice, pp 621–627
Turhan B, Menzies T, Bener AB, Di Stefano J (2009) On the relative value of cross-company and withincompany data for defect prediction. Empir Softw Eng 14(5):540–578
Valdivia Garcia H, Shihab E (2014) Characterizing and predicting blocking bugs in open source projects. In: Proceedings of the 11th working conference on mining software repositories, ACM, pp 72–81
Wilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83
Xia X, Bao L, Lo D, Li S (2016a) Automated debugging considered harmful considered harmful: a user study revisiting the usefulness of spectra-based fault localization techniques with professionals using real bugs from large systems. In: 2016 IEEE International conference on software maintenance and evolution (ICSME), IEEE, pp 267–278
Xia X, Lo D, Pan SJ, Nagappan N,Wang X (2016b) Hydra: Massively compositional model for cross-project defect prediction. IEEE Trans Softw Eng 42(10):977–998
Xia X, Lo D, Wang X, Yang X (2016c) Collective personalized change classification with multiobjective search. IEEE Trans Reliab 65(4):1810–1829
Yan M, Fang Y, Lo D, Xia X, Zhang X (2017) File-level defect prediction: Unsupervised vs. supervised models. In: Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ACM, p to appear
Yang X, Lo D, Xia X, Zhang Y, Sun J (2015) Deep learning for just-in-time defect prediction. In: 2015 IEEE International conference on software quality, reliability and security (QRS), IEEE, pp 17–26
Yang X, Lo D, Xia X, Sun J (2017) Tlel: a two-layer ensemble learning approach for just-in-time defect prediction. Inf Softw Technol 87:206–220
Yang Y, Zhou Y, Liu J, Zhao Y, Lu H, Xu L, Xu B, Leung H (2016) Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models. In: Proceedings of the 2016 24th ACM SIGSOFT International symposium on foundations of software engineering, ACM, pp 157–168
Yin Z, Yuan D, Zhou Y, Pasupathy S, Bairavasundaram L (2011) How do fixes become bugs? In: Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, ACM, pp 26–36
Zhou Y, Yang Y, Lu H, Chen L, Li Y, Zhao Y, Qian J, Xu B (2018) How far we have progressed in the journey? an examination of cross-project defect prediction. ACM Trans Softw Eng Methodol (TOSEM) 27(1):1
Zimmermann T, Nagappan N, Gall H, Giger E, Murphy B (2009) Cross-project defect prediction: a large scale experiment on data vs. domain vs. process. In: Proceedings of the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering, ACM



>><[N]>Revisiting the Impact of Concept Drift on Just-in-Time Quality Assurance
[1] T. Menzies, J. Greenwald, and A. Frank, “Data mining static code
attributes to learn defect predictors,” IEEE Transactions on Software
Engineering, vol. 33, no. 1, pp. 2–13, 2007.
[2] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking
classification models for software defect prediction: A proposed framework
and novel findings,” IEEE Transactions on Software Engineering,
vol. 34, no. 4, pp. 485–496, 2008.
[3] F. Xing, P. Guo, and M. R. Lyu, “A novel method for early software
quality prediction based on support vector machine,” in Proceedings
of the 16th IEEE International Symposium on Software Reliability
Engineering, 2005. ISSRE 2005. IEEE, 2005, pp. 10–pp.
[4] T. M. Khoshgoftaar and N. Seliya, “Comparative assessment of software
quality classification techniques: An empirical case study,” Empirical
Software Engineering, vol. 9, no. 3, pp. 229–257, 2004.
[5] J. Nam, “Survey on software defect prediction,” Master’s Thesis, 2009.
[6] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for
just-in-time defect prediction,” in Proceedings of the IEEE International
Conference on Software Quality, Reliability and Security. IEEE, 2015,
pp. 17–26.
[7] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic features
for defect prediction,” in Proceedings of the IEEE/ACM 38th International
Conference on Software Engineering (ICSE). IEEE, 2016, pp.
297–308.
[8] N. E. Fenton and M. Neil, “A critique of software defect prediction
models,” IEEE Transactions on software engineering, vol. 25, no. 5, pp.
675–689, 1999.
[9] D. Gray, D. Bowes, N. Davey, Y. Sun, and B. Christianson, “The misuse
of the nasa metrics data program data sets for automated software defect
prediction,” in Proceedings of the 15th Annual Conference on Evaluation
& Assessment in Software Engineering (EASE 2011), pp. 96–103, 2011.
[10] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2012.
[11] J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein, “Tracking
concept drift of software projects using defect prediction quality,” in
Proceedings of the 6th IEEE International Working Conference on
Mining Software Repositories. IEEE, 2009, pp. 51–60.
[12] S. Amasaki, “On applicability of cross-project defect prediction method
for multi-versions projects,” in Proceedings of the 13th International
Conference on Predictive Models and Data Analytics in Software
Engineering. ACM, 2017, pp. 93–96.
[13] L. L. Minku and X. Yao, “Can cross-company data improve performance
in software effort estimation?” in Proceedings of the 8th International
Conference on Predictive Models in Software Engineering. ACM, 2012,
pp. 69–78.
[14] M. A. Kabir, J. W. Keung, K. E. Benniny, and M. Zhang, “Assessing
the significant impact of concept drift in software defect prediction,”
in Proceedings of the IEEE 43rd Annual Computer Software and
Applications Conference (COMPSAC), vol. 1. IEEE, 2019, pp. 53–
58.
[15] K. E. Bennin, J. Keung, P. Phannachitta, A. Monden, and S. Mensah,
“Mahakil: Diversity based oversampling approach to alleviate the class
imbalance issue in software defect prediction,” IEEE Transactions on
Software Engineering, vol. 44, no. 6, pp. 534–550, 2017.
[16] K. E. Bennin, J. W. Keung, and A. Monden, “On the relative value of
data resampling approaches for software defect prediction,” Empirical
Software Engineering, vol. 24, no. 2, pp. 602–636, 2019.
[17] T. M. Khoshgoftaar, L. A. Bullard, and K. Gao, “Attribute selection using
rough sets in software quality classification,” International Journal of
Reliability, Quality and Safety Engineering, vol. 16, no. 01, pp. 73–89,
2009.
[18] J. Ekanayake, J. Tappolet, H. C. Gall, and A. Bernstein, “Time variance
and defect prediction in software projects,” Empirical Software Engineering,
vol. 17, no. 4-5, pp. 348–389, 2012.
[19] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5–32, 2001.
[20] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
in Proceedings of the 22nd ACM sigkdd international conference on
knowledge discovery and data mining. ACM, 2016, pp. 785–794.
[21] T. Chen, T. He, M. Benesty, V. Khotilovich, and Y. Tang, “Xgboost:
extreme gradient boosting,” R package version 0.4-2, pp. 1–4, 2015.
[22] D. Nielsen, “Tree boosting with xgboost-why does xgboost win” every”
machine learning competition?” Master’s thesis, NTNU, 2016.
[23] A. P. Bradley, “The use of the area under the roc curve in the evaluation
of machine learning algorithms,” Pattern recognition, vol. 30, no. 7, pp.
1145–1159, 1997.
[24] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The impact of
class rebalancing techniques on the performance and interpretation of
defect prediction models,” IEEE Transactions on Software Engineering,
2018.
[25] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: simple unsupervised models
could be better than supervised models,” in Proceedings of the 24th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 2016, pp. 157–168.


>><[N]>Revisiting Unsupervised Learning for Defect Prediction
[1] Amritanshu Agrawal and Tim Menzies. 2017. "Better Data" is Better than "Better
Data Miners" (Beneits of Tuning SMOTE for Defect Prediction). CoRR
abs/1705.03697 (2017). http://arxiv.org/abs/1705.03697
[2] Fumio Akiyama. 1971. An example of software system debugging. In IFIP Congress
(1), Vol. 71. 353ś359.
[3] Erik Arisholm and Lionel C Briand. 2006. Predicting fault-prone components in a
java legacy system. In Proceedings of the 2006 ACM/IEEE international symposium
on Empirical software engineering. ACM, 8ś17.
[4] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate:
a practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society. Series B (Methodological) (1995), 289ś300.
[5] Shyam R Chidamber and Chris F Kemerer. 1994. A metrics suite for object
oriented design. IEEE Transactions on Software Engineering 20, 6 (1994), 476ś493.
[6] Marco D’Ambros, Michele Lanza, and Romain Robbes. 2010. An extensive comparison
of bug prediction approaches. In 2010 7th IEEE Working Conference on
Mining Software Repositories. IEEE, 31ś41.
[7] Wei Fu, Tim Menzies, and Xipeng Shen. 2016. Tuning for software analytics: Is it
really necessary? Information and Software Technology 76 (2016), 135ś146.
[8] Wei Fu, Vivek Nair, and Tim Menzies. 2016. Why is diferential evolution better
than grid search for tuning defect predictors? arXiv preprint arXiv:1609.02613
(2016).
[9] Takafumi Fukushima, Yasutaka Kamei, Shane McIntosh, Kazuhiro Yamashita, and
Naoyasu Ubayashi. 2014. An empirical study of just-in-time defect prediction
using cross-project models. In Proceedings of the 11th Working Conference on
Mining Software Repositories. ACM, 172ś181.
[10] Todd L Graves, Alan F Karr, James S Marron, and Harvey Siy. 2000. Predicting
fault incidence using software change history. IEEE Transactions on Software
Engineering 26, 7 (2000), 653ś661.
[11] Philip J Guo, Thomas Zimmermann, Nachiappan Nagappan, and Brendan Murphy.
2010. Characterizing and predicting which bugs get ixed: an empirical study of
Microsoft Windows. In Proceedings of the 32nd ACM/IEEE International Conference
on Software Engineering - Volume 1, Vol. 1. ACM, 495ś504.
[12] Mark Hall, Eibe Frank, Geofrey Holmes, Bernhard Pfahringer, Peter Reutemann,
and Ian H Witten. 2009. The WEKA data mining software: an update. ACM
SIGKDD Explorations Newsletter 11, 1 (2009), 10ś18.
[13] Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2012.
A systematic literature review on fault prediction performance in software engineering.
IEEE Transactions on Software Engineering 38, 6 (2012), 1276ś1304.
[14] Maurice Howard Halstead. 1977. Elements of software science. Vol. 7. Elsevier
New York.
[15] Maggie Hamill and Katerina Goseva-Popstojanova. 2009. Common trends in
software fault and failure data. IEEE Transactions on Software Engineering 35, 4
(2009), 484ś496.
[16] Ahmed E Hassan. 2009. Predicting faults using the complexity of code changes.
In Proceedings of the 31st International Conference on Software Engineering. IEEE,
78ś88.
[17] Qiao Huang, Xin Xia, and David Lo. 2017. Supervised vs unsupervised models:
a holistic look at efort-aware just-in-time defect prediction. In 2017 IEEE
International Conference on Software Maintenance and Evolution. IEEE.
[18] Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction. In
Proceedings of the 28th IEEE/ACM International Conference on Automated Software
Engineering. IEEE, 279ś289.
[19] Xiao-Yuan Jing, Shi Ying, Zhi-Wu Zhang, Shan-Shan Wu, and Jin Liu. 2014.
Dictionary learning based software defect prediction. In Proceedings of the 36th
International Conference on Software Engineering. ACM, 414ś423.
[20] Dennis Kafura and Geereddy R. Reddy. 1987. The use of software complexity
metrics in software maintenance. IEEE Transactions on Software Engineering 3
(1987), 335ś343.
[21] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757ś773.
[22] Taghi M Khoshgoftaar and Edward B Allen. 2001. Modeling software quality
with. Recent Advances in Reliability and Quality Engineering 2 (2001), 247.
[23] Taghi M Khoshgoftaar and Naeem Seliya. 2003. Software quality classiication
modeling using the SPRINT decision tree algorithm. International Journal on
Artiicial Intelligence Tools 12, 03 (2003), 207ś225.
[24] Taghi M Khoshgoftaar, Xiaojing Yuan, and Edward B Allen. 2000. Balancing
misclassiication rates in classiication-tree models of software quality. Empirical
Software Engineering 5, 4 (2000), 313ś330.
[25] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering 34, 2 (2008),
181ś196.
[26] Sunghun Kim, Thomas Zimmermann, E James Whitehead Jr, and Andreas Zeller.
2007. Predicting faults from cached history. In Proceedings of the 29th International
Conference on Software Engineering. IEEE, 489ś498.
[27] Ekrem Kocaguneli, Tim Menzies, Ayse Bener, and Jacky W Keung. 2012. Exploiting
the essential assumptions of analogy-based efort estimation. IEEE
Transactions on Software Engineering 38, 2 (2012), 425ś438.
[28] A Güneş Koru, Dongsong Zhang, Khaled El Emam, and Hongfang Liu. 2009. An
investigation into the functional form of the size-defect relationship for software
modules. IEEE Transactions on Software Engineering 35, 2 (2009), 293ś304.
[29] Taek Lee, Jaechang Nam, DongGyun Han, Sunghun Kim, and Hoh Peter In. 2011.
Micro interaction metrics for defect prediction. In Proceedings of the 19th ACM
SIGSOFT Symposium and the 13th European Conference on Foundations of Software
Engineering. ACM, 311ś321.
[30] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking classiication models for software defect prediction: A proposed
framework and novel indings. IEEE Transactions on Software Engineering 34, 4
(2008), 485ś496.
[31] Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. 2017.
Code churn: A neglected metric in efort-aware Just-In-Time defect prediction.
In Empirical Software Engineering and Measurement, 2017 ACM/IEEE International
Symposium on. IEEE.
[32] Shinsuke Matsumoto, Yasutaka Kamei, Akito Monden, Ken-ichi Matsumoto, and
Masahide Nakamura. 2010. An analysis of developer metrics for fault prediction.
In Proceedings of the 6th International Conference on Predictive Models in Software
Engineering. ACM, 18.
[33] Thomas J McCabe. 1976. A complexity measure. IEEE Transactions on Software
Engineering 4 (1976), 308ś320.
[34] Thilo Mende and Rainer Koschke. 2010. Efort-aware defect prediction models.
In 2010 14th European Conference on Software Maintenance and Reengineering.
IEEE, 107ś116.
[35] Tim Menzies, Jeremy Greenwald, and Art Frank. 2007. Data mining static code
attributes to learn defect predictors. IEEE Transactions on Software Engineering
33, 1 (2007).
[36] Tim Menzies, Zach Milton, Burak Turhan, Bojan Cukic, Yue Jiang, and Ayşe Bener.
2010. Defect prediction from static code features: current results, limitations,
new approaches. Automated Software Engineering 17, 4 (2010), 375ś407.
[37] Ayse Tosun Misirli, Ayse Bener, and Resat Kale. 2011. Ai-based software defect
predictors: Applications and beneits in a case study. AI Magazine 32, 2 (2011),
57ś68.
[38] Audris Mockus and David M Weiss. 2000. Predicting risk of software changes.
Bell Labs Technical Journal 5, 2 (2000), 169ś180.
[39] Akito Monden, Takuma Hayashi, Shoji Shinoda, Kumiko Shirai, Junichi Yoshida,
Mike Barker, and Kenichi Matsumoto. 2013. Assessing the cost efectiveness of
fault prediction in acceptance testing. IEEE Transactions on Software Engineering
39, 10 (2013), 1345ś1357.
[40] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. 2008. A comparative
analysis of the eiciency of change metrics and static code attributes for defect
prediction. In Proceedings of the 30th International Conference on Software
Engineering. ACM, 181ś190.
[41] Nachiappan Nagappan and Thomas Ball. 2005. to predict system defect density.
In Proceedings of the 27th International Conference on Software Engineering. IEEE,
284ś292.
[42] Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. 2006. Mining metrics
to predict component failures. In Proceedings of the 28th International Conference
on Software Engineering. ACM, 452ś461.
[43] Jaechang Nam, Sinno Jialin Pan, and Sunghun Kim. 2013. Transfer defect learning.
In Proceedings of the 2013 International Conference on Software Engineering. IEEE,
382ś391.
[44] Thomas J Ostrand, Elaine J Weyuker, and Robert M Bell. 2004. Where the bugs
are. In ACM SIGSOFT Software Engineering Notes, Vol. 29. ACM, 86ś96.
[45] Thomas J Ostrand, Elaine J Weyuker, and Robert M Bell. 2005. Predicting the
location and number of faults in large software systems. IEEE Transactions on
Software Engineering 31, 4 (2005), 340ś355.
[46] Corina S Pasareanu, Peter C Mehlitz, David H Bushnell, Karen Gundy-Burlet,
Michael Lowry, Suzette Person, and Mark Pape. 2008. Combining unit-level
symbolic execution and system-level concrete execution for testing NASA software.
In Proceedings of the 2008 International Symposium on Software Testing and
Analysis. ACM, 15ś26.
[47] Foyzur Rahman, Sameer Khatri, Earl T Barr, and Premkumar Devanbu. 2014.
Comparing static bug inders and statistical prediction. In Proceedings of the 36th
International Conference on Software Engineering. ACM, 424ś434.
[48] Jeanine Romano, Jefrey D Kromrey, Jesse Coraggio, Jef Skowronek, and Linda
Devine. 2006. Exploring methods for evaluating group diferences on the NSSE
and other surveys: Are the t-test and CohenâĂŹsd indices the most appropriate
choices. In Annual Meeting of the Southern Association for Institutional Research.
[49] Forrest Shull, Ioana Rus, and Victor Basili. 2001. Improving software inspections
by using reading techniques. In Proceedings of the 23rd International Conference
on Software Engineering. IEEE, 726ś727.
[50] Burak Turhan, Tim Menzies, Ayşe B Bener, and Justin Di Stefano. 2009. On the
relative value of cross-company and within-company data for defect prediction.
Empirical Software Engineering 14, 5 (2009), 540ś578.
[51] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic
features for defect prediction. In Proceedings of the 38th International Conference
on Software Engineering. ACM, 297ś308.
[52] Maurice V Wilkes. 1985. Memoirs ofa Computer Pioneer. Cambridge, Mass.,
London (1985).
[53] David H Wolpert. 2002. The supervised learning no-free-lunch theorems. In Soft
Computing and Industry. Springer, 25ś42.
[54] Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei Xu,
Baowen Xu, and Hareton Leung. 2016. Efort-aware just-in-time defect prediction:
simple unsupervised models could be better than supervised models. In Proceedings
of the 2016 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering. ACM, 157ś168.
[55] Zuoning Yin, Ding Yuan, Yuanyuan Zhou, Shankar Pasupathy, and Lakshmi
Bairavasundaram. 2011. How do ixes become bugs?. In Proceedings of the 19th
ACM SIGSOFT Symposium and the 13th European Conference on Foundations of
Software Engineering. ACM, 26ś36.
[56] Thomas Zimmermann, Rahul Premraj, and Andreas Zeller. 2007. Predicting
defects for eclipse. In Proceedings of the Third International Workshop on Predictor
Models in Software Engineering. IEEE Computer Society, 9.



>><[N]>Simplified Deep Forest Model Based Just-in-Time Defect Prediction for Android Mobile Apps
[1] L.Wang, X. Sun, J.Wang, Y. Duan, and B. Li, “Construct bug knowledge
graph for bug resolution,” in Proc. IEEE/ACM 39th Int. Conf. Softw. Eng.
Companion, 2017, pp. 189–191.
[2] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via
convolutional neural network,” in Proc. Int. Conf. Softw. Qual., Rel., Secur.,
2017, pp. 318–328.
[3] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for just-intime
defect prediction,” in Proc. IEEE Int. Conf. Softw. Qual., Rel., Secur.,
2015, pp. 17–26.
[4] X. Kong, L. Zhang, W. E. Wong, and B. Li, “Experience report: How do
techniques, programs, and tests impact automated program repair?,” in
Proc. 26th Int. Symp. Softw. Rel. Eng., 2015, pp. 194–204.
[5] S. McIlroy, N. Ali, and A. E. Hassan, “Fresh apps: An empirical study
of frequently-updated mobile apps in the Google Play store,” Empirical
Softw. Eng., vol. 21, no. 3, pp. 1346–1370, 2016.
[6] M. Nayebi, B. Adams, and G. Ruhe, “Release practices for mobile appswhat
do users and developers think?,” in Proc. 23rd Int. Conf. Softw. Anal.,
Evol., Reengineering, vol. 1, 2016, pp. 552–562.
[7] T. Menzies, J. Greenwald, and A. Frank, “Data mining static code attributes
to learn defect predictors,” IEEE Trans. Softw. Eng., vol. 33, no. 1, pp. 2–13,
Jan. 2007.
[8] M. Jureczko and D. Spinellis, “Using object-oriented design metrics to
predict software defects,” Models and Methods of System Dependability.
Wrocław, Poland: Oficyna Wydawnicza Politechniki Wrocławskiej,
pp. 69–81, 2010.
[9] Z. Xu et al., “LDFR: Learning deep feature representation for software
defect prediction,” J. Syst. Softw., vol. 158, 2019, Art. no. 110402.
[10] Y. Kamei et al., “A large-scale empirical study of just-in-time quality
assurance,” IEEE Trans. Softw. Eng., vol. 39, no. 6, pp. 757–773, Jun. 2013.
[11] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi, and
A. E. Hassan, “Studying just-in-time defect prediction using cross-project
models,” Empirical Softw. Eng., vol. 21, no. 5, pp. 2072–2106, 2016.
[12] G. Catolino, D. Di Nucci, and F. Ferrucci, “Cross-project just-in-time bug
prediction for mobile apps: An empirical assessment,” in Proc. IEEE/ACM
6th Int. Conf. Mobile Softw. Eng. Syst., 2019, pp. 99–110.
[13] M.Yan, X. Xia,Y. Fan, A. E. Hassan, D. Lo, and S. Li, “Just-in-time defect
identification and localization: A two-phase framework,” IEEE Trans.
Softw. Eng., to be published.
[14] M. Yan, X. Xia, Y. Fan, D. Lo, A. E. Hassan, and X. Zhang, “Effort-aware
just-in-time defect identification in practice: A case study at Alibaba,” in
Proc. 28th ACM Joint Meeting Eur. Softw. Eng. Conf. Symp. Found. Softw.
Eng., 2020, pp. 1308–1319.
[15] G. Catolino, “Just-in-time bug prediction in mobile applications: The
domain matters!,” in Proc. IEEE/ACM 4th Int. Conf. Mobile Softw. Eng.
Syst., 2017, pp. 201–202.
[16] Z. Zhou and J. Feng, “Deep forest: Towards an alternative to deep neural
networks,” in Proc. 26th Int. Joint Conf. Artif. Intell., 2017, pp. 3553–3559.
[17] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
“An empirical study of just-in-time defect prediction using cross-project
models,” in Proc. 11th Work. Conf. Mining Softw. Repositories, 2014,
pp. 172–181.
[18] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target?
A longitudinal case study of just-in-time defect prediction,” IEEE Trans.
Softw. Eng., vol. 44, no. 5, pp. 412–428, May 2018.
[19] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-grained just-in-time
defect prediction,” J. Syst. Softw., vol. 150, pp. 22–36, 2019.
[20] X.Yang, D. Lo, X. Xia, and J. Sun, “TLEL:Atwo-layer ensemble learning
approach for just-in-time defect prediction,” Inf. Softw. Technol., vol. 87,
pp. 206–220, 2017.
[21] X. Chen, Y. Zhao, Q. Wang, and Z. Yuan, “Multi: Multi-objective effortaware
just-in-time software defect prediction,” Inf. Softw. Technol., vol. 93,
pp. 1–13, 2018.
[22] G. G. Cabral, L. L. Minku, E. Shihab, and S. Mujahid, “Class imbalance
evolution and verification latency in just-in-time software defect prediction,”
in Proc. 41st Int. Conf. Softw. Eng., 2019, pp. 666–676.
[23] M.Kondo, D. M. Germán, O. Mizuno, and E. Choi, “The impact of context
metrics on just-in-time defect prediction,” Empirical Softw. Eng., vol. 25,
no. 1, pp. 890–939, 2020.
[24] T. Zhou, X. Sun, X. Xia, B. Li, and X. Chen, “Improving defect prediction
with deep forest,” Inf. Softw. Technol., vol. 114, pp. 204–216, 2019.
[25] W. Zheng, S. Mo, X. Jin, Y. Qu, Z. Xie, and J. Shuai, “Software defect
prediction model based on improved deep forest and autoencoder by
forest,” in Proc. 31st Int. Conf. Softw. Eng. Knowl. Eng., 2019, pp. 419–540.
[26] X.-Y. Jing, S. Ying, Z.-W. Zhang, S.-S. Wu, and J. Liu, “Dictionary
learning based software defect prediction,” in Proc. 36th Int. Conf. Softw.
Eng., 2014, pp. 414–423.
[27] X. Jing, F.Wu, X. Dong, F. Qi, and B. Xu, “Heterogeneous cross-company
defect prediction by unified metric representation and CCA-based transfer
learning,” in Proc. 10th Joint Meeting Found. Softw. Eng., 2015, pp. 496–
507.
[28] Q. Song, Y. Guo, and M. Shepperd, “A comprehensive investigation of the
role of imbalanced learning for software defect prediction,” IEEE Trans.
Softw. Eng., vol. 45, no. 12, pp. 1253–1269, Dec. 2019.
[29] N. Li, M. Shepperd, and Y. Guo, “A systematic review of unsupervised
learning techniques for software defect prediction,” Inf. Softw. Technol.,
vol. 122, 2020, Art. no. 106287.
[30] X.-Y. Jing, F. Wu, X. Dong, and B. Xu, “An improved SDA based defect
prediction framework for both within-project and cross-project classimbalance
problems,” IEEE Trans. Softw. Eng., vol. 43, no. 4, pp. 321–339,
Apr. 2017.
[31] F. Zhang, Q. Zheng, Y. Zou, and A. E. Hassan, “Cross-project defect
prediction using a connectivity-based unsupervised classifier,” in Proc.
IEEE/ACM 38th Int. Conf. Softw. Eng., 2016, pp. 309–320.
[32] Y. Zhou et al., “How far we have progressed in the journey? An examination
of cross-project defect prediction,”ACMTrans. Softw. Eng. Methodol.,
vol. 27, no. 1, pp. 1–51, 2018.
[33] Z. Li, X.-Y. Jing, F. Wu, X. Zhu, B. Xu, and S. Ying, “Cost-sensitive
transfer kernel canonical correlation analysis for heterogeneous defect
prediction,” Automated Softw. Eng., vol. 25, no. 2, pp. 201–245, 2018.
[34] S. Wang and X. Yao, “Using class imbalance learning for software defect
prediction,” IEEE Trans. Rel., vol. 62, no. 2, pp. 434–443, Jun. 2013.
[35] Y. Yang et al., “Effort-aware just-in-time defect prediction: Simple unsupervised
models could be better than supervised models,” in Proc. 24th
ACM SIGSOFT Int. Symp. Found. Softw. Eng., 2016, pp. 157–168.
[36] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Softw.
Eng., vol. 24, no. 5, pp. 2823–2862, 2019.
[37] F. Wilcoxon, “Individual comparisons by ranking methods,” in Breakthroughs
in Statistics. Berlin, Germany: Springer, 1992, pp. 196–202.
[38] N. Cliff, Ordinal Methods for Behavioral Data Analysis. New York, NY,
USA: Psychology Press, 2014.
[39] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto, “An
empirical comparison of model validation techniques for defect prediction
models,” IEEE Trans. Softw. Eng., vol. 43, no. 1, pp. 1–18, Jan. 2017.
[40] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of
classification techniques on the performance of defect prediction models,”
in Proc. 37th Int. Conf. Softw. Eng., 2015, pp. 789–800.
[41] Z. Xu, J. Xuan, J. Liu, and X. Cui,“MICHAC: Defect prediction via feature
selection based on maximal information coefficient with hierarchical
agglomerative clustering,” in Proc. 23rd Int. Conf. Softw. Anal., Evol.,
Reengineering, vol. 1, 2016, pp. 370–381.
[42] Z. Xu, J. Liu, Z. Yang, G. An, and X. Jia, “The impact of feature selection
on defect prediction performance: An empirical comparison,” in Proc. 27th
Int. Symp. Softw. Rel. Eng., 2016, pp. 309–320.
[43] Z. Xu, J. Liu, X. Luo, and T. Zhang, “Cross-version defect prediction via
hybrid active learning with kernel principal component analysis,” in Proc.
25th Int. Conf. Softw. Anal., Evol., Reengineering., 2018, pp. 209–220.
[44] Z.Xuet al., “Software defect prediction based on kernelPCAand weighted
extreme learning machine,” Inf. Softw. Technol., vol. 106, pp. 182–200,
2019.
[45] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”
Chemometrics Intell. Lab. Syst., vol. 2, no. 1–3, pp. 37–52, 1987.
[46] B. Schölkopf, A. Smola, and K.-R. Müller, “Kernel principal component
analysis,” in Proc. Int. Conf. Artif. Neural Netw., 1997, pp. 583–588.
[47] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by
locally linear embedding,” Science, vol. 290, no. 5500, pp. 2323–2326,
2000.
[48] J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric
framework for nonlinear dimensionality reduction,” Science, vol. 290,
no. 5500, pp. 2319–2323, 2000.
[49] M. A. Hall, “Correlation-based feature selection of discrete and numeric
class machine learning,” in Proc. 17th Int. Conf. Mach. Learn., 2000,
pp. 359–366.
[50] M. Dash, H. Liu, and H. Motoda, “Consistency based feature selection,”
in Proc. 4th Pacific-Asia Conf. Knowl. Discovery Data Mining. Berlin,
Germany: Springer, 2000, pp. 98–109.



>><[N]>Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction
[1] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-grained
just-in-time defect prediction,” Journal of Systems and
Software, vol. 150, pp. 22 – 36, 2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0164121218302656
[2] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,” IEEE
Transactions on Software Engineering, vol. 44, no. 5, pp. 412–428, May
2018.
[3] M. Kondo, D. M. German, O. Mizuno, and E.-H. Choi, “The impact of
context metrics on just-in-time defect prediction,” Empirical Software
Engineering, vol. 25, no. 1, pp. 890–939, 2020.
[4] T. Hoang, H. Khanh Dam, Y. Kamei, D. Lo, and N. Ubayashi, “Deepjit:
An end-to-end deep learning framework for just-in-time defect prediction,”
in 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR), 2019, pp. 34–45.
[5] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” SIGSOFT Softw. Eng. Notes, vol. 30, no. 4, pp. 1–5, May 2005.
[6] G. G. Cabral, L. L. Minku, E. Shihab, and S. Mujahid, “Class imbalance
evolution and verification latency in just-in-time software defect prediction,”
in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), May 2019, pp. 666–676.
[7] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, June 2013.
[8] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction
for imbalanced data,” in 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering, vol. 2, May 2015, pp. 99–108.
[9] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu,
and H. Leung, “Effort-aware just-in-time defect prediction: Simple
unsupervised models could be better than supervised models,” in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering, ser. FSE 2016. New York,
NY, USA: Association for Computing Machinery, 2016, p. 157–168.
[Online]. Available: https://doi.org/10.1145/2950290.2950353
[10] M. D’Ambros, M. Lanza, and R. Robbes, “Evaluating defect prediction
approaches: A benchmark and an extensive comparison,” Empirical
Softw. Engg., vol. 17, no. 4-5, pp. 531–577, Aug. 2012. [Online].
Available: http://dx.doi.org/10.1007/s10664-011-9173-9
[11] F. Rahman, S. Khatri, E. T. Barr, and P. Devanbu, “Comparing static
bug finders and statistical prediction,” in Proceedings of the 36th
International Conference on Software Engineering, ser. ICSE 2014.
New York, NY, USA: ACM, 2014, pp. 424–434. [Online]. Available:
http://doi.acm.org/10.1145/2568225.2568269
[12] J. Zheng, L. Williams, N. Nagappan, W. Snipes, J. P. Hudepohl, and
M. A. Vouk, “On the value of static analysis for fault detection in
software,” IEEE Transactions on Software Engineering, vol. 32, no. 4,
pp. 240–253, April 2006.
[13] P. Devanbu, T. Zimmermann, and C. Bird, “Belief evidence in empirical
software engineering,” in 2016 IEEE/ACM 38th International Conference
on Software Engineering (ICSE), May 2016, pp. 108–119.
[14] S. Panichella, V. Arnaoudova, M. D. Penta, and G. Antoniol, “Would
static analysis tools help developers with code reviews?” in 2015 IEEE
22nd International Conference on Software Analysis, Evolution and
Reengineering (SANER), vol. 00, March 2015, pp. 161–170.
[15] L.-P. Querel and P. C. Rigby, “Warningsguru: Integrating statistical
bug models with static analysis to provide timely and specific bug
warnings,” in Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ser. ESEC/FSE 2018. New
York, NY, USA: Association for Computing Machinery, 2018, p.
892–895. [Online]. Available: https://doi.org/10.1145/3236024.3264599
[16] Y. Fan, D. Alencar da Costa, D. Lo, A. E. Hassan, and L. Shanping, “The
impact of mislabeled changes by szz on just-in-time defect prediction,”
IEEE Transactions on Software Engineering, 2020.
[17] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh, and Y.-G. Guéhéneuc,
“Is it a bug or an enhancement?: A text-based approach to classify
change requests,” in Proceedings of the 2008 Conference of the Center
for Advanced Studies on Collaborative Research: Meeting of Minds, ser.
CASCON ’08. New York, NY, USA: ACM, 2008, pp. 23:304–23:318.
[Online]. Available: http://doi.acm.org/10.1145/1463788.1463819
[18] K. Herzig, S. Just, and A. Zeller, “It’s not a bug, it’s a feature:
How misclassification impacts bug prediction,” in Proceedings of the
International Conference on Software Engineering, ser. ICSE ’13.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 392–401. [Online].
Available: http://dl.acm.org/citation.cfm?id=2486788.2486840
[19] S. Herbold, A. Trautsch, F. Trautsch, and B. Ledel, “Issues with
szz: An empirical study of the state of practice of defect prediction
data collection,” Submitted to: Empirical Software Engineering, 2020.
[Online]. Available: https://arxiv.org/abs/1911.08938
[20] E. C. Neto, D. A. da Costa, and U. Kulesza, “The impact of refactoring
changes on the szz algorithm: An empirical study,” in 2018 IEEE
25th International Conference on Software Analysis, Evolution and
Reengineering (SANER), March 2018, pp. 380–390.
[21] S. Herbold, “On the costs and profit of software defect prediction,” IEEE
Transactions on Software Engineering, pp. 1–1, 2019.
[22] Q. Huang, X. Xia, and D. Lo, “Supervised vs unsupervised models:
A holistic look at effort-aware just-in-time defect prediction,” in 2017
IEEE International Conference on Software Maintenance and Evolution
(ICSME), 2017, pp. 159–170.
[23] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in
2013 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE), Nov 2013, pp. 279–289.
[24] F. Rahman and P. Devanbu, “How, and why, process metrics are better,”
in 2013 35th International Conference on Software Engineering (ICSE),
May 2013, pp. 432–441.
[25] C. Rosen, B. Grawi, and E. Shihab, “Commit guru: Analytics and risk
prediction of software commits,” in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2015.
New York, NY, USA: Association for Computing Machinery, 2015, p.
966–969. [Online]. Available: https://doi.org/10.1145/2786805.2803183
[26] T. J. McCabe, “A complexity measure,” IEEE Trans. Softw. Eng., vol. 2,
no. 4, pp. 308–320, Jul. 1976.
[27] S. R. Chidamber and C. F. Kemerer, “A metrics suite for object oriented
design,” IEEE Trans. Softw. Eng., vol. 20, no. 6, pp. 476–493, Jun. 1994.
[28] S. Hosseini, B. Turhan, and D. Gunarathna, “A systematic literature
review and meta-analysis on cross project defect prediction,” IEEE
Transactions on Software Engineering, vol. PP, no. 99, pp. 1–1, 2017.
[29] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
IEEE Transactions on Software Engineering, vol. 38, no. 6, pp.
1276–1304, Nov 2012.
[30] L. Breiman, “Random forests,” Mach. Learn., vol. 45,
no. 1, pp. 5–32, Oct. 2001. [Online]. Available:
http://dx.doi.org/10.1023/A:1010933404324
[31] E. Kreyszig, Advanced Engineering Mathematics: Maple Computer
Guide, 8th ed. New York, NY, USA: John Wiley & Sons, Inc., 2000.
[32] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction for
imbalanced data,” in Proceedings of the 37th International Conference
on Software Engineering - Volume 2, ser. ICSE ’15. Piscataway, NJ,
USA: IEEE Press, 2015, pp. 99–108.
[33] S. Herbold, “Autorank: A python package for automated ranking of
classifiers,” Journal of Open Source Software, vol. 5, no. 48, p. 2173,
2020. [Online]. Available: https://doi.org/10.21105/joss.02173
[34] J. Demšar, “Statistical comparisons of classifiers over multiple data sets,”
J. Mach. Learn. Res., vol. 7, pp. 1–30, Dec. 2006.
[35] J. W. Tukey, “Comparing individual means in the analysis of variance,”
Biometrics, vol. 5, no. 2, pp. 99–114, 1949. [Online]. Available:
http://www.jstor.org/stable/3001913
[36] M. Friedman, “A comparison of alternative tests of significance for the
problem of m rankings,” The Annals of Mathematical Statistics, vol. 11,
no. 1, pp. 86–92, 1940.
[37] P. Nemenyi, “Distribution-free multiple comparison,” Ph.D. dissertation,
Princeton University, 1963.
[38] J. Cohen, Statistical power analysis for the behavioral sciences. L.
Erlbaum Associates, 1988.
[39] N. Cliff, “Dominance statistics: Ordinal analyses to answer ordinal
questions.” Psychological Bulletin, vol. 114, no. 3, p. 494, 1993.
[40] H. Abdi, “Bonferroni and Sidak corrections for multiple comparisons,”
in Encyclopedia of Measurement and Statistics. Sage, Thousand Oaks,
CA, 2007, pp. 103–107.
[41] F. Trautsch, S. Herbold, P. Makedonski, and J. Grabowski, “Addressing
problems with replicability and validity of repository mining studies
through a smart data platform,” Empirical Software Engineering, Aug.
2017.
[42] D. Spadini, M. Aniche, and A. Bacchelli, “PyDriller: Python
framework for mining software repositories,” in Proceedings of
the 2018 26th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations
of Software Engineering - ESEC/FSE 2018. New York, New
York, USA: ACM Press, 2018, pp. 908–911. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=3236024.3264598
[43] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Software
Engineering, pp. 1–40, 2018.
[44] X. Yang, D. Lo, X. Xia, and J. Sun, “Tlel: A two-layer ensemble
learning approach for just-in-time defect prediction,” Information and
Software Technology, vol. 87, pp. 206 – 220, 2017. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0950584917302501
[45] N. Fenton and J. Bieman, Software Metrics: A Rigorous and Practical
Approach, Third Edition, 3rd ed. Boca Raton, FL, USA: CRC Press,
Inc., 2014.
[46] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, and
A. Wesslén, Experimentation in Software Engineering: An Introduction.
Norwell, MA, USA: Kluwer Academic Publishers, 2000.
[47] D. A. da Costa, S. McIntosh,W. Shang, U. Kulesza, R. Coelho, and A. E.
Hassan, “A framework for evaluating the results of the szz approach for
identifying bug-introducing changes,” IEEE Transactions on Software
Engineering, vol. 43, no. 7, pp. 641–657, 2017.



>><A(Y)>Studying just-in-time defect prediction using cross-project models
Basili VR, Briand LC, Melo WL (1996) A validation of object-oriented design metrics as quality indicators.
IEEE Trans Softw Eng 22(10):751–761
Bettenburg N, Nagappan M, Hassan AE (2012) Think locally, act globally: Improving defect and effort
prediction models. In: Proc. Int’l Working Conf. on Mining Software Repositories (MSR’12), pp 60–69
Breiman L (2001) Random forests. Mach Learn 45(1):5–32
Briand LC, Melo WL, W¨ust J (2002) Assessing the applicability of fault-proneness models across objectoriented
software projects. IEEE Trans Softw Eng 28(7):706–720
Coolidge FL (2012) Statistics: A Gentle Introduction. SAGE Publications (3rd ed.)
D’Ambros M, Lanza M, Robbes R (2010) An extensive comparison of bug prediction approaches. In: Proc.
Int’l Working Conf. on Mining Software Repositories (MSR’10), pp 31–41
Fukushima T, Kamei Y, McIntosh S, Yamashita K, Ubayashi N (2014) An empirical study of just-intime
defect prediction using cross-project models. In: Proc. Int’l Working Conf. on Mining Software
Repositories (MSR’14), pp 172–181
Graves TL, Karr AF, Marron JS, Siy H (2000) Predicting fault incidence using software change history. IEEE
Trans Softw Eng 26(7):653–661
Guo PJ, Zimmermann T, Nagappan N, Murphy B (2010) Characterizing and predicting which bugs get
fixed: An empirical study of microsoft windows. In: Proc. Int’l Conf. on Softw. Eng. (ICSE’10) vol 1,
pp 495–504
Hall T, Beecham S, Bowes D, Gray D, Counsell S (2012) A systematic literature review on fault prediction
performance in software engineering. IEEE Trans Softw Eng 38(6):1276–1304
Hassan AE (2009) Predicting faults using the complexity of code changes. In: Proc. Int’l Conf. on Softw.
Eng. (ICSE’09), pp 78–88
He Z, Shu F, Yang Y, Li M, Wang Q (2012) An investigation on the feasibility of cross-project defect
prediction. Automated Software Engg 19(2):167–199
Jiang Y, Cukic B, Menzies T (2008) Can data transformation help in the detection of fault-prone modules?
In: Proc. Workshop on Defects in Large Software Systems (DEFECTS’08), pp 16–20
Kamei Y, Monden A, Matsumoto S, Kakimoto T, Matsumoto Ki (2007) The effects of over and under
sampling on fault-prone module detection. In: Proc. Int’l Symposium on Empirical Softw. Eng. and
Measurement (ESEM’07), pp 196–204
Kamei Y, Matsumoto S, Monden A, Matsumoto K, Adams B, Hassan AE (2010) Revisiting common bug
prediction findings using effort aware models. In: Proc. Int’l Conf. on Software Maintenance (ICSM’10),
pp 1–10
Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A large-scale empirical
study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773
Kampstra P (2008) Beanplot: A boxplot alternative for visual comparison of distributions. J Stat Softw,Code
Snippets 28(1):1–9
Kim S, Whitehead EJ, Zhang Y (2008) Classifying software changes: Clean or buggy IEEE Trans Softw Eng
34(2):181–196
Kocaguneli E, Menzies T, Keung J (2012) On the value of ensemble effort estimation. IEEE Trans Softw
Eng 38(6):1403–1416
Koru AG, Zhang D, El Emam K, Liu H (2009) An investigation into the functional form of the size-defect
relationship for software modules. IEEE Trans Softw Eng 35(2):293–304
Lessmann S, Baesens B, Mues C, Pietsch S (2008) Benchmarking classification models for software defect
prediction: A proposed framework and novel findings. IEEE Trans Softw Eng 34(4):485–496
Li PL, Herbsleb J, Shaw M, Robinson B (2006) Experiences and results from initiating field defect prediction
and product test prioritization efforts at ABB Inc. In: Proc. Int’l Conf. on Softw. Eng. (ICSE’06), pp 413–
422
Matsumoto S, Kamei Y, Monden A, Matsumoto K (2010) An analysis of developer metrics for fault
prediction. In: Proc. Int’l Conf. on Predictive Models in Softw. Eng. (PROMISE’10), pp 18:1–18:9
McIntosh S, Nagappan M, Adams B, Mockus A, Hassan AE (2014) A large-scale empirical study of
the relationship between build technology and build maintenance. Empirical Software Engineering.
doi:10.1.1/jpb001. http://link.springer.com/article/10.1007
Menzies T, Turhan B, Bener A, Gay G, Cukic B, Jiang Y (2008) Implications of ceiling effects in defect
predictors. In: Proc. Int’l Conf. on Predictive Models in Softw. Eng. (PROMISE’10), pp 47–54
Menzies T, Butcher A, Marcus A, Zimmermann T, Cok D (2011) Local vs. global models for effort estimation
and defect prediction. In: Proc. Int’l Conf. on Automated Software Engineering (ASE’11),
pp 343–351
Menzies T, Butcher A, Cok D, Marcus A, Layman L, Shull F, Turhan B, Zimmermann T (2013) Local versus
global lessons for defect prediction and effort estimation. IEEE Trans Softw Eng 39(6):822–834
Minku LL, Yao X (2014) How to make best use of cross-company data in software effort estimation? In:
Proc. Int’l Conf. on Software Engineering (ICSE’14), pp 446–456
Mısırlı AT, Bener AB, Turhan B (2011) An industrial case study of classifier ensembles for locating software
defects. Softw Qual J 19(3):515–536
Mockus A (2009) Amassing and indexing a large sample of version control systems: Towards the census of
public source code history. In: Proc. Int’l Working Conf. on Mining Software Repositories (MSR’09),
pp 11–20
Mockus A, Weiss DM (2000) Predicting risk of software changes. Bell Labs Tech J 5(2):169–180
Moser R, Pedrycz W, Succi G (2008) A comparative analysis of the efficiency of change metrics and static
code attributes for defect prediction. In: Proc. Int’l Conf. on Softw. Eng. (ICSE’08), 181–190
Nagappan N, Ball T (2005) Use of relative code churn measures to predict system defect density. In: Proc.
Int’l Conf. on Softw. Eng. (ICSE’05), pp 284–292
Nagappan N, Ball T, Zeller A (2006) Mining metrics to predict component failures. In: Proc. Int’l Conf. on
Softw. Eng. (ICSE’06), pp 452–461
Nam J, Pan SJ, Kim S (2013) Transfer defect learning. In: Proc. Int’l Conf. on Softw. Eng. (ICSE’13),
pp 382–391
Purushothaman R, Perry DE (2005) Toward understanding the rhetoric of small source code changes. IEEE
Trans Softw Eng 31(6):511–526
Rahman F, Posnett D, Devanbu P (2012) Recalling the ”imprecision” of cross-project defect prediction. In:
Proc. Int’l Symposium on the Foundations of Softw. Eng. (FSE’12), pp 61:1–61:11
Ratzinger J, Sigmund T, Gall HC (2008) On the relation of refactorings and software defect prediction. In:
Proc. Int’l Working Conf. on Mining Software Repositories (MSR’08), pp 35–38
Shihab E (2012) An exploration of challenges limiting pragmatic software defect prediction. PhD thesis,
Queen’s University
Shihab E, Hassan AE, Adams B, Jiang ZM (2012) An industrial study on the risk of software changes. In:
Proc. Int’l Symposium on the Foundations of Softw. Eng. (FSE’12), pp 62:1–62:11
Sliwerski J, Zimmermann T, Zeller A (2005) When do changes induce fixes? In: Proc. Int’l Working Conf.
on Mining Software Repositories (MSR’05), pp 1–5
Tan M, Tan L, Dara S, Mayuex C (2015) Online defect prediction for imbalanced data. In: Proc. Int’l Conf.
on Softw. Eng. (ICSE’13 SEIP), (To appear)
Thomas SW, Nagappan M, Blostein D, Hassan AE (2013) The impact of classifier configuration and
classifier combination on bug localization. IEEE Trans Softw Eng 39(10):1427–1443
Turhan B (2012) On the dataset shift problem in software engineering prediction models. Empirical Softw
Engg 17(1-2):62–74
Turhan B, Menzies T, Bener AB, Di Stefano J (2009) On the relative value of cross-company and withincompany
data for defect prediction. Empir Softw Eng 14(5):540–578
Turhan B, Tosun A, Bener A (2011) Empirical evaluation of mixed-project defect prediction models. In: Proc.
EUROMICRO Conf. on Software Engineering and Advanced Applications (SEAA’11), pp 396–403
Wu R, Zhang H, Kim S, Cheung SC (2011) Relink: recovering links between bugs and changes. In: Proc.
European Softw. Eng. Conf. and Symposium on the Foundations of Softw. Eng. (ESEC/FSE’11), pp 15–
25
Zhang F, Mockus A, Zou Y, Khomh F, Hassan AE (2013) How does context affect the distribution of
software maintainability metrics? In: Proc. Int’l Conf. on Software Maintenance (ICSM’13), pp 350–359
Zhang F, Mockus A, Keivanloo I, Zou Y (2014) Towards building a universal defect prediction model. In:
Proc. Int’l Working Conf. on Mining Software Repositories (MSR’14), pp 182–191
Zimmermann T, Nagappan N, Gall H, Giger E, Murphy B (2009) Cross-project defect prediction: a large
scale experiment on data vs. domain vs. process. In: Proc. European Softw. Eng. Conf. and Symposium
on the Foundations of Softw. Eng. (ESEC/FSE’09), pp 91–100



>><[N]>Supervised vs Unsupervised Models: A Holistic Look at Effort-Aware Just-in-Time Defect Prediction
[1] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Software Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[2] X. Xia, D. Lo, S. J. Pan, N. Nagappan, and X. Wang, “Hydra: Massively
compositional model for cross-project defect prediction,” IEEE
Transactions on Software Engineering, vol. 42, no. 10, pp. 977–998,
2016.
[3] T. Gyimothy, R. Ferenc, and I. Siket, “Empirical validation of objectoriented
metrics on open source software for fault prediction,” IEEE
Transactions on Software engineering, vol. 31, no. 10, pp. 897–910,
2005.
[4] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in Proceedings of the 31st International Conference on Software Engineering.
IEEE Computer Society, 2009, pp. 78–88.
[5] P. L. Li, J. Herbsleb, M. Shaw, and B. Robinson, “Experiences and results
from initiating field defect prediction and product test prioritization
efforts at abb inc.” in Proceedings of the 28th international conference
on Software engineering. ACM, 2006, pp. 413–422.
[6] J. C. Munson and T. M. Khoshgoftaar, “The detection of fault-prone
programs,” IEEE Transactions on Software Engineering, vol. 18, no. 5,
pp. 423–433, 1992.
[7] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[8] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[9] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-project defect prediction: a large scale experiment on data vs.
domain vs. process,” in Proceedings of the the 7th joint meeting of
the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering. ACM, 2009,
pp. 91–100.
[10] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: simple unsupervised models
could be better than supervised models,” in Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 2016, pp. 157–168.
[11] G. Koru, H. Liu, D. Zhang, and K. El Emam, “Testing the theory
of relative defect proneness for closed-source software,” Empirical
Software Engineering, vol. 15, no. 6, pp. 577–598, 2010.
[12] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
IEEE Transactions on Software Engineering, vol. 38, no. 6, pp.
1276–1304, 2012.
[13] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in Automated
Software Engineering (ASE), 2013 IEEE/ACM 28th International
Conference on. IEEE, 2013, pp. 279–289.
[14] F. Rahman and P. Devanbu, “How, and why, process metrics are
better,” in Proceedings of the 2013 International Conference on Software
Engineering. IEEE Press, 2013, pp. 432–441.
[15] A. N. Meyer, T. Fritz, G. C. Murphy, and T. Zimmermann, “Software
developers’ perceptions of productivity,” in Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 2014, pp. 19–29.
[16] C. Parnin and A. Orso, “Are automated debugging techniques actually
helping programmers?” in Proceedings of the 2011 international symposium
on software testing and analysis. ACM, 2011, pp. 199–209.
[17] P. S. Kochhar, X. Xia, D. Lo, and S. Li, “Practitioners’ expectations on
automated fault localization,” in Proceedings of the 25th International
Symposium on Software Testing and Analysis. ACM, 2016, pp. 165–
176.
[18] E. Arisholm, L. C. Briand, and M. Fuglerud, “Data mining techniques
for building fault-proneness models in telecom java software,” in The
18th IEEE International Symposium on Software Reliability (ISSRE’07).
IEEE, 2007, pp. 215–224.
[19] F. Rahman, D. Posnett, and P. Devanbu, “Recalling the imprecision of
cross-project defect prediction,” in Proceedings of the ACM SIGSOFT
20th International Symposium on the Foundations of Software Engineering.
ACM, 2012, p. 61.
[20] E. Shihab, A. Ihara, Y. Kamei, W. M. Ibrahim, M. Ohira, B. Adams,
A. E. Hassan, and K.-i. Matsumoto, “Studying re-opened bugs in open
source software,” Empirical Software Engineering, vol. 18, no. 5, pp.
1005–1042, 2013.
[21] H. Valdivia Garcia and E. Shihab, “Characterizing and predicting
blocking bugs in open source projects,” in Proceedings of the 11th
working conference on mining software repositories. ACM, 2014, pp.
72–81.
[22] A. G. Koru, D. Zhang, K. El Emam, and H. Liu, “An investigation into
the functional form of the size-defect relationship for software modules,”
IEEE Transactions on Software Engineering, vol. 35, no. 2, pp. 293–304,
2009.
[23] S. Kim, E. J. Whitehead Jr, and Y. Zhang, “Classifying software changes:
Clean or buggy?” IEEE Transactions on Software Engineering, vol. 34,
no. 2, pp. 181–196, 2008.
[24] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and L. Bairavasundaram, “How
do fixes become bugs?” in Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of software
engineering. ACM, 2011, pp. 26–36.
[25] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering. ACM, 2012, p. 62.
[26] T. Menzies and J. S. Di Stefano, “How good is your blind spot sampling
policy,” in High Assurance Systems Engineering, 2004. Proceedings.
Eighth IEEE International Symposium on. IEEE, 2004, pp. 129–138.
[27] T. Mende and R. Koschke, “Effort-aware defect prediction models,” in
Software Maintenance and Reengineering (CSMR), 2010 14th European
Conference on. IEEE, 2010, pp. 107–116.
[28] E. Arisholm, L. C. Briand, and E. B. Johannessen, “A systematic and
comprehensive investigation of methods to build and evaluate fault
prediction models,” Journal of Systems and Software, vol. 83, no. 1,
pp. 2–17, 2010.
[29] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect prediction from static code features: current results, limitations,
new approaches,” Automated Software Engineering, vol. 17, no. 4, pp.
375–407, 2010.
[30] M. Hamill and K. Goseva-Popstojanova, “Common trends in software
fault and failure data,” IEEE Transactions on Software Engineering,
vol. 35, no. 4, pp. 484–496, 2009.
[31] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Where the bugs are,”
in ACM SIGSOFT Software Engineering Notes, vol. 29, no. 4. ACM,
2004, pp. 86–96.
[32] M. Yan, Y. Fang, D. Lo, X. Xia, and X. Zhang, “File-level defect
prediction: Unsupervised vs. supervised models,” in Proceedings of
the 11th ACM/IEEE International Symposium on Empirical Software
Engineering and Measurement. ACM, 2017, p. to appear.
[33] M. D’Ambros, M. Lanza, and R. Robbes, “An extensive comparison
of bug prediction approaches,” in Mining Software Repositories (MSR),
2010 7th IEEE Working Conference on. IEEE, 2010, pp. 31–41.
[34] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in Proceedings of the 28th international conference
on Software engineering. ACM, 2006, pp. 452–461.
[35] R. Moser, W. Pedrycz, and G. Succi, “A comparative analysis of
the efficiency of change metrics and static code attributes for defect
prediction,” in Proceedings of the 30th international conference on
Software engineering. ACM, 2008, pp. 181–190.
[36] N. Nagappan and T. Ball, “Use of relative code churn measures to
predict system defect density,” in Software Engineering, 2005. ICSE
2005. Proceedings. 27th International Conference on. IEEE, 2005, pp.
284–292.
[37] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting fault
incidence using software change history,” IEEE Transactions on software
engineering, vol. 26, no. 7, pp. 653–661, 2000.
[38] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy, “Characterizing
and predicting which bugs get fixed: an empirical study of microsoft
windows,” in Software Engineering, 2010 ACM/IEEE 32nd International
Conference on, vol. 1. IEEE, 2010, pp. 495–504.
[39] R. Purushothaman and D. E. Perry, “Toward understanding the rhetoric
of small source code changes,” IEEE Transactions on Software Engineering,
vol. 31, no. 6, pp. 511–526, 2005.
[40] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and M. Nakamura,
“An analysis of developer metrics for fault prediction,” in Proceedings
of the 6th International Conference on Predictive Models in
Software Engineering. ACM, 2010, p. 18.
[41] X. Yang, D. Lo, X. Xia, and J. Sun, “Tlel: A two-layer ensemble learning
approach for just-in-time defect prediction,” Information and Software
Technology, vol. 87, pp. 206–220, 2017.
[42] X. Xia, D. Lo, X. Wang, and X. Yang, “Collective personalized
change classification with multiobjective search,” IEEE Transactions on
Reliability, vol. 65, no. 4, pp. 1810–1829, 2016.
[43] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for justin-
time defect prediction,” in Software Quality, Reliability and Security
(QRS), 2015 IEEE International Conference on. IEEE, 2015, pp. 17–
26.
[44] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques.
Elsevier, 2011.
[45] X. Xia, L. Bao, D. Lo, and S. Li, “automated debugging considered
harmful considered harmful: A user study revisiting the usefulness
of spectra-based fault localization techniques with professionals using
real bugs from large systems,” in Software Maintenance and Evolution
(ICSME), 2016 IEEE International Conference on. IEEE, 2016, pp.
267–278.
[46] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics
bulletin, vol. 1, no. 6, pp. 80–83, 1945.
[47] H. Abdi, “Bonferroni and ˇsid´ak corrections for multiple comparisons,”
Encyclopedia of measurement and statistics, vol. 3, pp. 103–107, 2007.
[48] N. Cliff, Ordinal methods for behavioral data analysis. Lawrence
Erlbaum Associates, 1996.
[49] J. L. Hintze and R. D. Nelson, “Violin plots: a box plot-density trace
synergism,” The American Statistician, vol. 52, no. 2, pp. 181–184, 1998.
[50] W. Fu and T. Menzies, “Revisiting unsupervised learning for defect prediction,”
in Proceedings of the 2017 25th ACM SIGSOFT International
Symposium on Foundations of Software Engineering. ACM, 2017, p.
to appear.
[51] C. Tantithamthavorn, “Towards a better understanding of the impact
of experimental components on defect prediction modelling,” in Proceedings
of the 38th International Conference on Software Engineering
Companion. ACM, 2016, pp. 867–870.
[52] T. Menzies and M. Shepperd, “Special issue on repeatable results
in software engineering prediction,” Empirical Software Engineering,
vol. 17, no. 1-2, pp. 1–17, 2012.
[53] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“An empirical comparison of model validation techniques for defect
prediction models,” IEEE Transactions on Software Engineering, vol. 43,
no. 1, pp. 1–18, 2017.
[54] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of
classification techniques on the performance of defect prediction models,”
in Proceedings of the 37th International Conference on Software
Engineering-Volume 1. IEEE Press, 2015, pp. 789–800.
[55] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“Automated parameter optimization of classification techniques for
defect prediction models,” in Proceedings of the 38th International
Conference on Software Engineering. ACM, 2016, pp. 321–332.
[56] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and K. Matsumoto,
“The impact of mislabelling on the performance and interpretation
of defect prediction models,” in Software Engineering (ICSE), 2015
IEEE/ACM 37th IEEE International Conference on, vol. 1. IEEE, 2015,
pp. 812–823.



>><A(Y)>The impact of context metrics on just-in-time defect prediction
Aversano L, Cerulo L, Del Grosso C (2007) Learning from bug-introducing changes to prevent fault prone
code. In: Proceedings of the 9th international workshop on principles of software evolution (IWPSE), pp
19–26. ACM
Basili VR, Briand LC, Melo WL (1996) A validation of object-oriented design metrics as quality indicators.
IEEE Trans Softw Eng 22(10):751–761
Bettenburg N, Nagappan M, Hassan AE (2012) Think locally, act globally: Improving defect and effort
prediction models. In: Proceedings of the 9th IEEE working conference on mining software repositories
(MSR), pp 60–69. IEEE Press
Boughorbel S, Jarray F, El-Anbari M (2017) Optimal classifier for imbalanced data using matthews
correlation coefficient metric. PloS One 12(6):e0177,678
Bowes D, Hall T, Gray D (2012) Comparing the performance of fault prediction models which report multiple
performance measures: recomputing the confusion matrix. In: Proceedings of the 8th international
conference on predictive models in software engineering, pp 109–118. ACM
Chicco D (2017) Ten quick tips for machine learning in computational biology. BioData Mining 10(1):35
Cohen J (1988) Statistical power analysis for the behavioral sciences
D’Ambros M, Lanza M, Robbes R (2010) An extensive comparison of bug prediction approaches. In:
Proceedings of the 7th working conference on mining software repositories (MSR), pp 31–41. IEEE
Farrar DE, Glauber RR (1967) Multicollinearity in regression analysis: the problem revisited. Rev Econ Stat
49(1):92–107
Fukushima T, Kamei Y, McIntosh S, Yamashita K, Ubayashi N (2014) An empirical study of just-in-time
defect prediction using cross-project models. In: Proceedings of the 11th working conference on mining
software repositories (MSR), pp 172–181. ACM
Ghotra B, McIntosh S, Hassan AE (2015) Revisiting the impact of classification techniques on the performance
of defect prediction models. In: Proceedings of the 37th international conference on software
engineering (ICSE), pp 789–800. IEEE Press
Graves TL, Karr AF,Marron JS, Siy H (2000) Predicting fault incidence using software change history. IEEE
Trans Softw Eng 26(7):653–661
Hall T, Beecham S, Bowes D, Gray D, Counsell S (2012) A systematic literature review on fault prediction
performance in software engineering. IEEE Trans Softw Eng 38(6):1276–1304
Halstead MH (1977) Elements of software science. Elsevier, New York
Han J, Moraga C (1995) The influence of the sigmoid function parameters on the speed of backpropagation
learning. In: Proceedings of the international workshop on artificial neural networks, pp 195–201.
Springer
Hassan AE (2009) Predicting faults using the complexity of code changes. In: Proceedings of the 31st
international conference on software engineering (ICSE), pp 78–88. IEEE
Hata H, Mizuno O, Kikuno T (2012) Bug prediction based on fine-grained module histories. In: Proceedings
of the 34th international conference on software engineering (ICSE), pp 200–210. IEEE
Hindle A, Godfrey MW, Holt RC (2008) Reading beside the lines: Indentation as a proxy for complexity
metric. In: Proceedings of the 16th international conference on program comprehension (ICPC), pp
133–142. IEEE
Ho TK (1995) Random decision forests. In: Proceedings of the 3rd international conference on document
analysis and recognition, vol 1, pp 278–282. IEEE
Jiang T, Tan L, Kim S (2013) Personalized defect prediction. In: Proceedings of the 28th international
conference on automated software engineering (ASE), pp 279–289. IEEE
Kamei Y, Fukushima T, McIntosh S, Yamashita K, Ubayashi N, Hassan AE (2016) Studying just-in-time
defect prediction using cross-project models. Empir Softw Eng 21(5):2072–2106
Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A large-scale empirical
study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773
Karunanithi N (1993) A neural network approach for software reliability growth modeling in the presence
of code churn. In: Proceedings of the 4th international symposium on software reliability engineering,
pp 310–317. IEEE
Khoshgoftaar TM, Allen EB, Goel N, Nandi A, McMullan J (1996) Detection of software modules with high
debug code churn in a very large legacy system. In: Proceedings of the 7th international symposium on
software reliability engineering, pp 364–371. IEEE
Khoshgoftaar TM, Szabo RM (1994) Improving code churn predictions during the system test and maintenance
phases. In: Proceedings of the international conference on software maintenance (ICSM),
pp 58–67. IEEE
Kim S, Whitehead Jr E J (2006) How long did it take to fix bugs? In: Proceedings of the 2006 international
workshop on Mining software repositories (MSR), pp 173–174. ACM
Kim S, Whitehead Jr E J, Zhang Y (2008) Classifying software changes: Clean or buggy? IEEE Trans Softw
Eng 34(2):181–196
Kim S, Zhang H, Wu R, Gong L (2011) Dealing with noise in defect prediction. In: Proceedings of the 33th
international conference on software engineering (ICSE), pp 481–490. IEEE
Kim S, Zimmermann T, Whitehead Jr E J, Zeller A (2007) Predicting faults from cached history. In:
Proceedings of the 29th international conference on software engineering (ICSE), pp 489–498. IEEE
Lessmann S, Baesens B, Mues C, Pietsch S (2008) Benchmarking classification models for software defect
prediction: A proposed framework and novel findings. IEEE Trans Softw Eng 34(4):485–496
Li J, He P, Zhu J, Lyu MR (2017) Software defect prediction via convolutional neural network. In:
Proceedings of the 2017 software quality, reliability and security (QRS), pp 318–328. IEEE
McCabe TJ (1976) A complexity measure. IEEE Trans Softw Eng 2(4):308–320
McDonald JH (2014) Handbook of biological statistics, 3rd edn. Sparky House Publishing, Baltimore
Menzies T, Milton Z, Turhan B, Cukic B, Jiang Y, Bener A (2010) Defect prediction from static code
features: current results, limitations, new approaches. Autom Softw Eng 17(4):375–407
Microsoft (2016) Overview of c++ statements. https://docs.microsoft.com/ja-jp/cpp/cpp/overview-of-cppstatements
Mizuno O, Kikuno T (2007) Training on errors experiment to detect fault-prone software modules by spam
filter. In: Proceedings of the 6th joint meeting on foundations of software engineering (ESEC/FSE), pp
405–414. ACM
Mockus A, Votta LG (2000) Identifying reasons for software changes using historic databases. In:
Proceedings of the 22th international conference on software maintenance (ICSE), pp 120–130. IEEE
Moser R, Pedrycz W, Succi G (2008) A comparative analysis of the efficiency of change metrics and static
code attributes for defect prediction. In: Proceedings of the 30th international conference on software
engineering (ICSE), pp 181–190. IEEE
Munson JC, Elbaum SG (1998) Code churn: A measure for estimating the impact of code change. In:
Proceedings of the international conference on software maintenance, pp 24–31. IEEE
Nagappan N, Ball T (2005) Use of relative code churn measures to predict system defect density. In:
Proceedings of the 27th international conference on Software engineering, pp 284–292. ACM
Ohlsson MC, Von Mayrhauser A, McGuire B, Wohlin C (1999) Code decay analysis of legacy software
through successive releases. In: Proceedings of the aerospace conference, proceedings, pp 69–81. IEEE
Oram A, Wilson G (2010) Making software: What really works, and why we believe it. ” O’Reilly Media
Inc.”
Ostrand TJ, Weyuker EJ, Bell RM (2004) Where the bugs are. In: ACM SIGSOFT Software Engineering
Notes, vol 29, pp 86–96. ACM
Quinlan R (1993) C4.5: Programs for machine learning. Morgan Kaufmann Publishers
Rice ME, Harris GT (2005) Comparing effect sizes in follow-up studies: Roc area, cohen’s d, and r. Law
Hum Behav 29(5):615–620
Romanski P, Kotthoff L (2018) Fselector
Rosen C, Grawi B, Shihab E (2015) Commit guru: Analytics and risk prediction of software commits. In:
Proceedings of the 10th joint meeting on foundations of software engineering, ESEC/FSE, pp 966–969.
ACM
Shannon CE (1948) A mathematical theory of communication. Bell Syst Tech J 27(3):379–423
Shepperd M, Bowes D, Hall T (2014) Researcher bias: The use of machine learning in software defect
prediction. IEEE Trans Softw Eng 40(6):603–616
Shihab E (2012) An exploration of challenges limiting pragmatic software defect prediction. Queen’s
University (Canada), Ph.D. thesis
Shihab E, Hassan AE, Adams B, Jiang ZM (2012) An industrial study on the risk of software changes. In:
Proceedings of the 20th international symposium on the foundations of software engineering (FSE), p.
62. ACM
Sliwerski J., Zimmermann T, Zeller A (2005) When do changes induce fixes? In: Proceedings of the 2th
international workshop on mining software repositories (MSR), 4, pp 1–5. ACM
Stevenson A, Lindberg CA (2010) New Oxford American dictionary. Oxford University Press, Oxford
Tan M, Tan L, Dara S, Mayeux C (2015) Online defect prediction for imbalanced data. In: Proceedings of
the 37th international conference on software engineering (ICSE), pp 99–108. IEEE
Tantithamthavorn C, Hassan AE (2018) An experience report on defect modelling in practice: Pitfalls and
challenges. In: Proceedings of the 40th international conference on software engineering: Software
engineering in practice track (ICSE-SEIP’18), p To Appear
Tantithamthavorn C, McIntosh S, Hassan AE, Matsumoto K (2016) Automated parameter optimization
of classification techniques for defect prediction models. In: Proceedings of the 38th international
conference on software engineering (ICSE), pp 321–332. ACM
Tantithamthavorn C, McIntosh S, Hassan AE, Matsumoto K (2017) An empirical comparison of model
validation techniques for defect prediction models. IEEE Trans Softw Eng 43(1):1–18
Tassey G (2002) The economic impacts of inadequate infrastructure for software testing. National Institute
of Standards and Technology
Thomas WS (2015) lscp: A lightweight source code preprocesser
Wang S, Liu T, Tan L (2016) Automatically learning semantic features for defect prediction. In: Proceedings
of the 38th international conference on software engineering (ICSE), pp 297–308. ACM
Yang X, Lo D, Xia X, Zhang Y, Sun J (2015) Deep learning for just-in-time defect prediction. In: Proceedings
of the 2015 software quality, reliability and security (QRS), pp 17–26. IEEE
Zhang F, Zheng Q, Zou Y, Hassan AE (2016) Cross-project defect prediction using a connectivity-based
unsupervised classifier. In: Proceedings of the 38th international conference on software engineering
(ICSE), pp 309–320. ACM
Zimmermann T, Premraj R, Zeller A (2007) Predicting defects for eclipse. In: Proceedings of the 3th
international workshop on predictor models in software engineering (PROMISE), pp 9–19. IEEE
Zwillinger D, Kokoska S (1999) CRC standard probability and statistics tables and formulae. CRC Press



>><[N]>The Impact of Duplicate Changes on Just-in-Time Defect Prediction
[1] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature reviewon fault prediction performance in software engineering,”
IEEE Trans. Softw. Eng., vol. 38, no. 6, pp. 1276–1304, Nov. 2012.
[2] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and K.
Matsumoto, “The impact of mislabelling on the performance and interpretation
of defect prediction models,” in Proc. 2015 IEEE/ACM 37th
IEEE Int. Conf. Softw. Eng., Florence, Italy, 2015, pp. 812–823.
[3] Y. Kamei and E. Shihab, “Defect prediction: Accomplishments and future
challenges,” in Proc. Leaders Tomorrow Symp.: Future Softw. Eng., 2016,
pp. 33–45.
[4] S. Kim, E. J. Whitehead, Jr., and Y. Zhang, “Classifying software changes:
Clean or buggy?,” IEEE Trans. Softw. Eng., vol. 34, no. 2, pp. 181–196,
Mar. 2008.
[5] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Tech. J., vol. 5, no. 2, pp. 169–180, Apr. 2000.
[6] Y. Kamei et al., “A large-scale empirical study of just-in-time quality
assurance,” IEEE Trans. Softw. Eng., vol. 39, no. 6, pp. 757–773, Jun. 2013.
[7] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using crossproject
models,” Empirical Softw. Eng., vol. 21, no. 5, pp. 2072–2106,
Oct. 2016
[8] M.Yan, X. Xia,Y. Fan, A. E. Hassan, D. Lo, and S. Li, “Just-in-time defect
identification and localization: A two-phase framework,” IEEE Trans.
Softw. Eng., early access, doi: 10.1109/TSE.2020.2978819.
[9] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in Proc. IEEE 31st Int. Conf. Softw. Eng., May 2009, pp. 78–88.
[10] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking classification
models for software defect prediction: A proposed framework
and novel findings,” IEEE Trans. Softw. Eng., vol. 34, no. 4, pp. 485–496,
Jul. 2008.
[11] T. Menzies et al., “Local versus global lessons for defect prediction and
effort estimation,” IEEE Trans. Softw. Eng., vol. 39, no. 6, pp. 822–834,
Jun. 2013.
[12] X. Xia, D. Lo, S. J. Pan, N. Nagappan, and X.Wang, “HYDRA: Massively
compositional model for cross-project defect prediction,” IEEE Trans.
Softw. Eng., vol. 42, no. 10, pp. 977–998, Oct. 2016.
[13] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton, D. M. Germán, and P. T.
Devanbu, “The promises and perils of mining git,” in Proc. 6th IEEE Int.
Work. Conf. Mining Softw. Repositories, 2009, pp. 1–10.
[14] V.Kovalenko, F. Palomba, and A. Bacchelli, “Mining file histories: Should
we consider branches?” in Proc. 33rd ACM/IEEE Int. Conf. Automated
Softw. Eng., 2018, pp. 202–213.
[15] T. Dhaliwal, F. Khomh, Y. Zou, and A. E. Hassan, “Recovering commit
dependencies for selective code integration in software product lines,” in
Proc. 28th IEEE Int. Conf. Softw. Maintenance, 2012, pp. 202–211.
[16] X.Yang, D. Lo, X. Xia, and J. Sun, “TLEL:Atwo-layer ensemble learning
approach for just-in-time defect prediction,” Inf. Softw. Technol., vol. 87,
pp. 206–220, 2017.
[17] Y. Fan, X. Xia, D. Alencar da Costa, D. Lo, A. E. Hassan, and S. Li, “The
impact of changes mislabeled by SZZ on just-in-time defect prediction,”
IEEE Trans. Softw. Eng., early access, doi: 10.1109/TSE.2019.2929761.
[18] E. C. Neto, D. A. da Costa, and U. Kulesza, “The impact of refactoring
changes on the SZZ algorithm: An empirical study,” in Proc. IEEE 25th
Int. Conf. Softw. Analysis, Evol. Reeng., 2018, pp. 380–390.
[19] D. W. Hosmer, Jr., S. Lemeshow, and R. X. Sturdivant, Applied Logistic
Regression, 3rd ed. Hoboken, NJ, USA:Wiley, 2013. [Online]. Available:
https://www.wiley.com/en-us/Applied Logistic Regression
[20] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32,
2001.
[21] P. Domingos and M. Pazzani, “On the optimality of the simple Bayesian
classifier under zero-one loss,” Mach. Learn., vol. 29, no. 2, pp. 103–130,
Nov. 1997.
[22] J. Huang and C. X. Ling, “Using AUC and accuracy in evaluating learning
algorithms,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 3, pp. 299–310,
Mar. 2005.
[23] P. Baldi, S. Brunak, Y. Chauvin, C. A. F. Andersen, and H. Nielsen,
“Assessing the accuracy of prediction algorithms for classification: An
overview,” Bioinformatics, vol. 16, no. 5, pp. 412–424, 2000.
[24] Q. Song, Y. Guo, and M. Shepperd, “A comprehensive investigation of the
role of imbalanced learning for software defect prediction,” IEEE Trans.
Softw. Eng., vol. 45, no. 12, pp. 1253–1269, Dec. 2019.
[25] Q. Huang, X. Xia, and D. Lo, “Supervised vs unsupervised models: A
holistic look at effort-aware just-in-time defect prediction,” in Proc. IEEE
Int. Conf. Softw. Maintenance Evol., Sep. 2017, pp. 159–170.
[26] W. Fu and T. Menzies, “Revisiting unsupervised learning for defect prediction,”
2017, arXiv:1703.00132.
[27] H. Li, H. Xu, C. Zhou, X. Lu, and Z. Han, “Joint optimization strategy
of computation offloading and resource allocation in multi-access edge
computing environment,” IEEE Trans. Veh. Technol., vol. 69, no. 9,
pp. 10214–10226, Sep. 2020.
[28] J. Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?,” ACM SIGSOFT Softw. Eng. Notes, vol. 30, no. 4, pp. 1–5,
May 2005.
[29] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for just-intime
defect prediction,” in Proc. IEEE Int. Conf. Softw. Qual., Rel. Secur.,
Aug. 2015, pp. 17–26.
[30] S. McIntosh andY. Kamei, “[Journal first] are fix-inducing changes a moving
target?: A longitudinal case study of just-in-time defect prediction,” in
Proc. IEEE/ACM 40th Int. Conf. Softw. Eng., May 2018, pp. 560–560.
[31] Y. Yang et al., “Effort-aware just-in-time defect prediction: Simple unsupervised
models could be better than supervised models,” in Proc.
24th ACM SIGSOFT Int. Symp. Found. Softw. Eng., Nov. 13–18, 2016,
pp. 157–168.
[32] A. Bachmann, C. Bird, F. Rahman, P. T. Devanbu, and A. Bernstein, “The
missing links: Bugs and bug-fix commits,” in Proc. 18th ACM SIGSOFT
Int. Symp. Found. Softw. Eng., 2010, pp. 97–106.
[33] C. Bird et al., “Fair and balanced?: Bias in bug-fix datasets,” in Proc. 7th
Joint Meeting Eur. Softw. Eng. Conf., 2009, pp. 121–130.
[34] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in defect
prediction,” in Proc. 33rd Int. Conf. Softw. Eng., 2011, pp. 481–490.
[35] K. Herzig, S. Just, and A. Zeller, “It’s not a bug, it’s a feature: How
misclassification impacts bug prediction,” in Software-Engineering and
Management 2015, Multikonferenz Der GI-Fachbereiche Softwaretechnik
(SWT) Und Wirtschaftsinformatik (WI), FA WI-MAW, Dresden, Germany,
März 17–20, 2015, pp. 103–104.
[36] D. A. da Costa, S. McIntosh,W. Shang, U. Kulesza, R. Coelho, and A. E.
Hassan, “A framework for evaluating the results of the SZZ approach for
identifying bug-introducing changes,” IEEE Trans. Softw. Eng., vol. 43,
no. 7, pp. 641–657, Jul. 2017.
[37] T. Mende, “Replication of defect prediction studies: Problems, pitfalls
and recommendations,” in Proc. 6th Int. Conf. Predictive Models Softw.
Eng., T. Menzies and G. Koru, Eds. Timisoara, Romania: ACM, Sep.
12–13, 2010, p. 5.
[38] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton, D. M. German, and P.
Devanbu, “The promises and perils of mining git,” in Proc 6th IEEE Int.
Work. Conf. Mining Softw. Repositories, May 2009, pp. 1–10.
[39] A. Bacchelli, M. Lanza, and R. Robbes, “Linking e-mails and source code
artifacts,” in Proc. 32nd ACM/IEEE Int. Conf. Softw. Eng., 2010, pp. 375–
384.
[40] M. Sasaki, S. Matsumoto, and S. Kusumoto, “Integrating source code
search into git client for effective retrieving of change history,” in
Proc. IEEE Workshop Mining Analyzing Interact. Hist., Mar. 2018,
pp. 1–5.
[41] F. Qiu et al., “JITO: A tool for just-in-time defect identification and
localization,” in Proc. 28th ACMJoint Eur. Softw. Eng. Conf. Symp. Found.
Softw. Eng., P. Devanbu, M. Cohen, and T. Zimmermann, Eds. 2020,
pp. 1586–1590.
[42] M. Yan, X. Xia, Y. Fan, D. Lo, A. E. Hassan, and X. Zhang, “Effort-aware
just-in-time defect identification in practice: A case study at Alibaba,” in
Proc. 28th ACMJoint Euro. Softw. Eng. Conf. Symp. Found. Softw. Eng., P.
Devanbu, M. B. Cohen, and T. Zimmermann, Eds. 2020, pp. 1308–1319.
[43] C. Tantithamthavorn and A. E. Hassan, “An experience report on defect
modelling in practice: Pitfalls and challenges,” in Proc. 40th Int. Conf.
Softw. Eng.: Softw. Eng. Pract. 2018, pp. 286–295.
[44] J. H. Zar, “Spearman rank correlation,” in Encyclopedia of Biostatistics,
vol. 7. New Jersey, NY, USA: Wiley, 2005.
[45] R. L. Plackett, “Karl Pearson and the chi-squared test,” Int. Stat. Rev./Revue
Int. Statistique, vol. 51, no. 1, pp. 59–72, 1983.
[46] F. E. H. Jr, “RMS: Regression modeling strategies,” R Package Version
5.1-3, 2019.
[47] G. K. Rajbahadur, S. Wang, Y. Kamei, and A. E. Hassan, “The impact
of using regression models to build defect classifiers,” in Proc. 14th Int.
Conf. Mining Softw. Repositories, 2017, pp. 135–145.
[48] J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, “The impact of
correlated metrics on the interpretation of defect models,” IEEE Trans.
Softw. Eng., vol. 47, no. 2, pp. 320–331, Feb. 2021.
[49] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The impact of
class rebalancing techniques on the performance and interpretation of
defect prediction models,”, 2018, arXiv:1801.10269.
[50] V.Kovalenko, F. Palomba, and A. Bacchelli, “Mining file histories: Should
we consider branches?” in Proc. 33rd IEEE/ACM Int. Conf. Autom. Softw.
Eng., 2018, pp. 202–213.
[51] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics
Bull., vol. 1, no. 6, pp. 80–83, 1945.
[52] H. Abdi, “The Bonferonni and Sid´ak corrections for multiple comparisons,”
Encyclopedia Meas. Statist., vol. 3, pp. 103–107, 2007.
[53] T. Hoang, H. K. Dam, Y. Kamei, D. Lo, and N. Ubayashi, “Deepjit: An
end-to-end deep learning framework for just-in-time defect prediction,” in
Proc. 16th Int. Conf. Mining Softw. Repositories,, 2019, pp. 34–45.
[54] N. Cliff, Ordinal Methods for Behavioral Data Analysis. Hove, U.K.:
Psychology Press, 2014. [Online]. Available: https://doi.org/10.4324/
9781315806730
[55] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto, “An
empirical comparison of model validation techniques for defect prediction
models,” IEEE Trans. Softw. Eng., vol. 43, no. 1, pp. 1–18, Jan. 2017.
[56] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto, “The
impact of automated parameter optimization on defect prediction models,”
IEEE Trans. Softw. Eng., vol. 45, no. 7, pp. 683–711, Jul. 2019.
[57] G. K. Rajbahadur, S. Wang, Y. Kamei, and A. E. Hassan, “The impact
of using regression models to build defect classifiers,” in Proc. 2017
IEEE/ACM14th Int. Conf. Mining Softw. Repositories, 2017, pp. 135–145.
[58] B. Adams, “On software release engineering,” ICSE Technical Briefing,
2012.
[59] M. Galar, A. Fernández, E. B. Tartas, H. B. Sola, and F. Herrera, “A review
on ensembles for the class imbalance problem: Bagging-, boosting-, and
hybrid-based approaches,” IEEE Trans. Syst. Man Cybern. Part C, vol. 42,
no. 4, pp. 463–484, Jul. 2012.
[60] N. V. Chawla, K.W. Bowyer, L. O. Hall, andW. P. Kegelmeyer, “SMOTE:
Synthetic minority over-sampling technique,” J. Artif. Intell. Res., vol. 16,
no. 1, pp. 321–357, 2002.
[61] Y. S. Nugroho, H. Hata, and K. Matsumoto, “How different are different
diff algorithms in git?,” Empir. Softw. Eng., vol. 25, no. 1, pp. 790–823,
2020.
[62] G. Soares, R. Gheyi, and T. Massoni, “Automated behavioral testing of
refactoring engines,” IEEE Trans. Softw. Eng., vol. 39, no. 2, pp. 147–162,
Feb. 2013.



>><[N]>The Impact of Human Discussions on Just-In-Time Quality Assurance
[1] A. Bacchelli and C. Bird. Expectations, outcomes, and challenges of
modern code review. In Proc. of the 2013 Intl. Conf. on Software
Engineering (ICSE), pages 712–721, 2013.
[2] A. Bacchelli, M. Lanza, and R. Robbes. Linking e-mails and source
code artifacts. In Proceedings of the 32Nd ACM/IEEE International
Conference on Software Engineering - Volume 1, ICSE ’10, pages 375–
384, New York, NY, USA, 2010. ACM.
[3] V. R. Basili, L. C. Briand, and W. L. Melo. A validation of objectoriented
design metrics as quality indicators. IEEE Trans. Softw. Eng.,
22(10):751–761, 1996.
[4] G. Bavota and B. Russo. Four eyes are better than two: On the impact
of code reviews on software quality. In Proc. ICSME, pages 81–90.
IEEE, 2015.
[5] N. Bettenburg and A. E. Hassan. Studying the impact of social
interactions on software quality. Empirical Softw. Engg., 18(2):375–431,
Apr. 2013.
[6] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton, D. M. German, and
P. Devanbu. The promises and perils of mining git. In Proceedings of the
2009 6th IEEE International Working Conference on Mining Software
Repositories (MSR), pages 1–10, 2009.
[7] J. M. Bland and D. G. Altman. Transformations, means, and confidence
intervals. BMJ, 312(7038):1079, Apr. 1996.
[8] B. W. Boehm and P. N. Papaccio. Understanding and controlling
software costs. IEEE Trans. Softw. Eng., 14(10):1462–1477, Oct. 1988.
[9] M. Cataldo, A. Mockus, J. A. Roberts, and J. D. Herbsleb. Software
dependencies, work dependencies, and their impact on failures. IEEE
Trans. Softw. Eng., 35(6):864–878, Nov. 2009.
[10] M. D’Ambros, M. Lanza, and R. Robbes. Evaluating defect prediction
approaches: A benchmark and an extensive comparison. Empirical
Softw. Engg., 17(4-5):531–577, Aug. 2012.
[11] B. Efron and R. Tibshirani. Cross-validation and the bootstrap: Estimating
the error rate of a prediction rule. Technical report, Stanford
University, 1995.
[12] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi.
An empirical study of just-in-time defect prediction using cross-project
models. In Proceedings of the 11th Working Conference on Mining
Software Repositories, MSR 2014, pages 172–181, New York, NY, USA,
2014. ACM.
[13] D. Galin. Software Quality Assurance: From Theory to Implementation.
Pearson, June 2003.
[14] J. M. Gonzalez-Barahona, G. Robles, and D. Izquierdo-Cortazar. The
metricsgrimoire database collection. In 12th Working Conference on
Mining Software Repositories (MSR), pages 478–481, May 2015.
[15] D. Graziotin, X. Wang, and P. Abrahamsson. Happy software developers
solve problems better: psychological measurements in empirical
software engineering. PeerJ, page e289, 3 2014.
[16] E. Guzman, D. Az´ocar, and Y. Li. Sentiment analysis of commit
comments in github: An empirical study. In Proceedings of the 11th
Working Conference on Mining Software Repositories, MSR 2014, pages
352–355, New York, NY, USA, 2014. ACM.
[17] T. Gyimothy, R. Ferenc, and I. Siket. Empirical validation of objectoriented
metrics on open source software for fault prediction. IEEE
Trans. Softw. Eng., 31(10):897–910, Oct. 2005.
[18] A. E. Hassan. Predicting faults using the complexity of code changes.
In Proceedings of the 31st International Conference on Software Engineering,
ICSE ’09, pages 78–88, Washington, DC, USA, 2009. IEEE
Computer Society.
[19] I. Herraiz, D. M. German, J. M. Gonzalez-Barahona, and G. Robles.
Towards a simplification of the bug report form in eclipse. In Proceedings
of the 2008 International Working Conference on Mining Software
Repositories, MSR ’08, pages 145–148, New York, NY, USA, 2008.
ACM.
[20] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto, and K. ichi
Matsumoto. The effects of over and under sampling on fault-prone
module detection. In ESEM, pages 196–204. IEEE Computer Society,
2007.
[21] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi. A large-scale empirical study of just-in-time quality
assurance. IEEE Trans. Software Eng., 39(6):757–773, 2013.
[22] S. Kim, E. J. Whitehead, Jr., and Y. Zhang. Classifying software
changes: Clean or buggy? IEEE Trans. Softw. Eng., 34(2):181–196,
Mar. 2008.
[23] O. Kononenko, O. Baysal, L. Guerrouj, Y. Cao, and M. W. Godfrey.
Investigating code review quality: Do people and participation matter?
In Proc. ICSME, pages 111–120. IEEE, 2015.
[24] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch. Benchmarking classification
models for software defect prediction: A proposed framework
and novel findings. IEEE Trans. Softw. Eng., 34(4):485–496, July 2008.
[25] C. L. Mallows. Some comments on Cp. Technometrics, 15:661–675,
1973.
[26] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan. The impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects. In Proc. of the 11th Working
Conf. on Mining Software Repositories (MSR), pages 192–201, 2014.
[27] N. Mishra and C. K. Jha. Article: Classification of opinion mining
techniques. International Journal of Computer Applications, 56(13):1–
6, October 2012. Published by Foundation of Computer Science, New
York, USA.
[28] A. Mockus, R. T. Fielding, and J. D. Herbsleb. Two case studies of
open source software development: Apache and mozilla. ACM Trans.
Softw. Eng. Methodol., 11(3):309–346, July 2002.
[29] A. Mockus and D. M. Weiss. Predicting risk of software changes. Bell
Labs Technical Journal, 5:169–180, 2000.
[30] J. C. Munson and T. M. Khoshgoftaar. The detection of fault-prone
programs. IEEE Trans. Software Eng., 18(5):423–433, 1992.
[31] A. Murgia, P. Tourani, B. Adams, and M. Ortu. Do developers feel
emotions? an exploratory analysis of emotions in software artifacts.
In Proceedings of the 11th Working Conference on Mining Software
Repositories, MSR 2014, pages 262–271, New York, NY, USA, 2014.
ACM.
[32] N. Nagappan and T. Ball. Use of relative code churn measures to
predict system defect density. In Proceedings of the 27th International
Conference on Software Engineering, ICSE ’05, pages 284–292, New
York, NY, USA, 2005. ACM.
[33] M. Ortu, B. Adams, G. Destefanis, P. Tourani, M. Marchesi, and
R. Tonelli. Are bullies more productive? empirical study of affectiveness
vs. issue fixing time. In Proceedings of the 12th IEEE Working
Conference on Mining Software Repositories (MSR), Florence, Italy,
May 2015.
[34] M. Ortu, G. Destefanis, M. Kassab, S. Counsell, M. Marchesi, and
R. Tonelli. Would you mind fixing this issue? an empirical analysis of
politeness and attractiveness in software developed using agile boards.
In XP2015, Helnsiki, page in press. Springer, 2015.
[35] B. Pang and L. Lee. Opinion Mining and Sentiment Analysis. Foundations
and Trends in Information Retrieval, 2(1-2):1–135, Jan. 2008.
[36] P. C. Rigby and C. Bird. Convergent contemporary software peer review
practices. In Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering, ESEC/FSE 2013, pages 202–212, New York,
NY, USA, 2013. ACM.
[37] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang. An industrial
study on the risk of software changes. In Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering, FSE ’12, pages 62:1–62:11, New York, NY, USA, 2012.
ACM.
[38] E. Shihab, Y. Kamei, B. Adams, and A. E. Hassan. Is lines of code
a good measure of effort in effort-aware models? Information and
Software Technology, 55(11):1981–1993, 2013.
[39] J. ´ Sliwerski, T. Zimmermann, and A. Zeller. When do changes induce
fixes? In Proceedings of the 2005 International Workshop on Mining
Software Repositories, MSR ’05, pages 1–5, New York, NY, USA, 2005.
ACM.
[40] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,
and C. Potts. Recursive deep models for semantic compositionality
over a sentiment treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing, pages 1631–
1642, Stroudsburg, PA, October 2013. Association for Computational
Linguistics.
[41] M. Thelwall. Heart and soul: Sentiment strength detection in the social
web with sentistrength (summary book chapter). In press.
[42] M. Thelwall, K. Buckley, and G. Paltoglou. Sentiment strength detection
for the social web. J. Am. Soc. Inf. Sci. Technol., 63(1):163–173, Jan.
2012.
[43] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas. Sentiment
in short strength detection informal text. J. Am. Soc. Inf. Sci.
Technol., 61(12):2544–2558, Dec. 2010.
[44] P. Tourani, Y. Jiang, and B. Adams. Monitoring sentiment in open
source mailing lists -– exploratory study on the apache ecosystem. In
Proceedings of the 2014 Conference of the Center for Advanced Studies
on Collaborative Research (CASCON), Toronto, ON, Canada, November
2014.
[45] T. Wolf, A. Schroter, D. Damian, and T. Nguyen. Predicting build
failures using social network analysis on developer communication. In
Proceedings of the 31st International Conference on Software Engineering,
ICSE ’09, pages 1–11, Washington, DC, USA, 2009. IEEE
Computer Society.



>><[N]>The Impact of Mislabeled Changes by SZZ on Just-in-Time Defect Prediction
[1] Activemq/amq-1381. https://issues.apache.org/jira/browse/
AMQ-1381. Accessed: 2018-09-18.
[2] H. Abdi. Bonferroni and ˇsid´ak corrections for multiple
comparisons. Encyclopedia of measurement and statistics, 3:103–107,
2007.
[3] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh, and Y.-G.
Gu´eh´eneuc. Is it a bug or an enhancement?: a text-based approach
to classify change requests. In Proceedings of the 18th conference
of the center for advanced studies on collaborative research: meeting of
minds, page 23. ACM, 2008.
[4] A. Bachmann, C. Bird, F. Rahman, P. Devanbu, and A. Bernstein.
The missing links: bugs and bug-fix commits. In Proceedings of
the 18th ACM SIGSOFT international symposium on Foundations of
software engineering, pages 97–106. ACM, 2010.
[5] G. Bavota, B. De Carluccio, A. De Lucia, M. Di Penta, R. Oliveto,
and O. Strollo. When does a refactoring induce bugs? an empirical
study. In 2012 IEEE 12th International Working Conference on Source
Code Analysis and Manipulation, pages 104–113. IEEE, 2012.
[6] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein, V. Filkov,
and P. Devanbu. Fair and balanced?: bias in bug-fix datasets. In
Proceedings of the the 7th ACM SIGSOFT international symposium on
Foundations of software engineering, pages 121–130. ACM, 2009.
[7] T. F. Bissyande, F. Thung, S. Wang, D. Lo, L. Jiang, and
L. Reveillere. Empirical evaluation of bug linking. In Proceedings
of the 17th European Conference on Software Maintenance and
Reengineering, pages 89–98. IEEE, 2013.
[8] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[9] G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella,
and S. Panichella. Multi-objective cross-project defect prediction.
In 2013 IEEE Sixth International Conference on Software Testing,
Verification and Validation, pages 252–261. IEEE, 2013.
[10] G. Canfora, A. D. Lucia, M. D. Penta, R. Oliveto, A. Panichella, and
S. Panichella. Defect prediction as a multiobjective optimization
problem. Software Testing, Verification and Reliability, 25(4):426–459,
2015.
[11] C. Catal. Software fault prediction: A literature review and current
trends. Expert systems with applications, 38(4):4626–4636, 2011.
[12] N. Cliff. Ordinal methods for behavioral data analysis. Psychology
Press, 2014.
[13] D. A. da Costa. Ma-szz implementation for svn repositories.
https://github.com/danielcalencar/ma-szz, 2019.
[14] D. A. da Costa. Refactoring-aware szz (ra-szz) implementation.
https://github.com/danielcalencar/ra-szz, 2019.
[15] D. A. da Costa, S. McIntosh, W. Shang, U. Kulesza, R. Coelho,
and A. E. Hassan. A framework for evaluating the results of
the szz approach for identifying bug-introducing changes. IEEE
Transactions on Software Engineering, 43(7):641–657, 2017.
[16] S. Davies, M. Roper, and M. Wood. Comparing text-based
and dependence-based approaches for determining the origins of
bugs. Journal of Software: Evolution and Process, 26(1):107–139, 2014.
[17] P. Domingos and M. Pazzani. On the optimality of the simple
bayesian classifier under zero-one loss. Machine learning, 29(2-
3):103–130, 1997.
[18] B. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC
press, 1994.
[19] Y. Fan, X. Xia, D. Lo, and A. E. Hassan. Chaff from the
wheat: Characterizing and determining valid bug reports. IEEE
Transactions on Software Engineering, 2018.
[20] M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts.
Refactoring: improving the design of existing code. Addison-Wesley
Professional, 1999.
[21] W. Fu and T. Menzies. Revisiting unsupervised learning for defect
prediction. In Proceedings of the 25th ACM SIGSOFT International
Symposium on Foundations of Software Engineering, pages 72–83.
ACM, 2017.
[22] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita, and
N. Ubayashi. An empirical study of just-in-time defect prediction
using cross-project models. In Proceedings of the 11th Working
Conference on Mining Software Repositories, pages 172–181. ACM,
2014.
[23] E. Giger, M. D’Ambros, M. Pinzger, and H. C. Gall. Method-level
bug prediction. In Proceedings of the 6th international symposium
on Empirical software engineering and measurement, pages 171–180.
ACM, 2012.
[24] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy.
Characterizing and predicting which bugs get fixed: an empirical
study of microsoft windows. In Proceedings of the 32nd ACM/IEEE
International Conference on Software Engineering, pages 495–504.
ACM, 2010.
[25] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell. A
systematic literature review on fault prediction performance in
software engineering. IEEE Transactions on Software Engineering,
38(6):1276–1304, 2012.
[26] M. Hamill and K. Goseva-Popstojanova. Common trends in
software fault and failure data. IEEE Transactions on Software
Engineering, 35(4):484–496, 2009.
[27] F. E. Harrell. Regression modeling strategies: with applications to linear
models, logistic regression, and survival analysis. Springer, 2001.
[28] F. E. Harrell Jr. rms: Regression Modeling Strategies, 2019. R package
version 5.1-3.
[29] A. E. Hassan. Predicting faults using the complexity of code
changes. In Proceedings of the 31st International Conference on
Software Engineering, pages 78–88. IEEE, 2009.
[30] K. Herzig, S. Just, and A. Zeller. It’s not a bug, it’s a feature: how
misclassification impacts bug prediction. In Proceedings of the 35th
international conference on software engineering, pages 392–401. IEEE,
2013.
[31] K. Herzig, S. Just, and A. Zeller. The impact of tangled
code changes on defect prediction models. Empirical Software
Engineering, 21(2):303–336, 2016.
[32] D. W. Hosmer Jr, S. Lemeshow, and R. X. Sturdivant. Applied
logistic regression, volume 398. John Wiley & Sons, 2013.
[33] J. Huang and C. X. Ling. Using auc and accuracy in evaluating
learning algorithms. IEEE Transactions on knowledge and Data
Engineering, 17(3):299–310, 2005.
[34] Q. Huang, X. Xia, and D. Lo. Supervised vs unsupervised models:
A holistic look at effort-aware just-in-time defect prediction.
In Proceedings of the 33rd International Conference on Software
Maintenance and Evolution, pages 159–170. IEEE, 2017.
[35] T. Jiang, L. Tan, and S. Kim. Personalized defect prediction. In
Proceedings of the 28th International Conference on Automated Software
Engineering, pages 279–289. IEEE, 2013.
[36] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan. Studying just-in-time defect prediction using
cross-project models. Empirical Software Engineering, 21(5):2072–
2106, 2016.
[37] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi. A large-scale empirical study of just-in-time
quality assurance. IEEE Transactions on Software Engineering,
39(6):757–773, 2013.
[38] S. Kim, E. J. Whitehead Jr, and Y. Zhang. Classifying software
changes: Clean or buggy? IEEE Transactions on Software
Engineering, 34(2):181–196, 2008.
[39] S. Kim, H. Zhang, R. Wu, and L. Gong. Dealing with noise in
defect prediction. In Proceeding of the 33rd international conference
on Software engineering, pages 481–490. IEEE, 2011.
[40] S. Kim, T. Zimmermann, K. Pan, E. James Jr, et al. Automatic
identification of bug-introducing changes. In Proceedings of the
21st IEEE/ACM International Conference on Automated Software
Engineering, pages 81–90. IEEE, 2006.
[41] P. S. Kochhar, F. Thung, and D. Lo. Automatic fine-grained
issue report reclassification. In Proceedings of the 19th International
Conference on Engineering of Complex Computer Systems, pages 126–
135. IEEE, 2014.
[42] A. G. Koru, D. Zhang, K. El Emam, and H. Liu. An investigation
into the functional form of the size-defect relationship for software
modules. IEEE Transactions on Software Engineering, 35(2):293–304,
2009.
[43] M. Kubat, S. Matwin, et al. Addressing the curse of imbalanced
training sets: one-sided selection. In Icml, volume 97, pages 179–
186. Nashville, USA, 1997.
[44] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch. Benchmarking
classification models for software defect prediction: A proposed
framework and novel findings. IEEE Transactions on Software
Engineering, 34(4):485–496, 2008.
[45] H. Li, W. Shang, Y. Zou, and A. E. Hassan. Towards just-intime
suggestions for log changes. Empirical Software Engineering,
22(4):1831–1865, 2017.
[46] A. Liaw and M. Wiener. Classification and regression by
randomforest. R News, 2(3):18–22, 2002.
[47] M. Majka. naivebayes: High Performance Implementation of the Naive
Bayes Algorithm, 2019. R package version 0.9.3.
[48] H. B. Mann and D. R. Whitney. On a test of whether one of two
random variables is stochastically larger than the other. The annals
of mathematical statistics, pages 50–60, 1947.
[49] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and
M. Nakamura. An analysis of developer metrics for fault
prediction. In Proceedings of the 6th International Conference on
Predictive Models in Software Engineering, page 18. ACM, 2010.
[50] S. McIntosh and Y. Kamei. Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction.
IEEE Transactions on Software Engineering, 44(5):412–428, 2018.
[51] T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman, F. Shull,
B. Turhan, and T. Zimmermann. Local versus global lessons
for defect prediction and effort estimation. IEEE Transactions on
software engineering, 39(6):822–834, 2013.
[52] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener.
Defect prediction from static code features: current results,
limitations, new approaches. Automated Software Engineering,
17(4):375–407, 2010.
[53] D. Meyer, A. Zeileis, and K. Hornik. vcd: Visualizing Categorical
Data, 2017. R package version 1.4-4.
[54] A. Mockus and D. M. Weiss. Predicting risk of software changes.
Bell Labs Technical Journal, 5(2):169–180, 2000.
[55] R. Moser, W. Pedrycz, and G. Succi. A comparative analysis of the
efficiency of change metrics and static code attributes for defect
prediction. In Proceedings of the 30th international conference on
Software engineering, pages 181–190. ACM, 2008.
[56] E. C. Neto, D. A. da Costa, and U. Kulesza. The impact of
refactoring changes on the szz algorithm: An empirical study.
In Proceedings of the 25th IEEE International Conference on Software
Analysis, Evolution and Reengineering, pages 380–390. IEEE, 2018.
[57] T. J. Ostrand, E. J. Weyuker, and R. M. Bell. Where the bugs are.
In ACM SIGSOFT Software Engineering Notes, volume 29, pages
86–96. ACM, 2004.
[58] R. L. Plackett. Karl pearson and the chi-squared test. International
Statistical Review/Revue Internationale de Statistique, pages 59–72,
1983.
[59] R. Purushothaman and D. E. Perry. Toward understanding the
rhetoric of small source code changes. IEEE Transactions on Software
Engineering, 31(6):511–526, 2005.
[60] G. K. Rajbahadur, S.Wang, Y. Kamei, and A. E. Hassan. The impact
of using regression models to build defect classifiers. In Proceedings
of the 14th International Conference on Mining Software Repositories,
pages 135–145. IEEE, 2017.
[61] A. J. Scott and M. Knott. A cluster analysis method for grouping
means in the analysis of variance. Biometrics, pages 507–512, 1974.
[62] E. Shihab, Z. M. Jiang,W. M. Ibrahim, B. Adams, and A. E. Hassan.
Understanding the impact of code and process metrics on postrelease
defects: a case study on the eclipse project. In Proceedings
of the 4th International Symposium on Empirical Software Engineering
and Measurement, page 4. ACM, 2010.
[63] S. Shivaji, E. J. Whitehead, R. Akella, and S. Kim. Reducing
features to improve code change-based bug prediction. IEEE
Transactions on Software Engineering, 39(4):552–569, 2013.
[64] D. Silva and M. T. Valente. Refdiff: detecting refactorings in
version histories. In Proceedings of the 14th International Conference
on Mining Software Repositories, pages 269–279. IEEE, 2017.
[65] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do changes
induce fixes? In Proceedings of the 2nd international workshop on
Mining software repositories, volume 30, pages 1–5. ACM, 2005.
[66] G. Soares, R. Gheyi, and T. Massoni. Automated behavioral testing
of refactoring engines. IEEE Transactions on Software Engineering,
39(2):147–162, 2013.
[67] G. Soares, R. Gheyi, D. Serey, and T. Massoni. Making program
refactoring safer. IEEE software, 27(4):52–57, 2010.
[68] Q. Song, Y. Guo, and M. Shepperd. A comprehensive investigation
of the role of imbalanced learning for software defect prediction.
IEEE Transactions on Software Engineering, 2018.
[69] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online defect prediction
for imbalanced data. In 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering, volume 2, pages 99–108. IEEE,
2015.
[70] C. Tantithamthavorn and A. E. Hassan. An experience report on
defect modelling in practice: Pitfalls and challenges. In Proceedings
of the 40th International Conference on Software Engineering: Software
Engineering in Practice, pages 286–295. ACM, 2018.
[71] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto. The
impact of class rebalancing techniques on the performance
and interpretation of defect prediction models. arXiv preprint
arXiv:1801.10269, 2018.
[72] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and
K. Matsumoto. The impact of mislabelling on the performance
and interpretation of defect prediction models. In Proceedings of
the 37th International Conference on Software Engineering, pages 812–
823. IEEE, 2015.
[73] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and
K. Matsumoto. An empirical comparison of model validation
techniques for defect prediction models. IEEE Transactions on
Software Engineering, 43(1):1–18, 2017.
[74] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and
K. Matsumoto. The impact of automated parameter optimization
on defect prediction models. IEEE Transactions on Software
Engineering, 2018.
[75] S. Wang, D. Lo, and X. Jiang. Understanding widespread changes:
A taxonomic study. In Proceedings of the 17th European Conference
on Software Maintenance and Reengineering, pages 5–14. IEEE, 2013.
[76] F. Wilcoxon. Individual comparisons by ranking methods.
Biometrics bulletin, 1(6):80–83, 1945.
[77] C. Williams and J. Spacco. Szz revisited: verifying when changes
induce fixes. In Proceedings of the 2008 workshop on Defects in large
software systems, pages 32–36. ACM, 2008.
[78] C. C. Williams and J. W. Spacco. Branching and merging in the
repository. In Proceedings of the 5th international working conference
on Mining software repositories, pages 19–22. ACM, 2008.
[79] X. Xia, L. Bao, D. Lo, P. S. Kochhar, A. E. Hassan, and Z. Xing.
What do developers search for on the web? Empirical Software
Engineering, 22(6):3149–3185, 2017.
[80] X. Xia, D. Lo, S. J. Pan, N. Nagappan, and X. Wang. Hydra:
Massively compositional model for cross-project defect prediction.
IEEE Transactions on software Engineering, 42(10):977–998, 2016.
[81] X. Yang, D. Lo, X. Xia, and J. Sun. Tlel: A two-layer ensemble
learning approach for just-in-time defect prediction. Information
and Software Technology, 87:206–220, 2017.
[82] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun. Deep learning
for just-in-time defect prediction. In Proceedings of the 2015 IEEE
International Conference on Software Quality, Reliability and Security,
pages 17–26. IEEE Computer Society, 2015.
[83] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung.
Effort-aware just-in-time defect prediction: simple unsupervised
models could be better than supervised models. In Proceedings of
the 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering, pages 157–168. ACM, 2016.
[84] J. H. Zar. Spearman rank correlation. Encyclopedia of Biostatistics,
7, 2005.
[85] T. Zimmermann, S. Kim, A. Zeller, and E. J. Whitehead Jr. Mining
version archives for co-changed lines. In Proceedings of the 3rd
international workshop on Mining software repositories, pages 72–75.
ACM, 2006.






>><[N]>Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network
[1] Kamei, Y., Fukushima, T., Mcintosh, S., et al.: ‘Studying just-in-time defect
prediction using cross-project models’, Empir. Softw. Eng., 2016, 21, (5), pp.
2072–2106
[2] Huang, Q., Xia, X., Lo, D.: ‘Revisiting supervised and unsupervised models
for effort-aware just-in-time defect prediction’, Empir. Softw. Eng., 2018, 24,
(5), pp. 2823–2862
[3] Yang, X., Lo, D., Xia, X., et al.: ‘TLEL: a two-layer ensemble learning
approach for just-in-time defect prediction’, Inf. Softw. Technol., 2017, 87, pp.
206–220
[4] Kamei, Y., Shihab, E., Adams, B., et al.: ‘A large-scale empirical study of
just-in-time quality assurance’, IEEE Trans. Softw. Eng., 2013, 39, (6), pp.
757–773
[5] Zhang, Y., Chan, W., Jaitly, N.: ‘Very deep convolutional networks for end-toend
speech recognition’. 2017 IEEE Int. Conf. on Acoustics, Speech and
Signal Processing (ICASSP), New Orleans, LA, USA, September 2017
[6] Karpathy, A., Li, F.: ‘Deep visual-semantic alignments for generating image
descriptions’. Proc. IEEE Conf. on Computer Vision and Pattern Recognition,
Boston, MA, USA, June 2015
[7] Grefenstette, E., Blunsom, P.: ‘A convolutional neural network for modelling
sentences’. The 52nd Annual Meeting of the Association for Computational
Linguistics, Baltimore, Maryland, June 2014
[8] Shen, Y., He, X., Gao, J., et al.: ‘Learning semantic representations using
convolutional neural networks for web search’. Proc. 23rd Int. Conf. on World
Wide Web, Seoul, South Korea, July 2014
[9] Vincent, P., Larochelle, H., Bengio, Y., et al.: ‘Extracting and composing
robust features with denoising autoencoders’. 25th Int. Conf. on Machine
Learning, Helsinki, Finland, July 2008
[10] Mockus, A., Weiss, D.M.: ‘Predicting risk of software changes’, Bell Labs
Tech. J., 2000, 5, (2), pp. 169–180
[11] Yang, X., Lo, D., Xia, X., et al.: ‘Deep learning for just-in-time defect
prediction’. Proc. Int. Conf. on Software Quality, Reliability and Security,
Washington, DC, USA, August 2015
[12] Fukushima, T., Kamei, Y., McIntosh, S., et al.: ‘An empirical study of just-intime
defect prediction using cross-project models’. Proc. 11th Working Conf.
on Mining Software Repositories, Hyderabad, India, May 2014
[13] Shihab, E., Hassan, A.E., Adams, B., et al.: ‘An industrial study on the risk of
software changes’. Proc. ACM SIGSOFT 20th Int. Symp. on the Foundations
of Software Engineering, Cary, North Carolina, USA, November 2012
[14] Hoang, T., Dam, H.K., Kamei, Y., et al.: ‘DeepJIT: an end-to-end deep
learning framework for just-in-time defect prediction’. IEEE Int. Working
Conf. on Mining Software Repositories (MSR), Montreal, QC, Canada, May
2019
[15] Li, J., Struzik, Z., Zhang, L., et al.: ‘Feature learning from incomplete EEG
with denoising autoencoder’, Neurocomputing, 2015, 165, pp. 23–31
[16] Ferles, C., Papanikolaou, Y., Naidoo, K.J.: ‘Denoising autoencoder selforganizing
map (DASOM)’, Neural Netw., 2018, 105, pp. 112–131
[17] Vincent, P.: ‘A connection between score matching and denoising
autoencoders’, Neural Comput., 2011, 23, (7), pp. 1661–1674
[18] Wang, M., Wu, Z., Sun, X., et al.: ‘Trust-aware collaborative filtering with a
denoising autoencoder’, Neural Process. Lett., 2019, 49, (2), pp. 835–849
[19] Wang, S., Liu, T., Tan, L.: ‘Automatically learning semantic features for
defect prediction’. Proc. Int. Conf. on Software Engineering, Austin, TX,
USA, May 2016
[20] Lam, A.N., Nguyen, A.T., Nguyen, H.A., et al.: ‘Combining deep learning
with information retrieval to localize buggy files for bug reports’. Proc. Int.
Conf. on Automated Software Engineering, Lincoln, NE, USA, November
2015
[21] Huo, X., Li, M., Zhou, Z.H., et al.: ‘Learning unified features from natural
and programming languages for locating buggy source code’. Proc. Int. Joint
Conf. Artificial Intelligence, NY, USA, July 2016
[22] Gu, X., Zhang, H., Zhang, D., et al.: ‘Deep API learning’. Proc. ACM
SIGSOFT Int. Symp. on Foundations of Software Engineering, Seattle, WA,
USA, November 2016
[23] Junjie, W., Qiang, C., Song, W., et al.: ‘Domain adaptation for test report
classification in crowdsourced testing’. Proc. Int. Conf. on Software
Engineering, Buenos Aires, Argentina, May 2017
[24] Xu, B., Ye, D., Xing, Z., et al.: ‘Predicting semantically linkable knowledge
in developer online forums via convolutional neural network’. Proc. Int. Conf.
on Automated Software Engineering, Austin, TX, USA, August 2016
[25] Guo, J., Cheng, J., Cleland-Huang, J.: ‘Semantically enhanced software
traceability using deep learning techniques’. Proc. Int. Conf. on Software
Engineering, Buenos Aires, Argentina, May 2017
[26] Nagappan, N., Ball, T., Zeller, V.: ‘Mining metrics to predict component
failures’. Proc. Int. Conf. on Software Engineering, 2006, Shanghai, China,
May 2006
[27] Hassan, A.E.: ‘Predicting faults using the complexity of code changes’. Proc.
Int. Conf. on Software Engineering, Vancouver, BC, Canada, May 2009
[28] Nagappan, N., Ball, T.: ‘Use of relative code churn measures to predict
system defect density’. Proc. Int. Conf. on Software Engineering, St. Louis,
Missouri, USA, May 2005
[29] Koru, A.G., Zhang, D., Emam, K.E., et al.: ‘An investigation into the
functional form of the size-defect relationship for software modules’, IEEE
Trans. Softw. Eng., 2009, 35, (2), pp. 293–304
[30] Purushothaman, R., Perry, D.E.: ‘Toward understanding the rhetoric of small
source code changes’, IEEE Trans. Softw. Eng., 2005, 31, (6), pp. 511–526
[31] Graves, T.L., Karr, A.F., Marron, J.S., et al.: ‘Predicting fault incidence using
software change history’, IEEE Trans. Softw. Eng., 2000, 26, (7), pp. 653–661
[32] Shin, Y., Meneely, A., Williams, L., et al.: ‘Evaluating complexity, code
churn, and developer activity metrics as indicators of software
vulnerabilities’, IEEE Trans. Softw. Eng., 2011, 37, (6), pp. 772–787
[33] Yin, Z., Yuan, D., Zhou, Y., et al.: ‘How do fixes become bugs?’. Proc. 19th
ACM SIGSOFT Symp. and the 13th European Conf. on Foundations of
Software Engineering, ESEC/FSE, Szeged, Hungary, September 2011
[34] Chawla, N.V., Bowyer, K.W., Hall, L.O., et al.: ‘SMOTE: synthetic minority
over-sampling technique’, J. Artif. Intell. Res., 2002, 16, pp. 321–357
[35] Fu, W., Menzies, T.: ‘Revisiting unsupervised learning for defect prediction’.
Joint Meeting on Foundations of Software Engineering. Paderborn, Germany,
September 2017
[36] Liu, J., Zhou, Y., Yang, Y., et al.: ‘Code churn: a neglected metric in effortaware
just-in-time defect prediction’. ACM/IEEE Int. Symp. on Empirical
Software Engineering & Measurement, Abu Dhabi, United Arab Emirates,
December 2019
[37] Xia, X., Lo, D., Wang, X., et al.: ‘Collective personalized change
classification with multiobjective search’, IEEE Trans. Reliab., 2016, 65, (4),
pp. 1–20
[38] Huang, Q., Shihab, E., Xia, X., et al.: ‘Identifying self-admitted technical debt
in open source projects using text mining’, Empir. Softw. Eng., 2017, 23, pp.
418–451
[39] Shiha, E., Ihara, A., Kamei, Y., et al.: ‘Studying re-opened bugs in open
source software’, Empir. Softw. Eng., 2013, 18, (5), pp. 1005–1042
[40] Yang, Y., Zhou, Y., Liu, J., et al.: ‘Effort-aware just-in-time defect prediction:
simple unsupervised models could be better than supervised models’. ACM
Sigsoft Int. Symp. on Foundations of Software Engineering, Bochum,
Germany, March 2016


#
# 2nd submission
#
>><N.>A compositional model for effort‐aware Just‐In‐Time defect prediction on android apps
1. Zhang, T., et al.: A literature review of research in bug resolution: tasks,
challenges and future directions. Comput. J. 59(5), 741–773 (2016)
2. Xu, Z., et al.: Cross version defect prediction with representative data via
sparse subset selection. In: Proceedings of the 26th International Conference
on Program Comprehension (ICPC), pp. 132–143 (2018)
3. Xu, Z., et al.: Cross project defect prediction via balanced distribution
adaptation based transfer learning. J. Comput. Sci. Technol. 34(5),
1039–1062 (2019)
4. McIlroy, S., Ali, N., Hassan, A.E.: Fresh apps: an empirical study of
frequently‐updated mobile apps in the google play store. Empir. Software
Eng. 21(3), 1346–1370 (2016)
5. Menzies, T., Greenwald, J., Frank, A.: Data mining static code attributes
to learn defect predictors. IEEE Trans. Software Eng. 33(1), 2–13 (2006)
6. Scanniello, G., et al.: Class level fault prediction using software clustering.
In: Proceedings of the 28th IEEE/ACM International Conference
on Automated Software Engineering (ASE). (IEEE), pp. 640–645
(2013)
7. Kamei, Y., et al.: A large‐scale empirical study of just‐in‐time quality
assurance. IEEE Trans. Software Eng. 39(6), 757–773 (2012)
8. Yang, X., et al.: Deep learning for just‐in‐time defect prediction. In:
Proceedings of the 15th IEEE International Conference on Software
Quality, Reliability and Security (QRS). (IEEE), pp. 17–26 (2015)
9. Catolino, G.: Just‐in‐time bug prediction in mobile applications: the
domain matters! In: Proceedings of the 4th IEEE/ACM International
Conference on Mobile Software Engineering and Systems (MOBILESoft).
(IEEE), pp. 201–202 (2017)
10. Catolino, G., Di.Nucci, D., Ferrucci, F.: Cross‐project just‐in‐time bug
prediction for mobile apps: an empirical assessment. In: Proceedings of
the 6th IEEE/ACM International Conference on Mobile Software Engineering
and Systems (MOBILESoft). (IEEE), pp. 99–110 (2019)
11. Liu, F., et al.: Software defect prediction model based on pca‐isvm.
Computer Simulation. 31(3), 397–401 (2014)
12. Cao, H., Qin, Z., Feng, T.: A novel pca‐bp fuzzy neural network model
for software defect prediction. Adv. Sci. Lett. 9(1), 423–428 (2012)
13. Shepperd, M., et al.: Data quality: some comments on the nasa
software defect datasets. IEEE Trans. Software Eng. 39(9), 1208–1215
(2013)
14. Schölkopf, B., Smola, A., Müller, K.R.: Kernel principal component
analysis. In: International conference on artificial neural networks.
(Springer), pp. 583–588 (1997)
15. Kim, K.I., Franz, M.O., Scholkopf, B.: Iterative kernel principal
component analysis for image modeling. IEEE Trans. Pattern Anal.
Mach. Intell. 27(9), 1351–1366 (2005)
16. Schölkopf, B., Smola, A., Müller, K.R.: Nonlinear component analysis as
a kernel eigenvalue problem. Neural Comput. 10(5), 1299–1319 (1998)
17. Xu, Z., et al.: Software defect prediction based on kernel pca and
weighted extreme learning machine. Inf. Software Technol. 106, 182–200
(2019)
18. Aurelio, Y.S., et al.: Learning from imbalanced data sets with weighted
cross‐entropy function. Neural Process. Lett. 50(2), 1937–1949 (2019)
19. Arisholm, E., Briand, L.C., Fuglerud, M.: Data mining techniques for
building fault‐proneness models in telecom java software. In: Proceedings
of the 18th IEEE International Symposium on Software Reliability
(ISSRE). (IEEE), pp. 215–224 (2007)
20. Zhao, K., et al.: Just‐in‐time defect prediction for android apps via
imbalanced deep learning model. In: Proceedings of the 36th Annual
ACM Symposium on Applied Computing. pp. 1447–1454. (2021)
21. Xu, Z., et al.: The impact of feature selection on defect prediction performance:
an empirical comparison. In: Proceedings of the 27th IEEE
International Symposium on Software Reliability Engineering (ISSRE).
(IEEE), pp. 309–320 (2016)
22. Liu, S., et al.: A feature selection framework for software defect prediction.
In: Proceedings of the 38th IEEE Annual Computer Software
and Applications Conference (COMPSAC). (IEEE), pp. 426–435 (2014)
23. Shivaji, S., et al.: Reducing features to improve code change‐based bug
prediction. IEEE Trans. Software Eng. 39(4), 552–569 (2012)
24. Chen, X., et al.: Applying feature selection to software defect prediction
using multi‐objective optimization. In: Proceedings of the 41st IEEE
Annual Computer Software and Applications Conference (COMPSAC).
(IEEE), 2, pp. 54–59 (2017)
25. Ghotra, B., McIntosh, S., Hassan, A.E.: A large‐scale study of the impact
of feature selection techniques on defect classification models. In: Proceedings
of the 14th IEEE/ACM International Conference on Mining
Software Repositories (MSR). (IEEE), pp. 146–157 (2017)
26. Ni, C., et al.: A cluster based feature selection method for cross‐project
software defect prediction. J. Comput. Sci. Technol. 32(6), 1090–1107
(2017)
27. Song, Q., Guo, Y., Shepperd, M.J.: A comprehensive investigation of the
role of imbalanced learning for software defect prediction. IEEE Trans.
Software Eng. 45(12), 1253–1269 (2019)
28. Liu, M., Miao, L., Zhang, D.: Two‐stage cost‐sensitive learning for
software defect prediction. IEEE Trans. Reliab. 63(2), 676–686 (2014)
29. Siers, M.J., Islam, M.Z.: Software defect prediction using a cost sensitive
decision forest and voting, and a potential solution to the class imbalance
problem. Inf. Syst. 51, 62–71 (2015)
30. Bennin, K.E., et al.: The significant effects of data sampling approaches
on software defect prioritization and classification. In: 2017 ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM). (IEEE), pp. 364–373 (2017)
31. Bennin, K.E., Keung, J., Monden, A.: Impact of the distribution
parameter of data sampling approaches on software defect prediction
models. In: 2017 24th Asia‐Pacific Software Engineering Conference
(APSEC). (IEEE), pp. 630–635 (2017)
32. Tantithamthavorn, C., Hassan, A.E., Matsumoto, K.: The impact of class
rebalancing techniques on the performance and interpretation of defect
prediction models. IEEE Trans. Software Eng. 46(11), 1200–1219 (2018)
33. Bennin, K.E., et al.: Mahakil: diversity based oversampling approach to
alleviate the class imbalance issue in software defect prediction. IEEE
Trans. Software Eng. 44(6), 534–550 (2017)
34. Fukushima, T., et al.: An empirical study of just‐in‐time defect prediction
using cross‐project models. In: Proceedings of the 11th Working Conference
on Mining Software Repositories (MSR). pp. 172–181 (2014)
35. Kamei, Y., et al.: Studying just‐in‐time defect prediction using crossproject
models. Empir. Software Eng. 21(5), 2072–2106 (2016)
36. McIntosh, S., Kamei, Y.: Are fix‐inducing changes a moving target? a
longitudinal case study of just‐in‐time defect prediction. IEEE Trans.
Software Eng. 44(5), 412–428 (2017)
37. Yang, X., et al.: Tlel: a two‐layer ensemble learning approach for just‐intime
defect prediction. Inf. Software Technol. 87, 206–220 (2017)
38. Pascarella, L., Palomba, F., Bacchelli, A.: Fine‐grained just‐in‐time defect
prediction. J. Syst. Software. 150, 22–36 (2019)
39. Cabral, G.G., et al.: Class imbalance evolution and verification latency in
just‐in‐time software defect prediction. In: Proceedings of the 41st
IEEE/ACM International Conference on Software Engineering (ICSE).
(IEEE), pp. 666–676 (2019)
40. Kondo, M., et al.: The impact of context metrics on just‐in‐time defect
prediction. Empir. Software Eng. 25(1), 890–939 (2020)
41. Ortu, M., et al.: Measuring high and low priority defects on traditional
and mobile open source software. In: Proceedings of the 7th International
Workshop on Emerging Trends in Software Metrics, pp. 1–7
(2017)
42. Khomh, F., et al.: Predicting post‐release defects using pre‐release field
testing results. In: Proceedings of the 27th IEEE International Conference
on Software Maintenance (ICSM). (IEEE), pp. 253–262 (2017)
43. Scandariato, R., Walden, J.: Predicting vulnerable classes in an android
application. In: Proceedings of the 4th International Workshop on Security
Measurements and Metrics, pp. 11–16 (2012)
44. Kaur, A., Kaur, K., Kaur, H.: An investigation of the accuracy of code
and process metrics for defect prediction of mobile applications. In:
Proceedings of the 4th International Conference on Reliability, Infocom
Technologies and Optimization. (IEEE), pp. 1–6 (2015)
45. Ricky, M.Y., Purnomo, F., Yulianto, B.: Mobile application software
defect prediction. In: 2016 IEEE Symposium on Service‐Oriented System
Engineering. (IEEE), pp. 307–313 (2016)
46. Malhotra, R.: An empirical framework for defect prediction using machine
learning techniques with android software. Appl. Soft Comput. 49,
1034–1050 (2016)
47. Kaur, A., Kaur, K., Kaur, H.: Application of machine learning on
process metrics for defect prediction in mobile application. In: Information
Systems Design and Intelligent Applications. (Springer), pp.
81–98 (2016)
48. Li, J., et al.: Software defect prediction via convolutional neural network.
In: Proceedings of the 17th IEEE International Conference on Software
Quality, Reliability and Security (QRS). (IEEE), pp. 318–328 (2017)
49. Phan, A.V., Le.Nguyen, M., Bui, L.T.: Convolutional neural networks
over control flow graphs for software defect prediction. In: Proceedings
of the 29th IEEE International Conference on Tools with Artificial
Intelligence (ICTAI). (IEEE), pp. 45–52 (2017)
50. Manjula, C., Florence, L.: Deep neural network based hybrid approach
for software defect prediction using software metrics. Cluster Comput.
22(4), 9847–9863 (2019)
51. Xu, Z., et al.: Ldfr: learning deep feature representation for software
defect prediction. J. Syst. Software. 158, 110402 (2019)
52. Li, S.Z., et al. Kernel machine based learning for multi‐view face
detection and pose estimation. In: Proceedings of the 8th IEEE International
Conference on Computer Vision. ICCV 2001. (IEEE), 2,
pp. 674–679 (2001)
53. Huang, J., Yan, X.: Relevant and independent multi‐block approach for
plant‐wide process and quality‐related monitoring based on kpca and
svdd. ISA (Instrum. Soc. Am.) Trans. 73, 257–267 (2018)
54. Li, W., et al.: On improving the accuracy with auto‐encoder on
conjunctivitis. Appl. Soft Comput. 81, 105489 (2019)
55. Yan, M., et al.: File‐level defect prediction: unsupervised vs. supervised
models. In: Proceedings of the 11th International Symposium on
Empirical Software Engineering and Measurement (ESEM). (IEEE),
pp. 344–353 (2017)
56. Huang, Q., Xia, X., Lo, D.: Revisiting supervised and unsupervised
models for effort‐aware just‐in‐time defect prediction. Empir. Software
Eng. 24(5), 2823–2862 (2019)
57. Xu, Z., et al.: Tstss: a two‐stage training subset selection framework for
cross version defect prediction. J. Syst. Software. 154, 59–78 (2019)
58. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning. The MIT Press
(2016)
59. Tantithamthavorn, C., et al.: An empirical comparison of model validation
techniques for defect prediction models. IEEE Trans. Software Eng.
43(1), 1–18 (2016)
60. Guo, L., et al.: Robust prediction of fault‐proneness by random forests.
In: Proceedings of the 15th International Symposium on Software
Reliability Engineering (ISSRE). (IEEE), pp. 417–428 (2004)
61. Catal, C., Diri, B.: Investigating the effect of dataset size, metrics sets, and
feature selection techniques on software fault prediction problem. Inf.
Sci. 179(8), 1040–1058 (2009)
62. Xia, X., et al.: Cross‐project build co‐change prediction. In: Proceedings
of the 22nd IEEE International Conference on Software
Analysis, Evolution, and Reengineering (SANER). (IEEE), pp. 311–320
(2015)
63. Magal, R.K., Gracia, J.S.: Improved random forest algorithm for software
defect prediction through data mining techniques. Int. J. Comput. Appl.
117(23), 18–22 (2015)
64. Islam, M.Z., Giggins, H.: Knowledge discovery through sysfor ‐ a systematically
developed forest of multiple decision trees. In: Proceedings of
the 9th Australasian Data Mining Conference, AusDM, pp. 195–204
(2011)
65. Li, J., Liu, H.: Ensembles of cascading trees. In: Proceedings of the 3rd
IEEE International Conference on Data Mining (ICDM). pp. 585–588
(2003)
66. Hu, H., et al.: A maximally diversified multiple decision tree algorithm for
microarray data classification. In: Proceedings of the 2006 Workshop on
Intelligent Systems for Bioinformatics. 73, pp. 35–38 (2006)


>><[N]>Differential Evolution-Based Approach for Effort-Aware Just-in-Time Software Defect Prediction
[1] Erik Arisholm, Lionel C. Briand, and Eivind B. Johannessen. 2010. A systematic
and comprehensive investigation of methods to build and evaluate fault prediction
models. Journal of Systems and Software 83, 1 (2010), 2–17.
[2] Liang Cai, Yuan-Rui FAN, Meng Yan, and Xin Xia. 2019. Just-in-time software
defect prediction: literature review. Journal of Software 30, 5 (2019), 1288–1307.
[3] Wei Fu and Tim Menzies. 2017. Revisiting unsupervised learning for defect
prediction. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering, ESEC/FSE. ACM, Paderborn, Germany, 72–83.
[4] Yuchen Guo, Martin J. Shepperd, and Ning Li. 2018. Bridging effort-aware
prediction and strong classification: a just-in-time software defect prediction
study. In Proceedings of the 40th International Conference on Software Engineering:
Companion Proceeedings, ICSE. ACM, Gothenburg, Sweden, 325–326.
[5] Qiao Huang, Xin Xia, and David Lo. 2017. Supervised vs unsupervised models:
a holistic look at effort-aware just-in-time defect prediction. In IEEE International
Conference on Software Maintenance and Evolution, ICSME. IEEE Computer
Society, Shanghai, China, 159–170.
[6] Qiao Huang, Xin Xia, and David Lo. 2019. Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction. Empirical Software
Engineering 24, 5 (2019), 2823–2862.
[7] Enio G Jelihovschi, José Cláudio Faria, and Ivan Bezerra Allaman. 2014. ScottKnott:
a package for performing the Scott-Knott clustering algorithm in R. TEMA
(São Carlos) 15, 1 (2014), 3–17.
[8] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E. Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072–2106.
[9] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757–773.
[10] Thilo Mende and Rainer Koschke. 2010. Effort-aware defect prediction models.
In 14th European Conference on Software Maintenance and Reengineering, CSMR.
IEEE Computer Society, Madrid, Spain, 107–116.
[11] Tim Menzies, Andrew Butcher, David R. Cok, Andrian Marcus, Lucas Layman,
Forrest Shull, Burak Turhan, and Thomas Zimmermann. 2013. Local versus global
lessons for defect prediction and effort estimation. IEEE Transactions on Software
Engineering 39, 6 (2013), 822–834.
[12] Tim Menzies, Andrew Butcher, Andrian Marcus, Thomas Zimmermann, and
David R. Cok. 2011. Local vs. global models for effort estimation and defect
prediction. In 26th IEEE/ACM International Conference on Automated Software
Engineering ASE. IEEE Computer Society, KS, USA, 343–351.
[13] Audris Mockus and David M. Weiss. 2000. Predicting risk of software changes.
Bell Labs Technical Journal 5, 2 (2000), 169–180.
[14] Anh Tuan Nguyen, Tung Thanh Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen.
2012. Multi-layered approach for recovering links between bug reports and fixes.
In ACM SIGSOFT Symposium on the Foundations of Software Engineering. ACM,
NC, USA, 63.
[15] Jacek Sliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When do
changes induce fixes? ACM SIGSOFT Software Engineering Notes 30, 4 (2005),
1–5.
[16] Rainer Storn and Kenneth V. Price. 1997. Differential evolution - a simple and
efficient heuristic for global optimization over continuous spaces. Journal of
Global Optimization 11, 4 (1997), 341–359.
[17] Chakkrit Tantithamthavorn and Ahmed E. Hassan. 2018. An experience report
on defect modelling in practice: pitfalls and challenges. In Proceedings of the 40th
International Conference on Software Engineering: Software Engineering in Practice,
ICSE (SEIP). ACM, Gothenburg, Sweden, 286–295.
[18] Elaine J. Weyuker, Thomas J. Ostrand, and Robert M. Bell. 2010. Comparing
the effectiveness of several modeling methods for fault prediction. Empirical
Software Engineering 15, 3 (2010), 277–295.
[19] Rongxin Wu, Hongyu Zhang, Sunghun Kim, and Shing-Chi Cheung. 2011. Re-
Link: recovering links between bugs and changes. In ACM SIGSOFT Symposium
on the Foundations of Software Engineering and European Software Engineering
Conference. ACM, Szeged, Hungary, 15–25.
[20] Xingguang Yang, Huiqun Yu, Guisheng Fan, Kai Shi, and Liqiong Chen. 2019.
Local versus global models for just-in-time software defect prediction. Scientific
Programming 2019 (2019), 2384706:1–2384706:13.
[21] Xingguang Yang, Huiqun Yu, Guisheng Fan, Kang Yang, and Kai Shi. 2019. An
Empirical Study on Progressive Sampling for Just-in-Time Software Defect Prediction.
In Proceedings of the 7th International Workshop on Quantitative Approaches
to Software Quality, Vol. 2511. CEUR-WS.org, Putrajaya, Malaysia, 12–18.
[22] Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei Xu,
Baowen Xu, and Hareton Leung. 2016. Effort-aware just-in-time defect prediction:
simple unsupervised models could be better than supervised models. In
Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering, FSE. ACM, Seattle, WA, USA, 157–168.


>><[N]>A Preliminary Evaluation of CPDP Approaches on Just-in-Time Software Defect Prediction
[1] T. Menzies, J. Greenwald, and A. Frank, “Data mining static code
attributes to learn defect predictors,” IEEE Transactions on Software
Engineering, vol. 33, no. 1, pp. 2–13, 2007.
[2] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[3] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using crossproject
models,” Empirical Software Engineering, vol. 21, no. 6, pp.
2072–2106, 2016.
[4] S. Herbold, “CrossPare: A Tool for Benchmarking Cross-Project Defect
Predictions,” in Proc. of ASEW ’15. IEEE, 2015, pp. 90–96.
[5] L. Pascarella, F. Palomba, and A. Bacchelli, “Re-evaluating methodlevel
bug prediction,” in Proc. of International Conference on Software
Analysis, Evolution and Reengineering, 2018, pp. 592–601.
[6] S. Kim, E. J. Whitehead,, and Y. Zhang, “Classifying software changes:
Clean or buggy?” IEEE Transactions on Software Engineering, vol. 34,
no. 2, pp. 181–196, 2008.
[7] Y. Yang, Y. Zhou, J. Liu, Y. Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: Simple unsupervised
models could be better than supervised models,” in Proceedings of the
2016 24th ACM SIGSOFT International Symposium on Foundations of
Software Engineering, 2016, p. 157–168.
[8] X. Yang, D. Lo, X. Xia, and J. Sun, “TLEL: A two-layer ensemble
learning approach for just-in-time defect prediction,” Information and
Software Technology, vol. 87, pp. 206–220, 2017.
[9] G. G. Cabral, L. L. Minku, E. Shihab, and S. Mujahid, “Class imbalance
evolution and verification latency in just-in-time software defect prediction,”
in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), 2019, pp. 666–676.
[10] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-grained just-in-time
defect prediction,” The Journal of Systems & Software, vol. 150, pp.
22–36, 2019.
[11] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for justin-
time defect prediction,” in 2015 IEEE International Conference on
Software Quality, Reliability and Security, 2015, pp. 17–26.
[12] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Software
Engineering, 2018.
[13] G. Catolino, D. Di Nucci, and F. Ferrucci, “Cross-project just-intime
bug prediction for mobile apps: An empirical assessment,” in
2019 IEEE/ACM 6th International Conference on Mobile Software
Engineering and Systems (MOBILESoft), 2019, pp. 99–110.
[14] W. Li, W. Zhang, X. Jia, and Z. Huang, “Effort-aware semi-supervised
just-in-time defect prediction,” Information and Software Technology,
vol. 126, p. 106364, 2020.
[15] K. Zhu, N. Zhang, S. Ying, and D. Zhu, “Within-project and crossproject
just-in-time defect prediction based on denoising autoencoder
and convolutional neural network,” IET Software, vol. 14, no. 3, pp.
185–195, 2020.
[16] L. C. Briand, W. L. Melo, and J. Wust, “Assessing the applicability of
fault-proneness models across object-oriented software projects,” IEEE
Transactions on Software Engineering, vol. 28, no. 7, pp. 706–720, 2002.
[17] M. Jureczko and L. Madeyski, “Towards identifying software project
clusters with regard to defect prediction,” in Proc. of International
Conference on Predictive Models in Software Engineering, 2010.
[18] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,” IEEE Transactions
on Software Engineering, vol. 44, no. 5, pp. 412–428, 2018.
[19] S. Tabassum, L. L. Minku, D. Feng, G. G. Cabral, and L. Song, “An
investigation of cross-project learning in online just-in-time software
defect prediction,” in Proc. of International Conference on Software
Engineering, New York, NY, USA, 2020, p. 554–565.
[20] H. Jahanshahi, D. Jothimani, A. Bas¸ar, and M. Cevik, “Does chronology
matter in jit defect prediction? a partial replication study,” in Proceedings
of the Fifteenth International Conference on Predictive Models and Data
Analytics in Software Engineering, 2019, p. 90–99.
[21] S. Amasaki, K. Kawata, and T. Yokogawa, “Improving Cross-Project
Defect Prediction Methods with Data Simplification,” in Proc. of SEAA
’15. IEEE, 2015, pp. 96–103.
[22] C.-C. A. Erika and K. Ochimizu, “Towards logistic regression models
for predicting fault-prone code across software projects,” in Proc. of
ESEM ’09. IEEE, 2009, pp. 460–463.
[23] G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella,
and S. Panichella, “Multi-objective Cross-Project Defect Prediction,” in
Proc. of ICST ’13. IEEE, 2013, pp. 252–261.
[24] S. Herbold, “Training data selection for cross-project defect prediction,”
in Proc. of PROMISE ’13. ACM, 2013, pp. 6:1–6:10.
[25] K. Kawata, S. Amasaki, and T. Yokogawa, “Improving relevancy filter
methods for cross-project defect prediction,” in Proc. of ACIT-CSI ’15,
2015, pp. 2–7.
[26] Y. Liu, T. M. Khoshgoftaar, and N. Seliya, “Evolutionary Optimization
of Software Quality Modeling with Multiple Repositories,” IEEE Transactions
on Software Engineering, vol. 36, no. 6, pp. 852–864, 2010.
[27] Y. Ma, G. Luo, X. Zeng, and A. Chen, “Transfer learning for crosscompany
software defect prediction,” Information and Software Technology,
vol. 54, no. 3, pp. 248–256, 2012.
[28] J. Nam and S. Kim, “CLAMI: Defect Prediction on Unlabeled Datasets,”
in Proc. of ASE ’15. IEEE, 2015, pp. 452–463.
[29] A. Panichella, R. Oliveto, and A. De Lucia, “Cross-project defect
prediction models: L’Union fait la force,” in Proc. of CSMR-WCRE ’14.
IEEE, 2014, pp. 164–173.
[30] F. Peters and T. Menzies, “Privacy and utility for defect prediction:
experiments with MORPH,” in Proc. of ICSE ’12. IEEE, 2012, pp.
189–199.
[31] F. Peters, T. Menzies, L. Gong, and H. Zhang, “Balancing Privacy and
Utility in Cross-Company Defect Prediction,” IEEE Transactions on
Software Engineering, vol. 39, no. 8, pp. 1054–1068, 2013.
[32] P. He, B. Li, X. Liu, J. Chen, and Y. Ma, “An empirical study on software
defect prediction with a simplified metric set,” Information and Software
Technology, vol. 59, pp. 170–190, 2015.
[33] F. Peters, T. Menzies, and L. Layman, “LACE2: Better Privacy-
Preserving Data Sharing for Cross Project Defect Prediction,” in Proc. of
ICSE ’15. IEEE, 2015, pp. 801–811.
[34] D. Ryu, O. Choi, and J. Baik, “Value-cognitive boosting with a support
vector machine for cross-project defect prediction,” Empirical Software
Engineering, vol. 21, no. 1, pp. 1–29, 2014.
[35] D. Ryu, J.-I. Jang, and J. Baik, “A hybrid instance selection using
nearest-neighbor for cross-project defect prediction,” Journal of Computer
Science and Technology, vol. 30, no. 5, pp. 969–980, 2015.
[36] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Software Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[37] S. Uchigaki, S. Uchida, K. Toda, and A. Monden, “An Ensemble Approach
of Simple Regression Models to Cross-Project Fault Prediction,”
in Proc. of SNPD ’12. IEEE, 2012, pp. 476–481.
[38] S. Watanabe, H. Kaiya, and K. Kaijiri, “Adapting a fault prediction
model to allow inter languagereuse,” in Proc. of PROMISE ’08. ACM,
2008, pp. 19–24.
[39] Y. Zhang, D. Lo, X. Xia, and J. Sun, “An Empirical Study of Classifier
Combination for Cross-Project Defect Prediction,” in Proc. of COMPSAC
’15. IEEE, 2015, pp. 264–269.
[40] ——, “Combined classifier for cross-project defect prediction: an extended
empirical study,” Frontiers of Computer Science, vol. 12, no. 2,
pp. 280–296, 2018.
[41] Z. He, F. Peters, T. Menzies, and Y. Yang, “Learning from Open-Source
Projects: An Empirical Study on Defect Prediction,” in Proc. of ESEM
’13. IEEE, 2013, pp. 45–54.
[42] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-project defect prediction: a large scale experiment on data vs.
domain vs. process,” in Proc. of ESEC/FSE ’09. ACM, 2009, pp.
91–100.
[43] T. M. Khoshgoftaar, P. Rebours, and N. Seliya, “Software quality
analysis by combining multiple projects and learners,” Software Quality
Journal, vol. 17, no. 1, pp. 25–49, 2009.
[44] T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, and D. Cok, “Local
versus global models for effort estimation and defect prediction,” in
Proc. of ASE ’11. IEEE, 2011, pp. 343–351.
[45] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in Proc. of
ICSE ’13. IEEE, 2013, pp. 382–391.
[46] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“An empirical comparison of model validation techniques for defect
prediction models,” IEEE Transactions on Software Engineering, vol. 43,
no. 1, pp. 1–18, 2017.
[47] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” in Proc. of Working Conference on Mining Software Repositories,
2005.



>><[N]>An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction
[1] “Ai fairness 360,” https://aif360.mybluemix.net/, accessed: 2021-05-1.
[2] “Open stack data set,” https://docs.openstack.org/wallaby/?_ga=2.205840979.
1124305833.1619313296-1069099767.1617679651, accessed: 2020-07-1.
[3] “Openstack,” https://www.openstack.org/, accessed: 2021-05-1.
[4] “Qt,” https://www.qt.io/, accessed: 2021-05-1.
[5] “What-if tool,” https://pair-code.github.io/what-if-tool/, accessed: 2021-05-1.
[6] A. Aggarwal, P. Lohia, S. Nagar, K. Dey, and D. Saha, “Black box fairness testing
of machine learning models,” in Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering, 2019, pp. 625–635.
[7] H. Altae-Tran, B. Ramsundar, A. S. Pappu, and V. Pande, “Low data drug discovery
with one-shot learning,” ACS central science, vol. 3, no. 4, pp. 283–293, 2017.
[8] S. Barocas, M. Hardt, and A. Narayanan, “Fairness and machine learning: Limitations
and opportunities,” 2018.
[9] K. E. Bennin, K. Toda, Y. Kamei, J. Keung, A. Monden, and N. Ubayashi, “Empirical
evaluation of cross-release effort-aware defect prediction models,” in 2016 IEEE
International Conference on Software Quality, Reliability and Security (QRS). IEEE,
2016, pp. 214–221.
[10] A. Bernstein, J. Ekanayake, and M. Pinzger, “Improving defect prediction using
temporal features and non linear models,” in Ninth international workshop on
Principles of software evolution: in conjunction with the 6th ESEC/FSE joint meeting.
ACM, 2007, pp. 11–18.
[11] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein, V. Filkov, and P. Devanbu,
“Fair and balanced? bias in bug-fix datasets,” in Proceedings of the 7th joint meeting
of the European Software Engineering Conference and the ACM SIGSOFT Symposium
on the Foundations of Software Engineering, 2009, pp. 121–130.
[12] S. Biswas and H. Rajan, “Do the machine learning models on a crowd sourced
platform exhibit bias? an empirical study on model fairness,” in Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, 2020, pp. 642–653.
[13] J. M. Bland and D. G. Altman, “Multiple significance tests: the bonferroni method,”
Bmj, vol. 310, no. 6973, p. 170, 1995.
[14] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, “Signature verification
using a" siamese" time delay neural network,” Advances in neural information
processing systems, vol. 6, pp. 737–744, 1993.
[15] J. Chakraborty, S. Majumder, Z. Yu, and T. Menzies, “Fairway: a way to build
fair ml software,” in Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering, 2020, pp. 654–665.
[16] J. Chakraborty, K. Peng, and T. Menzies, “Making fair ml software using trustworthy
explanation,” in 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, 2020, pp. 1229–1233.
[17] J. Chakraborty, T. Xia, F. M. Fahid, and T. Menzies, “Software engineering
for fairness: A case study with hyperparameter optimization,” arXiv preprint
arXiv:1905.05786, 2019.
[18] N. V. Chawla, K.W. Bowyer, L.O. Hall, andW. P. Kegelmeyer, “Smote: synthetic minority
over-sampling technique,” Journal of artificial intelligence research, vol. 16,
pp. 321–357, 2002.
[19] D. Di Nucci, F. Palomba, G. De Rosa, G. Bavota, R. Oliveto, and A. De Lucia, “A
developer centered bug prediction model,” IEEE Transactions on Software Engineering,
vol. 44, no. 1, pp. 5–24, 2018.
[20] Y. Fan, D. A. da Costa, D. Lo, A. Hassan, and L. Shanping, “The impact of mislabeled
changes by szz on just-in-time defect prediction,” IEEE Transactions on
Software Engineering, 2020.
[21] M. Galar, A. Fernandez, E. Barrenechea, H. Bustince, and F. Herrera, “A review
on ensembles for the class imbalance problem: bagging-, boosting-, and hybridbased
approaches,” IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), vol. 42, no. 4, pp. 463–484, 2011.
[22] A. Ghazikhani, H. S. Yazdi, and R. Monsefi, “Class imbalance handling using
wrapper-based random oversampling,” in 20th Iranian Conference on Electrical
Engineering (ICEE2012). IEEE, 2012, pp. 611–616.
[23] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of classification
techniques on the performance of defect prediction models,” in 2015 IEEE/ACM
37th IEEE International Conference on Software Engineering, vol. 1. IEEE, 2015,
pp. 789–800.
[24] E. Giger, M. D’Ambros, M. Pinzger, and H. C. Gall, “Method-level bug prediction,”
in Proceedings of the ACM-IEEE international symposium on Empirical software
engineering and measurement. ACM, 2012, pp. 171–180.
[25] E. Giger, M. Pinzger, and H. C. Gall, “Comparing fine-grained source code changes
and code churn for bug prediction,” in Proceedings of the 8th Working Conference
on Mining Software Repositories. ACM, 2011, pp. 83–92.
[26] D. Gray, D. Bowes, N. Davey, Y. Sun, and B. Christianson, “Reflections on the
nasa mdp data sets,” IET software, vol. 6, no. 6, pp. 549–558, 2012.
[27] F. Harel-Canada, L.Wang, M. A. Gulzar, Q. Gu, and M. Kim, “Is neuron coverage a
meaningful measure for testing deep neural networks?” in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2020, pp. 851–862.
[28] A. E. Hassan, “Predicting faults using the complexity of code changes,” in 2009
IEEE 31st international conference on software engineering. IEEE, 2009, pp. 78–88.
[29] H. He and E. A. Garcia, “Learning from imbalanced data,” IEEE Transactions on
knowledge and data engineering, vol. 21, no. 9, pp. 1263–1284, 2009.
[30] K. Herzig and N. Nagappan, “Empirically detecting false test alarms using association
rules,” in 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 2. IEEE, 2015, pp. 39–48.
[31] T. Hoang, H. K. Dam, Y. Kamei, D. Lo, and N. Ubayashi, “Deepjit: an end-to-end
deep learning framework for just-in-time defect prediction,” in 2019 IEEE/ACM
16th International Conference on Mining Software Repositories (MSR). IEEE, 2019,
pp. 34–45.
[32] T. Hoang, H. J. Kang, D. Lo, and J. Lawall, “Cc2vec: Distributed representations
of code changes,” in Proceedings of the ACM/IEEE 42nd International Conference
on Software Engineering, 2020, pp. 518–529.
[33] X. Huo, M. Li, Z.-H. Zhou et al., “Learning unified features from natural and
programming languages for locating buggy source code.” in IJCAI, vol. 16, 2016,
pp. 1606–1612.
[34] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in 2013 28th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
Ieee, 2013, pp. 279–289.
[35] X.-Y. Jing, S. Ying, Z.-W. Zhang, S.-S. Wu, and J. Liu, “Dictionary learning based
software defect prediction,” in Proceedings of the 36th International Conference on
Software Engineering, 2014, pp. 414–423.
[36] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi, and A. E. Hassan,
“Studying just-in-time defect prediction using cross-project models,” Empirical
Software Engineering, vol. 21, no. 5, pp. 2072–2106, 2016.
[37] Y. Kamei, S. Matsumoto, A. Monden, K.-i. Matsumoto, B. Adams, and A. E. Hassan,
“Revisiting common bug prediction findings using effort-aware models,” in 2010
IEEE international conference on software maintenance. IEEE, 2010, pp. 1–10.
[38] Y. Kamei, A. Monden, S. Matsumoto, T. Kakimoto, and K.-i. Matsumoto, “The
effects of over and under sampling on fault-prone module detection,” in First
International Symposium on Empirical Software Engineering and Measurement
(ESEM 2007). IEEE, 2007, pp. 196–204.
[39] Y. Kamei and E. Shihab, “Defect prediction: Accomplishments and future challenges,”
in 2016 IEEE 23rd international conference on software analysis, evolution,
and reengineering (SANER), vol. 5. IEEE, 2016, pp. 33–45.
[40] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha, and N. Ubayashi,
“A large-scale empirical study of just-in-time quality assurance,” IEEE Transactions
on Software Engineering, vol. 39, no. 6, pp. 757–773, 2012.
[41] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tantithamthavorn,
M. Choetkiertikul, C. Ragkhitwetsagul, and T. Sunetnanta, “Jitbot: An
explainable just-in-time defect prediction bot,” in 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 2020, pp.
1336–1339.
[42] S. Kim, E. J. Whitehead, and Y. Zhang, “Classifying software changes: Clean or
buggy?” IEEE Transactions on Software Engineering, vol. 34, no. 2, pp. 181–196,
2008.
[43] S. Kim and E. J. Whitehead Jr, “How long did it take to fix bugs?” in Proceedings
of the 2006 international workshop on Mining software repositories. ACM, 2006,
pp. 173–174.
[44] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in defect prediction,”
in 2011 33rd International Conference on Software Engineering (ICSE). IEEE, 2011,
pp. 481–490.
[45] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and A. Zeller, “Predicting faults from
cached history,” in 29th International Conference on Software Engineering (ICSE’07).
IEEE, 2007, pp. 489–498.
[46] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese neural networks for one-shot
image recognition,” in ICML deep learning workshop, vol. 2. Lille, 2015.
[47] S. Kotsiantis, D. Kanellopoulos, P. Pintelas et al., “Handling imbalanced datasets:
A review,” GESTS International Transactions on Computer Science and Engineering,
vol. 30, no. 1, pp. 25–36, 2006.
[48] N. Kumar, A. Berg, P. N. Belhumeur, and S. Nayar, “Describable visual attributes
for face verification and image search,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 33, no. 10, pp. 1962–1977, 2011.
[49] C. Le Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, “A systematic study of
automated program repair: Fixing 55 out of 105 bugs for $8 each,” in Software
Engineering (ICSE), 2012 34th International Conference on. IEEE, 2012, pp. 3–13.
[50] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553,
pp. 436–444, 2015.
[51] T. Lee, J. Nam, D. Han, S. Kim, and H. P. In, “Micro interaction metrics for defect
prediction,” in Proceedings of the 19th ACM SIGSOFT symposium and the 13th
European conference on Foundations of software engineering, 2011, pp. 311–321.
[52] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via convolutional
neural network,” in 2017 IEEE International Conference on Software Quality, Reliability
and Security (QRS). IEEE, 2017, pp. 318–328.
[53] S. Liu, X. Chen, W. Liu, J. Chen, Q. Gu, and D. Chen, “Fecar: A feature selection
framework for software defect prediction,” in 2014 IEEE 38th Annual Computer
Software and Applications Conference. IEEE, 2014, pp. 426–435.
[54] V. López, A. Fernández, S. García, V. Palade, and F. Herrera, “An insight into
classification with imbalanced data: Empirical results and current trends on using
data intrinsic characteristics,” Information sciences, vol. 250, pp. 113–141, 2013.
[55] R. Malhotra and S. Kamal, “An empirical study to investigate oversampling
methods for improving software defect prediction using imbalanced data,” Neurocomputing,
vol. 343, pp. 120–140, 2019.
[56] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving target? a longitudinal
case study of just-in-time defect prediction,” IEEE Transactions on Software
Engineering, vol. 44, no. 5, pp. 412–428, 2017.
[57] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on
bias and fairness in machine learning,” arXiv preprint arXiv:1908.09635, 2019.
[58] I. Melekhov, J. Kannala, and E. Rahtu, “Siamese network features for image
matching,” in 2016 23rd International Conference on Pattern Recognition (ICPR).
IEEE, 2016, pp. 378–383.
[59] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell Labs
Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[60] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, “Do automated program
repair techniques repair hard and important bugs?” Empirical Software
Engineering, vol. 23, no. 5, pp. 2901–2947, 2018.
[61] N. Nagappan, A. Zeller, T. Zimmermann, K. Herzig, and B. Murphy, “Change
bursts as defect predictors,” in Software Reliability Engineering (ISSRE), 2010 IEEE
21st International Symposium on. IEEE, 2010, pp. 309–318.
[62] P. Neculoiu, M. Versteegh, and M. Rotaru, “Learning text similarity with siamese
recurrent networks,” in Proceedings of the 1stWorkshop on Representation Learning
for NLP, 2016, pp. 148–157.
[63] C. Pornprasit and C. Tantithamthavorn, “Jitline: A simpler, better, faster, finergrained
just-in-time defect prediction,” arXiv preprint arXiv:2103.07068, 2021.
[64] L. Qiao and Y.Wang, “Effort-aware and just-in-time defect prediction with neural
network,” PloS one, vol. 14, no. 2, p. e0211359, 2019.
[65] F. Rahman and P. Devanbu, “Ownership, experience and defects: a fine-grained
study of authorship,” in Proceedings of the 33rd International Conference on Software
Engineering. ACM, 2011, pp. 491–500.
[66] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, “Sample size vs. bias in defect
prediction,” in Proceedings of the 2013 9th joint meeting on foundations of software
engineering, 2013, pp. 147–157.
[67] A. Rajkomar, M. Hardt, M. D. Howell, G. Corrado, and M. H. Chin, “Ensuring
fairness in machine learning to advance health equity,” Annals of internal medicine,
vol. 169, no. 12, pp. 866–872, 2018.
[68] Z. A. Rana, M. M. Awais, and S. Shamail, “Impact of using information gain
in software defect prediction models,” in International Conference on Intelligent
Computing. Springer, 2014, pp. 637–648.
[69] J. Romano, J. D. Kromrey, J. Coraggio, and J. Skowronek, “Appropriate statistics
for ordinal level data: Should we really be using t-test and cohen’sd for evaluating
group differences on the nsse and other surveys,” in annual meeting of the Florida
Association of Institutional Research, vol. 177, 2006.
[70] D. Ryu, J.-I. Jang, and J. Baik, “A hybrid instance selection using nearest-neighbor
for cross-project defect prediction,” Journal of Computer Science and Technology,
vol. 30, no. 5, pp. 969–980, 2015.
[71] N. Seliya and T. M. Khoshgoftaar, “The use of decision trees for cost-sensitive
classification: an empirical study in software quality prediction,” Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery, vol. 1, no. 5, pp. 448–459,
2011.
[72] S. Tabassum, L. L. Minku, D. Feng, G. G. Cabral, and L. Song, “An investigation of
cross-project learning in online just-in-time software defect prediction,” in 2020
IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEE,
2020, pp. 554–565.
[73] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction for imbalanced
data,” in 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 2. IEEE, 2015, pp. 99–108.
[74] Y. Tian, Z. Zhong, V. Ordonez, G. Kaiser, and B. Ray, “Testing dnn image classifiers
for confusion & bias errors,” in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering, 2020, pp. 1122–1134.
[75] F. Tramer, V. Atlidakis, R. Geambasu, D. Hsu, J.-P. Hubaux, M. Humbert, A. Juels,
and H. Lin, “Fairtest: Discovering unwarranted associations in data-driven applications,”
in 2017 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 2017, pp. 401–416.
[76] M. Veale and R. Binns, “Fairer machine learning in the real world: Mitigating
discrimination without collecting sensitive data,” Big Data & Society, vol. 4, no. 2,
p. 2053951717743530, 2017.
[77] A. Wang, A. Narayanan, and O. Russakovsky, “Revise: A tool for measuring and
mitigating bias in visual datasets,” in European Conference on Computer Vision.
Springer, 2020, pp. 733–751.
[78] Q. Wang, J. Gao, and Y. Yuan, “Embedding structured contour and location prior
in siamesed fully convolutional networks for road detection,” IEEE Transactions
on Intelligent Transportation Systems, vol. 19, no. 1, pp. 230–241, 2017.
[79] S. Wang and X. Yao, “Using class imbalance learning for software defect prediction,”
IEEE Transactions on Reliability, vol. 62, no. 2, pp. 434–443, 2013.
[80] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few examples: A
survey on few-shot learning,” ACM Computing Surveys (CSUR), vol. 53, no. 3, pp.
1–34, 2020.
[81] K. Q.Weinberger, J. Blitzer, and L. K. Saul, “Distance metric learning for large margin
nearest neighbor classification,” in Advances in neural information processing
systems, 2006, pp. 1473–1480.
[82] M. Wen, R. Wu, and S.-C. Cheung, “How well do change sequences predict
defects? sequence learning from software changes,” IEEE Transactions on Software
Engineering, vol. 46, no. 11, pp. 1155–1175, 2018.
[83] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “Relink: recovering links between
bugs and changes,” in Proceedings of the 19th ACM SIGSOFT symposium and the
13th European conference on Foundations of software engineering, 2011, pp. 15–25.
[84] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for just-in-time defect
prediction,” in 2015 IEEE International Conference on Software Quality, Reliability
and Security. IEEE, 2015, pp. 17–26.
[85] Y. Yao, M. Xu, Y. Wang, D. J. Crandall, and E. M. Atkins, “Unsupervised traffic
accident detection in first-person videos,” arXiv preprint arXiv:1903.00618, 2019.
[86] F. Zhang, A. Mockus, I. Keivanloo, and Y. Zou, “Towards building a universal
defect prediction model,” in Proceedings of the 11th Working Conference on Mining
Software Repositories, 2014, pp. 182–191.
[87] L. Zhao, Z. Shang, L. Zhao, A. Qin, and Y. Y. Tang, “Siamese dense neural network
for software defect prediction with small data,” IEEE Access, vol. 7, pp. 7663–7677,
2018.
[88] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy, “Cross-project
defect prediction: a large scale experiment on data vs. domain vs. process,” in
Proceedings of the 7th joint meeting of the European software engineering conference
and the ACM SIGSOFT symposium on The foundations of software engineering,
2009, pp. 91–100.
[89] T. Zimmermann, N. Nagappan, and A. Zeller, “Predicting bugs from history,” in
Software evolution. Springer, 2008, pp. 69–88.
12


>><N.>An empirical study of software change classification with imbalance data-handling methods
1. Bishnu PS, Bhattacherjee V. Software fault prediction using quad tree-based k-means clustering algorithm. IEEE Trans Knowl Data Eng.
2012;24(6):1146-1150.
2. Elish MO, Al-Rahman Al-Khiaty M. A suite of metrics for quantifying historical changes to predict future change prone classes in object
oriented software. J Softw Evol Process. 2013;25(5):407-437.
3. Ostrand TJ,Weyuker EJ, Bell RM.Where the bugs are. In: Proceedings of the 2004 ACMSIGSOFT International Symposium on Software
Testing and Analysis; 2004; Boston, MA.
4. Pan K, Kim S, Whitehead Jr EJ. Bug classification using program slicing metrics. Paper presented at: Sixth IEEE InternationalWorkshop
on Source Code Analysis and Manipulation; 2006; Philadelphia, PA.
5. Walia GS, Carver JC, Nagappan N. The effect of the number of inspectors on the defect estimates produced by capture-recapture models.
In: Proceedings of the 30th International Conference on Software Engineering; 2008; Leipzig, Germany.
6. Ebrahimi NB. On the statistical analysis of the number of errors remaining in a software design document after inspection. IEEE Trans
Softw Eng. 1997;23(8):529-532.
7. Thelin T, Runeson P. Confidence intervals for capture-recapture estimations in software inspections. Inf Softw Technol. 2002;44(12):
683-702.
8. Walia GS, Carver JC. Evaluation of capture-recapture models for estimating the abundance of naturally-occurring defects. In: Proceedings
of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement; 2008; Kaiserslautern,
Germany.
9. Mockus A,Weiss DM. Predicting risk of software changes. Bell Labs Tech J. 2000;5(2):169-180.
10. Kim S,Whitehead Jr EJ, Zhang Y. Classifying software changes: clean or buggy? IEEE Trans Softw Eng. 2008;34(2):181-196.
11. Shivaji S, Whitehead Jr EJ, Akella R, Kim S. Reducing features to improve code change-based bug prediction. IEEE Trans Softw Eng.
2013;39(4):552-569.
12. Jiang T, Tan L, Kim S. Personalized defect prediction. In: Proceedings of the 28th IEEE/ACM International Conference on Automated
Software Engineering; 2014; Silicon Valley, CA.
13. Duda RO, Hart PE. Pattern Classification and Scene Analysis. New York, NY:Wiley; 1973.
14. Joachims T. Text categorization with support vector machines: learning with many relevant features. In: Machine Learning: ECML-98:
10th European Conference on Machine Learning Chemnitz, Germany, April 21-23, 1998 Proceedings. Berlin, Germany: Springer Berlin
Heidelberg; 1998:137-142.
15. Tan M, Tan L, Dara S, Mayeux C. Online defect prediction for imbalanced data. Paper presented at: IEEE/ACM 37th IEEE International
Conference on Software Engineering; 2015; Florence, Italy.
16. Kamei Y, Shihab E, Adams B, et al. A large-scale empirical study of just-in-time quality assurance. IEEE Trans Softw Eng.
2013;39(6):757-773.
17. Yang X, Lo D, Xia X, Sun J. TLEL: a two-layer ensemble learning approach for just-in-time defect prediction. Inf Softw Technol.
2017;87:206-220.
18. Rokach L. Ensemble-based classifiers. Artif Intell Rev. 2010;33(1-2):1-39.
19. Li C. Classifying imbalanced data using a bagging ensemble variation (BEV). In: Proceedings of the 45th Annual Southeast Regional
Conference; 2007; Winston-Salem, NC.
20. Guo H, Viktor HL. Learning from imbalanced data sets with boosting and data generation: the databoost-IM approach. ACM SIGKDD
Explor Newsl. 2004;6(1):30-39.
21. Breiman L. Bagging predictors. Mach Learn. 1996;24(2):123-140.
22. Freund Y, Schapire RE. Experiments with a new boosting algorithm. In: Proceedings of the Thirteenth International Conference on
International Conference on Machine Learning; 1996; Bari, Italy.
23. Sadowski C, Lewis C, Lin Z, Zhu X,Whitehead Jr EJ. An empirical analysis of the fixcache algorithm. In: Proceedings of the 8thWorking
Conference on Mining Software Repositories; 2011; Honolulu, HI.
24. S´ liwerski J, Zimmermann T, Zeller A. When do changes induce fixes? In: Proceedings of the 2005 International Workshop on Mining
Software Repositories; 2005; St. Louis, MO.
25. Kim S, Zimmermann T, Whitehead Jr EJ, Zeller A. Predicting faults from cached history. In: Proceedings of the 29th International
Conference on Software Engineering; 2008; New York, NY.
26. Rahman F, Devanbu P. How, and why, process metrics are better. Paper presented at: 35th International Conference on Software
Engineering; 2013; San Francisco, CA.
27. Menzies T, Greenwald J, Frank A. Data mining static code attributes to learn defect predictors. IEEE Trans Softw Eng. 2007;33(1):2-13.
28. Lessmann S, Baesens B, Mues C, Pietsch S. Benchmarking classification models for software defect prediction: a proposed framework and
novel findings. IEEE Trans Softw Eng. 2008;34(4):485-496.
29. Zimmermann T, Premraj R, Zeller A. Predicting defects for eclipse. In: Proceedings of the Third International Workshop on Predictor
Models in Software Engineering; 2007; Minneapolis, MN.
30. Understand static code analysis tool; 2014. https://scitools.com/
31. Yang X, Lo D, Xia X, Zhang Y, Sun J. Deep learning for just-in-time defect prediction. Paper presented at: IEEE International Conference
on Software Quality, Reliability and Security; 2015; Vancouver, Canada.
32. Yang Y, Zhou Y, Liu J, et al. Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised
models. In: Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering; 2016;
Seattle, WA.
33. Dash M, Liu H. Feature selection for classification. Intell Data Anal. 1997;1(3):131-156.
34. John GH, Kohavi R, Pfleger K. Irrelevant features and the subset selection problem. In: Proceedings of the 11th International Conference
on Machine Learning; 1994; San Francisco, CA.
35. Hall MA, Holmes G. Benchmarking attribute selection techniques for discrete class data mining. IEEE Trans Knowl Data Eng.
2003;15(6):1437-1447.
36. Hall MA. Correlation-Based Feature Selection for Machine Learning. [PhD thesis]. Hamilton, New Zealand: The University of Waikato;
1999.
37. Quinlan JR. C4. 5: Programs for Machine Learning. New York, NY: Elsevier; 2014.
38. Quinlan JR. C4. 5: Programs for Machine Learning. San Francisco, CA: Morgan Kaufmann Publishers Inc; 1993.
39. Breiman L. Random forests. Mach Learn. 2001;45(1):5-32.
40. Lewis C, Lin Z, Sadowski C, Zhu X, Ou R,Whitehead Jr EJ. Does bug prediction support human developers? Findings from a Google case
study. In: Proceedings of the 2013 International Conference on Software Engineering; 2013; San Francisco, CA.
41. Menzies T, Dekhtyar A, Distefano J, Greenwald J. Problems with precision: a response to comments on data mining static code attributes
to learn defect predictors. IEEE Trans Softw Eng. 2007;33(9):637-640.
42. Rahman F, Posnett D, Devanbu P. Recalling the imprecision of cross-project defect prediction. In: Proceedings of the ACM SIGSOFT 20th
International Symposium on the Foundations of Software Engineering; 2012; Cary, NC.
43. Xia X, Lo D, Pan SJ, Nagappan N, Wang X. Hydra: massively compositional model for cross-project defect prediction. IEEE Trans Softw
Eng. 2016;42(10):977-998.
44. Arisholm E, Briand LC, Johannssen EB. A systematic and comprehensive investigation of methods to build and evaluate fault prediction
models. J Syst Softw. 2010;83(1):2-17.
45. Tantithamthavorn C, McIntosh S, HassanAE, Matsumoto K. An empirical comparison ofmodel validation techniques for defect prediction
models. IEEE Trans Softw Eng. 2017;43(1):1-18.
46. Kittler J, Hatef M, Duin RPW, Matas J. On combining classifiers. IEEE Trans Pattern Anal Mach Intell. 1998;20(3):226-239.
47. Xia X, Lo D, Wang X, Yang X. Collective personalized change classification with multiobjective search. IEEE Trans Reliab.
2016;65(1):1810-1829.
48. Huang Q, Xia X, Lo D. Supervised vs unsupervised models: a holistic look at effort-aware just-in-time defect prediction. Paper presented
at: IEEE International Conference on Software Maintenance and Evolution; 2017; Shanghai, China.
49. John GH, Langley P. Estimating continuous distributions in Bayesian classifiers. In: Proceedings of the Eleventh Conference on
Uncertainty in Artificial Intelligence; 1995; Montréal, Canada.
50. Aha DW, Kibler D, Albert MK. Instance-based learning algorithms. Mach Learn. 1991;6(1):37-66.
51. Cleary JG, Trigg LE. K*: an instance-based learner using an entropic distance measure. In: Proceedings of the Twelfth International
Conference on Machine Learning; 1995; Tahoe City, CA.
52. Friedman J, Hastie T, Tibshirani R. Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the
authors). Ann Stat. 2000;28(2):337-407.
53. Atkeson CG, Moore AW, Schaal S. Locally weighted learning. Artif Intell Rev. 1997;11(1-5):11-73.
54. Martin B. Instance-Based Learning: Nearest Neighbor With Generalization Thesis. [MS thesis]. Hamilton, New Zealand: The University of
Waikato; 1995.
55. Shalev-Shwartz S, Singer Y, Srebro N, Cotter A. Pegasos: primal estimated sub-gradient solver for SVN. In: Proceedings of the 24th
International Conference on Machine Learning; 2007; Corvalis, OR.
56. Wang T, Zhang Z, Jing X, Zhang L. Multiple kernel ensemble learning for software defect prediction. Autom Softw Eng. 2016;23(4):1-22.
57. Wang S, Yao X. Using class imbalance learning for software defect prediction. IEEE Trans Reliab. 2013;62(2):434-443.
58. Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance systems. IEEE Trans Syst
Man Cybern Part A SystHum. 2009;39(5):1097-1107.
59. Holland JH. Adaptation in Natural and Artificial Systems: An Introductory Analysis With Applications to Biology, Control, and Artificial
Intelligence. Ann Arbor, MI: The University of Michigan Press; 1975.
60. Seiffert C, Khoshgoftaar TM, Van Hulse J. Improving software-quality predictions with data sampling and boosting. IEEE Trans SystMan
Cybern Part A Syst Hum. 2009;39(6):1283-1294.
61. Sun Z, Song Q, Zhu X. Using coding-based ensemble learning to improve software defect prediction. IEEE Trans Syst Man Cybern Part C
Appl Rev. 2012;42(6):1806-1817.
62. Jing X-Y, Wu F, Dong X, Xu B. An improved SDA based defect prediction framework for both within-project and cross-project
class-imbalance problems. IEEE Trans Softw Eng. 2017;43(4):321-339.
63. Zhu M, Martinez MA. Subclass discriminant analysis. IEEE Trans Pattern Anal Mach Intell. 2006;28(8):1274-1286.


>><[N]>An Implementation of Just-In-Time Fault-Prone Prediction Technique Using Text Classifier
[1] P. Bellini, I. Bruno, P. Nesi, and D. Rogai, “Comparing faultproneness
estimation models,” in Proc. of 10th IEEE International
Conference on Engineering of Complex Computer
Systems, 2005, pp. 205–214.
[2] T. Menzies, J. Greenwald, and A. Frank, “Data mining static
code attributes to learn defect predictors,” IEEE Trans. on
Software Engineering, vol. 33, no. 1, pp. 2–13, 2007.
[3] N. Seliya, T. M. Khoshgoftaar, and S. Zhong, “Analyzing
software quality with limited fault-proneness defect data,” in
Proc. of 9th IEEE International Symposium on High-Assurance
Systems Engineering, 2005, pp. 89–98.
[4] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus,
A. Sinha, and N. Ubayashi, “A large-scale empirical study of
just-in-time quality assurance,” IEEE Trans. Softw. Eng., vol. 39,
no. 6, pp. 757–770, 2013.
[5] O. Mizuno and T. Kikuno, “Training on errors experiment to
detect fault-prone software modules by spam filter,” in The 6th
Joint Meeting of the European Software Engineering Conference
and the ACM SIGSOFT Symposium on the Foundations of
Software Engineering (ESEC/FSE2007), 2007, pp. 405–414.
[6] The crm114 discriminator - the controllable regex. [Online].
Available: http://crm114.sourceforge.net/
[7] (2015, Mar.) FindbugsTM- find bugs in java programs. [Online].
Available: http://findbugs.sourceforge.net/
[8] (2015, Mar.) igrigorik/bugspots github. [Online]. Available:
https://github.com/igrigorik/bugspots
[9] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes
induce fixes?” in Proceedings of the 2005 International Workshop
on Mining Software Repositories. New York, NY, USA:
ACM, 2005, pp. 1–5.



>><[N]>CLEVER: Combining Code Metrics with Clone Detection for Just-In-Time Fault Prevention and Resolution in Large Industrial Projects
[1] Bhattacharya, P. and Neamtiu, I. 2011. Bug-fix time prediction
models: can we do better? Proceeding of the international conference
on mining software repositories (New York, New York, USA), 207–
210.
[2] Briand, L. et al. 1999. A unified framework for coupling measurement
in object-oriented systems. IEEE Transactions on Software Engineering.
25, 1 (1999), 91–121. DOI:https://doi.org/10.1109/32.748920.
[3] Bultena, B. and Ruskey, F. 1998. An Eades-McKay algorithm for
well-formed parentheses strings. Information Processing Letters. 68,
5 (1998), 255–259.
[4] Chen, T.-h. et al. 2014. An Empirical Study of Dormant Bugs
Categories and Subject Descriptors. Proceedings of the international
conference on mining software repository 82–91.
[5] Chidamber, S. and Kemerer, C. 1994. A metrics suite for object
oriented design. IEEE Transactions on Software Engineering. 20, 6
(Jun. 1994), 476–493. DOI:https://doi.org/10.1109/32.295895.
[6] Cordy, J.R. and Roy, C.K. 2011. The NiCad Clone Detector. Proceedings
of the international conference on program comprehension
219–220.
[7] Dallmeier, V. et al. 2009. Generating Fixes from Object Behavior
Anomalies. Proceedings of the international conference on automated
software engineering 550–554.
[8] Ducasse, S. et al. 1999. A Language Independent Approach
for Detecting Duplicated Code. Proceedings of the international
conference on software maintenance 109–118.
[9] El Emam, K. et al. 2001. The prediction of faulty classes using
object-oriented design metrics. Journal of Systems and Software.
56, 1 (Feb. 2001), 63–75. DOI:https://doi.org/10.1016/S0164-1212(00)
00086-8.
[10] Girvan, M. and Newman, M.E.J. 2002. Community structure in
social and biological networks. Proceedings of the National Academy
of Sciences. 99, 12 (Jun. 2002), 7821–7826. DOI:https://doi.org/10.
1073/pnas.122653799.
[11] Gyimothy, T. et al. 2005. Empirical validation of object-oriented
metrics on open source software for fault prediction. IEEE Transactions
on Software Engineering. 31, 10 (Oct. 2005), 897–910. DOI:https:
//doi.org/10.1109/TSE.2005.112.
[12] Hassan, A. and Holt, R. 2005. The top ten list: dynamic fault
prediction. Proceedings of the international conference on software
maintenance 263–272.
[13] Hassan, A.E. 2009. Predicting faults using the complexity of
code changes. Proceedings of the international conference on software
engineering 78–88.
[14] Hindle, A. et al. 2008. What do large commits tell us?: a taxonomical
study of large commits. Proceedings of the international
workshop on mining software repositories (New York, New York,
USA), 99–108.
[15] Hunt, J.W. and Szymanski, T.G. 1977. A fast algorithm for
computing longest common subsequences. Communications of the
ACM. 20, 5 (May 1977), 350–353. DOI:https://doi.org/10.1145/359581.
359603.
[16] Johnson, J.H. 1993. Identifying redundancy in source code
using fingerprints. Proceedings of the conference of the centre for
advanced studies on collaborative research 171–183.
[17] Johnson, J.H. 1994. Visualizing textual redundancy in legacy
source. Proceedings of the conference of the centre for advanced studies
on collaborative research 32.
[18] Kamei, Y. et al. 2013. A large-scale empirical study of just-intime
quality assurance. IEEE Transactions on Software Engineering.
39, 6 (Jun. 2013), 757–773. DOI:https://doi.org/10.1109/TSE.2012.70.
[19] Kapser, C. and Godfrey, M.W. 2003. Toward a Taxonomy of
Clones in Source Code: A Case Study. International workshop on
evolution of large scale industrial software architectures 67–78.
[20] Kim, D. et al. 2013. Automatic patch generation learned from
human-written patches. Proceedings of the international conference
on software engineering 802–811.
[21] Kim, S. et al. 2006. Automatic Identification of Bug-Introducing
Changes. Proceedings of the international conference on automated
software engineering 81–90.
[22] Kim, S. et al. 2007. Predicting Faults from Cached History.
Proceedings of the international conference on software engineering
489–498.
[23] Kpodjedo, S. et al. 2010. Design evolution metrics for defect
prediction in object oriented systems. Empirical Software Engineering.
16, 1 (Dec. 2010), 141–175. DOI:https://doi.org/10.1007/
s10664-010-9151-7.
[24] Le Goues, C. et al. 2012. A systematic study of automated
program repair: Fixing 55 out of 105 bugs for $8 each. Proceedings
of the international conference on software engineering 3–13.
[25] Le, X.-B.D. et al. 2015. Should fixing these failures be delegated
to automated program repair? Proceedings of the international
symposium on software reliability engineering 427–437.
[26] Lee, T. et al. 2011. Micro interaction metrics for defect prediction.
Proceedings of the european conference on foundations of
software engineering (New York, New York, USA), 311–231.
[27] Manber, U. 1994. Finding similar files in a large file system.
Proceedings of the usenix winter 1–10.
[28] Marcus, A. and Maletic, J. 2001. Identification of high-level
concept clones in source code. Proceedings international conference
on automated software engineering 107–114.
[29] Nagappan, N. and Ball, T. 2005. Static analysis tools as early
indicators of pre-release defect density. Proceedings of the international
conference on software engineering (New York, New York,
USA), 580–586.
[30] Nagappan, N. and Ball, T. 2005. Use of relative code churn
measures to predict system defect density. Proceedings of the international
conference on software engineering 284–292.
[31] Nagappan, N. et al. 2006. Mining metrics to predict component
failures. Proceeding of the international conference on software
engineering (New York, New York, USA), 452–461.
[32] Newman, M.E.J. and Girvan, M. 2004. Finding and evaluating
community structure in networks. Physical Review E. 69, 2 (Feb.
2004), 026113. DOI:https://doi.org/10.1103/PhysRevE.69.026113.
[33] Ostrand, T. et al. 2005. Predicting the location and number
of faults in large software systems. IEEE Transactions on Software
Engineering. 31, 4 (Apr. 2005), 340–355. DOI:https://doi.org/10.1109/
TSE.2005.49.
[34] Pan, K. et al. 2008. Toward an understanding of bug fix patterns.
Empirical Software Engineering. 14, 3 (Aug. 2008), 286–315.
DOI:https://doi.org/10.1007/s10664-008-9077-5.
[35] Rahman, F. and Devanbu, P. 2013. How, and why, process
metrics are better. Proceedings of the international conference on
software engineering 432–441.
[36] Rosen, C. et al. 2015. Commit guru: analytics and risk prediction
of software commits. Proceedings of the joint meeting on foundations
of software engineering (New York, New York, USA), 966–969.
[37] Roy, C. and Cordy, J. 2008. NICAD: Accurate Detection of Near-
Miss Intentional Clones Using Flexible Pretty-Printing and Code
Normalization. 2008 16th iEEE international conference on program
comprehension 172–181.
[38] Roy, C.K. 2009. Detection and Analysis of Near-Miss Software
Clones. Queen’s University.
[39] Shivaji, S. et al. 2013. Reducing Features to Improve Code
Change-Based Bug Prediction. IEEE Transactions on Software Engineering.
39, 4 (2013), 552–569.
[40] Subramanyam, R. and Krishnan, M. 2003. Empirical analysis of
CK metrics for object-oriented design complexity: implications for
software defects. IEEE Transactions on Software Engineering. 29, 4
(Apr. 2003), 297–310. DOI:https://doi.org/10.1109/TSE.2003.1191795.
[41] Sunghun Kim, S. et al. 2008. Classifying Software Changes:
Clean or Buggy? IEEE Transactions on Software Engineering. 34, 2
(Mar. 2008), 181–196. DOI:https://doi.org/10.1109/TSE.2007.70773.
[42] Tao, Y. et al. 2014. Automatically generated patches as debugging
aids: a human study. Proceedings of the international symposium
on foundations of software engineering 64–74.
[43] Tin Kam Ho 1995. Random decision forests. Proceedings of
the international conference on document analysis and recognition
278–282.
[44] Tin Kam Ho 1998. The random subspace method for constructing
decision forests. IEEE Transactions on Pattern Analysis and Machine
Intelligence. 20, 8 (1998), 832–844. DOI:https://doi.org/10.1109/
34.709601.
[45] Wettel, R. and Marinescu, R. 2005. Archeology of code duplication:
recovering duplication chains from small duplication
fragments. Proceedings of the seventh international symposium on
symbolic and numeric algorithms for scientific computing 63–71.
[46] Zimmermann, T. and Nagappan, N. 2008. Predicting defects
using network analysis on dependency graphs. Proceedings of the
international conference on software engineering (New York, New
York, USA), 531.
[47] Zimmermann, T. et al. 2007. Predicting Defects for Eclipse.
Proceedings of the international workshop on predictor models in
software engineering 9.



>><[N]>Deep Just-in-Time Defect Prediction: How Far Are We?
[1] 2020. Eclipse JDT adn Eclipse Platform. Website. https://git.eclipse.org/r/.
[2] 2020. Gerrit Code ReviewWebsite. Website. https://www.gerritcodereview.com/.
[3] 2020. Golang Code Review Website. Website. https://go-review.googlesource.
com/.
[4] 2020. OpenStack Code Review Website. Website. https://review.opendev.org/.
[5] 2020. QT Code Review Website. Website. https://codereview.qt-project.org/.
[6] George G Cabral, Leandro L Minku, Emad Shihab, and Suhaib Mujahid. 2019.
Class imbalance evolution and verification latency in just-in-time software defect
prediction. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 666ś676. https://doi.org/10.1109/ICSE.2019.00076
[7] Gemma Catolino, Dario Di Nucci, and Filomena Ferrucci. 2019. Cross-project
just-in-time bug prediction for mobile apps: an empirical assessment. In 2019
IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems
(MOBILESoft). IEEE, 99ś110.
[8] Xiang Chen, Yingquan Zhao, Qiuping Wang, and Zhidan Yuan. 2018. MULTI:
Multi-objective effort-aware just-in-time software defect prediction. Information
and Software Technology 93 (2018), 1ś13.
[9] Marco D’Ambros, Michele Lanza, and Romain Robbes. 2010. An extensive comparison
of bug prediction approaches. In 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR 2010). IEEE, 31ś41.
[10] Geanderson Esteves, Eduardo Figueiredo, Adriano Veloso, Markos Viggiato, and
Nivio Ziviani. 2020. Understanding machine learning software defect predictions.
Automated Software Engineering 27, 3 (2020), 369ś392.
[11] Yuanrui Fan, Xin Xia, Daniel Alencar da Costa, David Lo, Ahmed E Hassan, and
Shanping Li. 2019. The impact of changes mislabeled by SZZ on just-in-time
defect prediction. IEEE Transactions on Software Engineering (2019). https:
//doi.org/10.1109/TSE.2019.2929761
[12] John Fox. 1997. Applied regression analysis, linear models, and related methods.
Sage Publications, Inc.
[13] Todd L Graves, Alan F Karr, James S Marron, and Harvey Siy. 2000. Predicting
fault incidence using software change history. IEEE Transactions on software
engineering 26, 7 (2000), 653ś661. https://doi.org/10.1109/32.859533
[14] Philip J Guo, Thomas Zimmermann, Nachiappan Nagappan, and Brendan Murphy.
2010. Characterizing and predicting which bugs get fixed: an empirical study of
Microsoft Windows. In Proceedings of the 32Nd ACM/IEEE International Conference
on Software Engineering-Volume 1. 495ś504.
[15] Ahmed E Hassan. 2009. Predicting faults using the complexity of code changes.
In 2009 IEEE 31st international conference on software engineering. IEEE, 78ś88.
[16] Steffen Herbold, Alexander Trautsch, Fabian Trautsch, and Benjamin Ledel. 2019.
Issues with SZZ: An empirical assessment of the state of practice of defect
prediction data collection. arXiv preprint arXiv:1911.08938 (2019).
[17] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality
of data with neural networks. science 313, 5786 (2006), 504ś507.
[18] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu
Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-intime
defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR). IEEE, 34ś45. https://doi.org/10.1109/MSR.2019.00016
[19] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Distributed
representations of code changes. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering. 518ś529. https://doi.org/10.
1145/3377811.3380361
[20] Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E Hassan. 2016. Studying just-in-time defect
prediction using cross-project models. Empirical Software Engineering 21, 5 (2016),
2072ś2106. https://doi.org/10.1007/s10664-015-9400-x
[21] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2012. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2012), 757ś773. https://doi.org/10.1109/TSE.2012.70
[22] Sunghun Kim, Thomas Zimmermann, Kai Pan, E James Jr, et al. 2006. Automatic
identification of bug-introducing changes. In 21st IEEE/ACM international
conference on automated software engineering (ASE’06). IEEE, 81ś90.
[23] Ron Kohavi et al. 1995. A study of cross-validation and bootstrap for accuracy
estimation and model selection. In Ijcai, Vol. 14. Montreal, Canada, 1137ś1145.
[24] A Güneş Koru, Dongsong Zhang, Khaled El Emam, and Hongfang Liu. 2008. An
investigation into the functional form of the size-defect relationship for software
modules. IEEE Transactions on Software Engineering 35, 2 (2008), 293ś304.
[25] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature
521, 7553 (2015), 436ś444. https://doi.org/10.1038/nature14539
[26] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking classification models for software defect prediction: A proposed
framework and novel findings. IEEE Transactions on Software Engineering 34, 4
(2008), 485ś496. https://doi.org/10.1109/TSE.2008.35
[27] Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. 2017. Software defect prediction
via convolutional neural network. In 2017 IEEE International Conference
on Software Quality, Reliability and Security (QRS). IEEE, 318ś328. https:
//doi.org/10.1109/QRS.2017.42
[28] Weiwei Li,Wenzhou Zhang, Xiuyi Jia, and Zhiqiu Huang. 2020. Effort-Aware semi-
Supervised just-in-Time defect prediction. Information and Software Technology
126 (2020), 106364. https://doi.org/10.1016/j.infsof.2020.106364
[29] Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. 2017.
Code churn: A neglected metric in effort-aware just-in-time defect prediction. In
2017 ACM/IEEE International Symposium on Empirical Software Engineering and
Measurement (ESEM). IEEE, 11ś19. https://doi.org/10.1109/ESEM.2017.8
[30] Shinsuke Matsumoto, Yasutaka Kamei, Akito Monden, Ken-ichi Matsumoto, and
Masahide Nakamura. 2010. An analysis of developer metrics for fault prediction.
In Proceedings of the 6th International Conference on Predictive Models in Software
Engineering. 1ś9. https://doi.org/10.1145/1868328.1868356
[31] Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction. IEEE
Transactions on Software Engineering 44, 5 (2017), 412ś428.
[32] Audris Mockus and David M Weiss. 2000. Predicting risk of software changes.
Bell Labs Technical Journal 5, 2 (2000), 169ś180. https://doi.org/10.1002/bltj.2229
[33] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. 2008. A comparative
analysis of the efficiency of change metrics and static code attributes for defect
prediction. In Proceedings of the 30th international conference on Software
engineering. 181ś190. https://doi.org/10.1145/1368088.1368114
[34] Nachiappan Nagappan and Thomas Ball. 2005. Use of relative code churn measures
to predict system defect density. In Proceedings of the 27th international conference
on Software engineering. 284ś292. https://doi.org/10.1145/1062455.1062514
[35] Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. 2006. Mining metrics
to predict component failures. In Proceedings of the 28th international conference
on Software engineering. 452ś461. https://doi.org/10.1145/1134285.1134349
[36] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems
and Opportunities in Training Deep Learning Software Systems: An Analysis of
Variance. In 2020 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 771ś783.
[37] Ranjith Purushothaman and Dewayne E Perry. 2005. Toward understanding the
rhetoric of small source code changes. IEEE Transactions on Software Engineering
31, 6 (2005), 511ś526. https://doi.org/10.1109/TSE.2005.74
[38] Gema Rodríguez-Pérez, Gregorio Robles, and Jesús M González-Barahona. 2018.
Reproducibility and credibility in empirical software engineering: A case study
based on a systematic literature reviewof the use of the szz algorithm. Information
and Software Technology 99 (2018), 164ś176.
[39] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE international
conference on computer vision. 618ś626.
[40] Asaf Shabtai, Yuval Elovici, and Lior Rokach. 2012. A survey of data leakage
detection and prevention solutions. Springer Science & Business Media.
[41] Jacek Śliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When do
changes induce fixes? ACM sigsoft software engineering notes 30, 4 (2005), 1ś5.
[42] Sadia Tabassum, Leandro L Minku, Danyi Feng, George G Cabral, and Liyan
Song. [n.d.]. An Investigation of Cross-Project Learning in Online Just-In-Time
Software Defect Prediction. ([n. d.]).
[43] Chakkrit Tantithamthavorn, Ahmed E Hassan, and Kenichi Matsumoto. 2018.
The impact of class rebalancing techniques on the performance and interpretation
of defect prediction models. IEEE Transactions on Software Engineering (2018).
[44] Zhiyuan Wan, Xin Xia, Ahmed E Hassan, David Lo, Jianwei Yin, and Xiaohu
Yang. 2018. Perceptions, expectations, and challenges in defect prediction. IEEE
Transactions on Software Engineering (2018).
[45] SongWang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2018. Deep semantic feature
learning for software defect prediction. IEEE Transactions on Software Engineering
(2018). https://doi.org/10.1109/TSE.2018.2877612
[46] Tiejian Wang, Zhiwu Zhang, Xiaoyuan Jing, and Liqiang Zhang. 2016. Multiple
kernel ensemble learning for software defect prediction. Automated Software
Engineering 23, 4 (2016), 569ś590. https://doi.org/10.1007/s10515-015-0179-1
[47] Meng Yan, Xin Xia, Yuanrui Fan, Ahmed E Hassan, David Lo, and Shanping
Li. 2020. Just-In-Time Defect Identification and Localization: A Two-Phase
Framework. IEEE Transactions on Software Engineering (2020).
[48] Meng Yan, Xin Xia, Yuanrui Fan, David Lo, Ahmed E Hassan, and Xindong Zhang.
2020. Effort-aware just-in-time defect identification in practice: a case study
at Alibaba. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
1308ś1319. https://doi.org/10.1145/3368089.3417048
[49] Xinli Yang, David Lo, Xin Xia, and Jianling Sun. 2017. TLEL: A two-layer ensemble
learning approach for just-in-time defect prediction. Information and Software
Technology 87 (2017), 206ś220. https://doi.org/10.1016/j.infsof.2017.03.007
[50] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning
for just-in-time defect prediction. In 2015 IEEE International Conference on
Software Quality, Reliability and Security. IEEE, 17ś26.
[51] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard
Hovy. 2016. Hierarchical attention networks for document classification. In
Proceedings of the 2016 conference of the North American chapter of the association
for computational linguistics: human language technologies. 1480ś1489.
[52] Steven Young, Tamer Abdou, and Ayse Bener. 2018. A replication study: justin-
time defect prediction with ensemble learning. In Proceedings of the 6th International
Workshop on Realizing Artificial Intelligence Synergies in Software
Engineering. 42ś47. https://doi.org/10.1145/3194104.3194110
[53] Motahareh Bahrami Zanjani, Huzefa Kagdi, and Christian Bird. 2015. Automatically
recommending peer reviewers in modern code review. IEEE Transactions
on Software Engineering 42, 6 (2015), 530ś543.


>><N.>Deployment of a change-level software defect prediction solution into an industrial setting
1. Hall GA, Munson JC. Software evolution: code delta and code churn. J Syst Softw. 2000;54(2):111-118.
2. Kamei Y, Shihab E, Adams B, et al. A large-scale empirical study of just-in-time quality assurance. IEEE Trans Softw Eng. 2012;39(6):757-773.
3. Yang Y, Zhou Y, Liu J, et al. Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models. In: Proceedings
of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. 2016: 157–168.
4. Qiao L, Wang Y. Effort-aware and just-in-time defect prediction with neural network. PLoS ONE. 2019;14(2):e0211359.
5. Yu X, Bennin KE, Liu J, Keung JW, Yin X, Xu Z. An Empirical Study of Learning to Rank Techniques for Effort-Aware Defect Prediction. In: 2019 IEEE
26th International Conference on Software Analysis, Evolution and Reengineering (SANER); 2019: 298–309.
6. Mockus A, Weiss DM. Predicting risk of software changes. Bell Labs Tech J. 2000;5(2):169-180.
7. Czerwonka J, Das R, Nagappan N, Tarvo A, Teterev A. Crane: Failure prediction, change analysis and test prioritization in practice–experiences from
windows. In: 2011 Fourth IEEE International Conference on Software Testing, Verification and Validation; 2011: 357–366.
8. Tan M, Tan L, Dara S, Mayeux C. Online defect prediction for imbalanced data. In: 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering. 2015: 99–108.
9. Altinger H, Herbold S, Schneemann F, Grabowski J, Wotawa F. Performance tuning for automotive software fault prediction. In: 2017 IEEE 24th International
Conference on Software Analysis, Evolution and Reengineering (SANER). 2017: 526–530.
10. Yan M, Xia X, Fan Y, Lo D, Hassan AE, Zhang X. Effort-aware just-in-time defect identification in practice: a case study at Alibaba. In: Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2020:
1308–1319.
11. Tantithamthavorn C, Hassan AE. An experience report on defect modelling in practice: Pitfalls and challenges. In: Proceedings of the 40th International
Conference on Software Engineering: Software Engineering in Practice. 2018: 286–295.
12. Marijan D, Gotlieb A. Industry-Academia research collaboration in software engineering: The Certus model. Inf Softw Technol. 2021;132:106473.
13. Kamei Y, Shihab E. Defect prediction: Accomplishments and future challenges. In: 2016 IEEE 23rd international conference on software analysis, evolution,
and reengineering (SANER). 2016: 33–45.
14. Kang J, Ryu D, Baik J. Predicting just-in-time software defects to reduce post-release quality costs in the maritime industry. Softw: Pract Exp. 2021;
51(4):748-771.
15. Khanan C, Luewichana W, Pruktharathikoon K, et al. JITBot: An Explainable Just-In-Time Defect Prediction Bot. In: 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE). 2020: 1336–1339.
16. McIntosh S, Kamei Y. Are fix-inducing changes a moving target? a longitudinal case study of just-in-time defect prediction. IEEE Trans Softw Eng.
2017;44(5):412-428.
17. Bangash AA, Sahar H, Hindle A, Ali K. On the Time-Based Conclusion Stability of Software Defect Prediction Models. CoRR 2019.
18. Eken B, Atar R, Sertalp S, Tosun A. Predicting Defects with Latent and Semantic Features from Commit Logs in an Industrial Setting. In: 2019 34th
IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW). 2019: 98–105.
19. Cabral GG, Minku LL, Shihab E, Mujahid S. Class imbalance evolution and verification latency in just-in-time software defect prediction. In: 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). 2019: 666–676.
20. Hall T, Beecham S, Bowes D, Gray D, Counsell S. A systematic literature review on fault prediction performance in software engineering. IEEE Trans
Softw Eng. 2011;38(6):1276-1304.
21. Radjenovic D, Hericko M, Torkar R, Živkovic A. Software fault prediction metrics: A systematic literature review. Inf Softw Technol. 2013;55(8):1397-
1418.
22. Malhotra R. A systematic review of machine learning techniques for software fault prediction. Appl Soft Comput. 2015;27:504-518.
23. Pascarella L, Palomba F, Bacchelli A. Fine-grained just-in-time defect prediction. J Syst Softw. 2019;150:22-36.
24. Mockus A, Eick SG, Graves TL, Karr AF. On measurement and analysis of software changes. IEEE Trans Softw Eng. 1999;20:29.
25. Shihab E, Hassan AE, Adams B, Jiang ZM. An industrial study on the risk of software changes. In: Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering. 2012: 1–11.
26. Kim S, Whitehead EJ Jr, Zhang Y. Classifying software changes: Clean or buggy? IEEE Trans Softw Eng. 2008;34(2):181-196.
27. Ghotra B, McIntosh S, Hassan AE. Revisiting the impact of classification techniques on the performance of defect prediction models. In:. 1 of 2015
IEEE/ACM 37th IEEE International Conference on Software Engineering. IEEE. 2015: 789–800.
28. Huang Q, Xia X, Lo D. Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction. Empir Softw Eng. 2019;24(5):
2823-2862.
29. Misirli AT, Shihab E, Kamei Y. Studying high impact fix-inducing changes. Empir Softw Eng. 2016;21(2):605-641.
30. Pornprasit C, Tantithamthavorn C. JITLine: A Simpler, Better, Faster, Finer-grained Just-In-Time Defect Prediction. arXiv preprint arXiv:2103.07068
2021.
31. Kamei Y, Fukushima T, McIntosh S, Yamashita K, Ubayashi N, Hassan AE. Studying just-in-time defect prediction using cross-project models. Empir
Softw Eng. 2016;21(5):2072-2106.
32. Catolino G, Di Nucci D, Ferrucci F. Cross-project just-in-time bug prediction for mobile apps: an empirical assessment. In: 2019 IEEE/ACM 6th International
Conference on Mobile Software Engineering and Systems (MOBILESoft). 2019: 99–110.
33. Barnett JG, Gathuru CK, Soldano LS, McIntosh S. The relationship between commit message detail and defect proneness in java projects on github. In:
2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR). 2016: 496–499.
34. Yang X, Lo D, Xia X, Zhang Y, Sun J. Deep learning for just-in-time defect prediction. In: 2015 IEEE International Conference on Software Quality, Reliability
and Security. IEEE; 2015: 17–26.
35. Hoang T, Dam HK, Kamei Y, Lo D, Ubayashi N. DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction. In: 2019
IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). 2019: 34–45.
36. Xia X, Lo D, Wang X, Yang X. Collective personalized change classification with multiobjective search. IEEE Trans Reliab. 2016;65(4):1810-1829.
37. Yang X, Yu H, Fan G, Shi K, Chen L. Local versus Global Models for Just-In-Time Software Defect Prediction. Sci Prog. 2019;2019:1-13.
38. Jiang T, Tan L, Kim S. Personalized defect prediction. In: Automated Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on.
2013: 279–289.
39. Young S, Abdou T, Bener A. A replication study: just-in-time defect prediction with ensemble learning. In: Proceedings of the 6th International Workshop
on Realizing Artificial Intelligence Synergies in Software Engineering. 2018: 42–47.
40. Bergmeir C, Benıt́ ez JM. On the use of cross-validation for time series predictor evaluation. Inform Sci. 2012;191:192-213.
41. Shalev-Shwartz S, Ben-David S. Understanding machine learning: From theory to algorithms. Cambridge University Press; 2014.
42. Kim S, Whitehead Jr EJ. How long did it take to fix bugs?. In: Proceedings of the 2006 international workshop on Mining software repositories. 2006:
173–174.
43. Śliwerski J, Zimmermann T, Zeller A. When do changes induce fixes? In: No. 4 in ACM sigsoft software engineering notes. ACM; 2005:1-5.
44. Graves TL, Karr AF, Marron JS, Siy H. Predicting fault incidence using software change history. IEEE Trans Softw Eng. 2000;26(7):653-661.
45. Matsumoto S, Kamei Y, Monden A, Matsumoto K, Nakamura M. An analysis of developer metrics for fault prediction. In: Proceedings of the 6th International
Conference on Predictive Models in Software Engineering. ACM. 2010: 18.
46. Nagappan N, Ball T. Use of relative code churn measures to predict system defect density. In: Software Engineering, 2005. ICSE 2005. Proceedings.
27th International Conference on. 2005: 284–292.
47. D'Ambros M, Lanza M, Robbes R. An extensive comparison of bug prediction approaches. In: 2010 7th IEEE Working Conference on Mining Software
Repositories (MSR 2010). 2010: 31–41.
48. Nagappan N, Ball T, Zeller A. Mining metrics to predict component failures. In: Proceedings of the 28th international conference on Software engineering.
2006: 452–461.
49. Lee DD, Seung HS. Learning the parts of objects by non-negative matrix factorization. Nature. 1999;401(6755):788-791.
50. Quinlan JR. Induction of decision trees. Mach Learn. 1986;1(1):81-106.
51. Murphy KP. Machine learning: a probabilistic perspective. MIT Press; 2012.
52. Alpaydin E. Introduction to machine learning. MIT Press; 2009.
53. Lessmann S, Baesens B, Mues C, Pietsch S. Benchmarking classification models for software defect prediction: A proposed framework and novel findings.
IEEE Trans Softw Eng. 2008;34(4):485-496.
54. Matthews BW. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophys Acta (BBA)-Protein Struct.
1975;405(2):442-451.
55. Hanley JA, McNeil BJ. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology. 1982;143(1):29-36.
56. Menzies T, Greenwald J, Frank A. Data mining static code attributes to learn defect predictors. IEEE Trans Softw Eng. 2007;33(1):2-13.
57. Tharwat A. Classification assessment methods. Appl Comput Inf. 2020.
58. Davis J, Goadrich M. The relationship between Precision-Recall and ROC curves. In: Proceedings of the 23rd international conference on Machine
learning. 2006: 233–240.
59. Menzies T, Dekhtyar A, Distefano J, Greenwald J. Problems with Precision: A Response to" comments on'data mining static code attributes to learn
defect predictors'". IEEE Trans Softw Eng. 2007;33(9):637-640.
60. Turhan B, Menzies T, Bener AB, Di Stefano J. On the relative value of cross-company and within-company data for defect prediction. Empir Softw Eng.
2009;14(5):540-578.
61. Turhan B, Mısırlı AT, Bener A. Empirical evaluation of the effects of mixed project data on learning defect predictors. Inf Softw Technol. 2013;55(6):
1101-1118.
62. Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP. SMOTE: synthetic minority over-sampling technique. J Artif Intell Res. 2002;16:321-357.
63. Guillamet D, Vitrià J. Non-negative matrix factorization for face recognition. In: Catalonian Conference on Artificial Intelligence. Berlin, Heidelberg:
Springer; 2002:336-344.
64. Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. IEEE Comput. 2009;8:30-37.
65. Borg M, Svensson O, Berg K, Hansson D. SZZ Unleashed: An Open Implementation of the SZZ Algorithm - Featuring Example Usage in a Study of
Just-in-time Bug Prediction for the Jenkins Project. In: MaLTeSQuE 2019. ACM; 2019: 7–12.
66. Fan Y, Xia X, Costa DDA, Lo D, Hassan AE, Li S. The Impact of Changes Mislabeled by SZZ on Just-in-Time Defect Prediction. IEEE Trans Softw Eng.
2019.
67. Nugroho YS, Hata H, Matsumoto K. How different are different diff algorithms in Git? Empir Softw Eng. 2020;25(1):790-823.
68. Tantithamthavorn C, McIntosh S, Hassan AE, Matsumoto K. An empirical comparison of model validation techniques for defect prediction models.
IEEE Trans Softw Eng. 2017;43(1):1-18.
69. Avazpour I, Pitakrat T, Grunske L, Grundy J. Dimensions and metrics for evaluating recommendation systems. Springer; 2014:245-273.
70. Northcutt C, Jiang L, Chuang I. Confident learning: Estimating uncertainty in dataset labels. J Artif Intell Res. 2021;70:1373-1411.
  
  
  
  
>><[N]>Does chronology matter in JIT defect prediction? A Partial Replication Study  
[1] Leo Breiman. 2001. Random Forests. Mach. Learn. 45, 1 (Oct. 2001), 5–32. https:
//doi.org/10.1023/A:1010933404324
[2] M. D’Ambros, M. Lanza, and R. Robbes. 2010. An extensive comparison of bug
prediction approaches. In 2010 7th IEEE Working Conference on Mining Software
Repositories (MSR 2010). 31–41. https://doi.org/10.1109/MSR.2010.5463279
[3] Francis X. Diebold and Roberto S. Mariano. 1995. Comparing Predictive
Accuracy. Journal of Business & Economic Statistics 13, 3
(1995), 253–263. https://doi.org/10.1080/07350015.1995.10524599
arXiv:https://amstat.tandfonline.com/doi/pdf/10.1080/07350015.1995.10524599
[4] Emanuel Giger, Marco D’Ambros, Martin Pinzger, and Harald C. Gall. 2012.
Method-level Bug Prediction. In Proceedings of the ACM-IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM ’12). ACM,
New York, NY, USA, 171–180. https://doi.org/10.1145/2372251.2372285
[5] Tilmann Gneiting and Adrian E Raftery. 2007. Strictly Proper Scoring
Rules, Prediction, and Estimation. J. Amer. Statist. Assoc. 102,
477 (2007), 359–378. https://doi.org/10.1198/016214506000001437
arXiv:https://doi.org/10.1198/016214506000001437
[6] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy. 2000. Predicting fault incidence
using software change history. IEEE Transactions on Software Engineering 26, 7
(July 2000), 653–661. https://doi.org/10.1109/32.859533
[7] Philip J. Guo, Thomas Zimmermann, Nachiappan Nagappan, and Brendan Murphy.
2010. Characterizing and Predicting Which Bugs Get Fixed: An Empirical
Study of Microsoft Windows. In Proceedings of the 32Nd ACM/IEEE International
Conference on Software Engineering - Volume 1 (ICSE ’10). ACM, New York, NY,
USA, 495–504. https://doi.org/10.1145/1806799.1806871
[8] David J. Hand. 2009. Measuring Classifier Performance: A Coherent Alternative
to the Area Under the ROC Curve. Mach. Learn. 77, 1 (Oct. 2009), 103–123.
https://doi.org/10.1007/s10994-009-5119-5
[9] A. E. Hassan. 2009. Predicting faults using the complexity of code changes. In
2009 IEEE 31st International Conference on Software Engineering. 78–88. https:
//doi.org/10.1109/ICSE.2009.5070510
[10] Hideaki Hata, Osamu Mizuno, and Tohru Kikuno. 2012. Bug Prediction Based on
Fine-grained Module Histories. In Proceedings of the 34th International Conference
on Software Engineering (ICSE ’12). IEEE Press, Piscataway, NJ, USA, 200–210.
http://dl.acm.org/citation.cfm?id=2337223.2337247
[11] Malley JD, Kruppa J, Dasgupta A, Malley KG, and Ziegler A. 2012. Probability machines:
consistent probability estimation using nonparametric learning machines.
Methods of Information in Medicine 51, 1 (2012), 74–81.
[12] Yasutaka Kamei, Takafumi Fukushima, Shane Mcintosh, Kazuhiro Yamashita,
Naoyasu Ubayashi, and Ahmed E. Hassan. 2016. Studying Just-in-time Defect
Prediction Using Cross-project Models. Empirical Softw. Engg. 21, 5 (Oct. 2016),
2072–2106. https://doi.org/10.1007/s10664-015-9400-x
[13] Y. Kamei, S. Matsumoto, A. Monden, K. Matsumoto, B. Adams, and A. E. Hassan.
2010. Revisiting common bug prediction findings using effort-aware models.
In 2010 IEEE International Conference on Software Maintenance. 1–10. https:
//doi.org/10.1109/ICSM.2010.5609530
[14] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha, and N. Ubayashi.
2013. A large-scale empirical study of just-in-time quality assurance. IEEE
Transactions on Software Engineering 39, 6 (June 2013), 757–773. https://doi.org/
10.1109/TSE.2012.70
[15] S. Kim, E. J. Whitehead, Jr., and Y. Zhang. 2008. Classifying Software Changes:
Clean or Buggy? IEEE Transactions on Software Engineering 34, 2 (March 2008),
181–196. https://doi.org/10.1109/TSE.2007.70773
[16] A. G. Koru, D. Zhang, K. El Emam, and H. Liu. 2009. An Investigation into the
Functional Form of the Size-Defect Relationship for Software Modules. IEEE
Transactions on Software Engineering 35, 2 (March 2009), 293–304. https://doi.
org/10.1109/TSE.2008.90
[17] Paul Luo Li, James Herbsleb, Mary Shaw, and Brian Robinson. 2006. Experiences
and Results from Initiating Field Defect Prediction and Product Test Prioritization
Efforts at ABB Inc.. In Proceedings of the 28th International Conference on Software
Engineering (ICSE ’06). ACM, New York, NY, USA, 413–422. https://doi.org/10.
1145/1134285.1134343
[18] Shinsuke Matsumoto, Yasutaka Kamei, Akito Monden, Ken-ichi Matsumoto,
and Masahide Nakamura. 2010. An Analysis of Developer Metrics for Fault
Prediction. In Proceedings of the 6th International Conference on Predictive Models
in Software Engineering (PROMISE ’10). ACM, New York, NY, USA, Article 18,
9 pages. https://doi.org/10.1145/1868328.1868356
[19] S. McIntosh and Y. Kamei. 2018. Are Fix-Inducing Changes a Moving Target? A
Longitudinal Case Study of Just-In-Time Defect Prediction. IEEE Transactions
on Software Engineering 44, 5 (May 2018), 412–428. https://doi.org/10.1109/TSE.
2017.2693980
[20] A. Mockus and D. M. Weiss. 2000. Predicting risk of software changes. Bell Labs
Technical Journal 5, 2 (April 2000), 169–180. https://doi.org/10.1002/bltj.2229
[21] Nachiappan Nagappan and Thomas Ball. 2005. Use of Relative Code Churn
Measures to Predict System Defect Density. In Proceedings of the 27th International
Conference on Software Engineering (ICSE ’05). ACM, New York, NY, USA, 284–292.
https://doi.org/10.1145/1062455.1062514
[22] Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. 2006. Mining Metrics
to Predict Component Failures. In Proceedings of the 28th International Conference
on Software Engineering (ICSE ’06). ACM, New York, NY, USA, 452–461. https:
//doi.org/10.1145/1134285.1134349
[23] Foyzur Rahman, Daryl Posnett, Israel Herraiz, and Premkumar Devanbu. 2013.
Sample Size vs. Bias in Defect Prediction. In Proceedings of the 2013 9th Joint
Meeting on Foundations of Software Engineering (ESEC/FSE 2013). ACM, New York,
NY, USA, 147–157. https://doi.org/10.1145/2491411.2491418
[24] Gema Rodríguez-Pérez, Gregorio Robles, and Jesús M. González-Barahona.
2018. Reproducibility and credibility in empirical software engineering: A
case study based on a systematic literature review of the use of the SZZ algorithm.
Information and Software Technology 99 (2018), 164 – 176. https:
//doi.org/10.1016/j.infsof.2018.03.009
[25] Shaoqing Ren, X. Cao, YichenWei, and J. Sun. 2015. Global refinement of random
forest. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
723–730. https://doi.org/10.1109/CVPR.2015.7298672
[26] Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007.
Bias in random forest variable importance measures: illustrations, sources and a
solution. BMC Bioinformatics 8, 25 (Oct. 2007), 1–25.
[27] Thomas Zimmermann, Rahul Premraj, and Andreas Zeller. 2007. Predicting
Defects for Eclipse. In Proceedings of the Third International Workshop on Predictor
Models in Software Engineering (PROMISE ’07). IEEE Computer Society,
Washington, DC, USA, 9–. https://doi.org/10.1109/PROMISE.2007.10

>><[N]>Effort-Aware Just-in-Time Bug Prediction for Mobile Apps Via Cross-Triplet Deep Feature Embedding
[1] G. Catolino, D. Di Nucci, and F. Ferrucci, “Cross-project just-in-time bug
prediction for mobile apps: An empirical assessment,” in Proc. 6th Int.
Conf. Mobile Softw. Eng. Syst., 2019, pp. 99–110.
[2] X. Kong, L. Zhang, W. E. Wong, and B. Li, “Experience report: How
do techniques, programs, and tests impact automated program repair?” in
Proc. 26th Int. Symp. Softw. Rel. Eng., 2015, pp. 194–204.
[3] G. Catolino, “Just-in-time bug prediction in mobile applications: The
domain matters!” in Proc. 4th Int. Conf. Mobile Softw. Eng. Syst., 2017,
pp. 201–202.
[4] M.Yan, X. Xia,Y. Fan, A. E. Hassan, D. Lo, and S. Li, “Just-in-time defect
identification and localization: A two-phase framework,” IEEE Trans.
Softw. Eng., 2020, doi: 10.1109/TSE.2020.2978819.
[5] M. Yan, X. Xia, Y. Fan, D. Lo, A. E. Hassan, and X. Zhang, “Effort-aware
just-in-time defect identification in practice: A case study at Alibaba,” in
Proc. 28th ACM Joint Meeting Eur. Softw. Eng. Conf. Symp. Foundations
Softw. Eng., 2020, pp. 1308–1319.
[6] Z. Xu et al., “LDFR: Learning deep feature representation for software
defect prediction,” J. Syst. and Softw., vol. 158, 2019, Art. no. 110402.
[7] S. Jiang,Y.Wu, andY. Fu, “Deep bi-directional cross-triplet embedding for
cross-domain clothing retrieval,” in Proc. 24th ACMInt. Conf. Multimedia,
2016, pp. 52–56.
[8] E. Arisholm, L. C. Briand, and M. Fuglerud, “Data mining techniques for
building fault-proneness models in telecom java software,” in Proc. 18th
IEEE Int. Symp. Softw. Rel. Eng., 2007, pp. 215–224.
[9] Y. Kamei et al., “A large-scale empirical study of just-in-time quality
assurance,” IEEE Trans. Softw. Eng., vol. 39, no. 6, pp. 757–773, Jun. 2013.
[10] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
“An empirical study of just-in-time defect prediction using cross-project
models,” in Proc. 11th Work. Conf. Mining Softw. Repositories, 2014,
pp. 172–181.
[11] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi, and
A. E. Hassan, “Studying just-in-time defect prediction using cross-project
models,” Empirical Softw. Eng., vol. 21, no. 5, pp. 2072–2106, 2016.
[12] Y. Yang et al., “Effort-aware just-in-time defect prediction: Simple unsupervised
models could be better than supervised models,” in Proc. 24th
ACM SIGSOFT Int. Symp. Foundations Softw. Eng., 2016, pp. 157–168.
[13] P. Tourani and B. Adams, “The impact of human discussions on just-intime
quality assurance: An empirical study on openstack and eclipse,”
in Proc. 23rd Int. Conf. Softw. Anal. Evol. Reengineering, 2016, vol. 1,
pp. 189–200.
[14] Q. Huang, X. Xia, and D. Lo, “Supervised vs unsupervised models: A
holistic look at effort-aware just-in-time defect prediction,” in Proc. 33 rd
Int. Conf. Softw. Maintenance Evol., 2017, pp. 159–170.
[15] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Softw.
Eng., vol. 24, no. 5, pp. 2823–2862, 2019.
[16] X. Chen, Y. Zhao, Q. Wang, and Z. Yuan, “Multi: Multi-objective effortaware
just-in-time software defect prediction,” Inf. Softw. Technol., vol. 93,
pp. 1–13, 2018.
[17] X.Yang, D. Lo, X. Xia, and J. Sun, “TLEL:Atwo-layer ensemble learning
approach for just-in-time defect prediction,” Inf. Softw. Technol., vol. 87,
pp. 206–220, 2017.
[18] M. Kondo, D. M. German, O. Mizuno, and E.-H. Choi, “The impact of
context metrics on just-in-time defect prediction,” Empirical Softw. Eng.,
vol. 25, no. 1, pp. 890–939, 2020.
[19] Y. Fan, X. Xia, D. A. da Costa, D. Lo, A. E. Hassan, and S. Li, “The
impact of changes mislabeled by SZZ on just-in-time defect prediction,”
IEEE Trans. Softw. Eng., 2019, doi: 10.1109/TSE.2019.2929761.
[20] T. Mende and R. Koschke, “Revisiting the evaluation of defect prediction
models,” in Proc. 5th Int. Conf. Predictor Models Softw. Eng., 2009, pp. 1–
10.
[21] T. Mende and R. Koschke, “Effort-aware defect prediction models,” in
Proc. 14th Eur. Conf. Softw. Maintenance Reengineering, 2010, pp. 107–
116.
[22] E. Shihab, Y. Kamei, B. Adams, and A. E. Hassan, “Is lines of code a good
measure of effort in effort-aware models?” Inf. Softw. Technol., vol. 55,
no. 11, pp. 1981–1993, 2013.
[23] Y. Yang et al., “Are slice-based cohesion metrics actually useful in effortaware
post-release fault-proneness prediction? an empirical study,” IEEE
Trans. Softw. Eng., vol. 41, no. 4, pp. 331–357, Apr. 2015.
[24] Y.Yang et al., “An empirical study on dependence clusters for effort-aware
fault-proneness prediction,” in Proc. 31st Int. Conf. Automated Softw. Eng.,
2016, pp. 296–307.
[25] K. E. Bennin, J. Keung, A. Monden, Y. Kamei, and N. Ubayashi, “Investigating
the effects of balanced training and testing datasets on effort-aware
fault prediction models,” in Proc. 40th Annu. Comput. Softw. Appl. Conf.,
2016, vol. 1, pp. 154–163.
[26] W. Ma, L. Chen, Y. Yang, Y. Zhou, and B. Xu, “Empirical analysis of
network measures for effort-aware fault-proneness prediction,” Inf. Softw.
Technol., vol. 69, pp. 50–70, 2016.
[27] J. Liu, Y. Zhou, Y. Yang, H. Lu, and B. Xu, “Code churn: A neglected
metric in effort-aware just-in-time defect prediction,” in Proc. 11th Int.
Symp. Empirical Softw. Eng. Meas., 2017, pp. 11–19.
[28] X. Yu, K. E. Bennin, J. Liu, J. W. Keung, X. Yin, and Z. Xu, “An
empirical study of learning to rank techniques for effort-aware defect prediction,”
in Proc. 26th Int. Conf. Softw. Anal., Evol. Reengineering, 2019,
pp. 298–309.
[29] S. Albahli, “A deep ensemble learning method for effort-aware just-in-time
defect prediction,” Future Internet, vol. 11, no. 12, pp. 1–13, 2019.
[30] L. Qiao andY.Wang, “Effort-aware and just-in-time defect prediction with
neural network,” PLoS One, vol. 14, no. 2, pp. 1–19. 2019.
[31] W. Li, W. Zhang, X. Jia, and Z. Huang, “Effort-aware semi-supervised
just-in-time defect prediction,” Inf. Softw. Technol., vol. 126, 2020,
Art. no. 106364.
[32] R. Scandariato and J.Walden, “Predicting vulnerable classes in an android
application,” in Proc. 4th Int. Workshop Secur. Meas. Metrics, 2012,
pp. 11–16.
[33] A. Kaur, K. Kaur, and H. Kaur, “An investigation of the accuracy of code
and process metrics for defect prediction of mobile applications,” in Proc.
4th Int. Conf. Reliability Infocom Technol. Optim., 2015, pp. 1–6.
[34] A. Kaur, K. Kaur, and H. Kaur, “Application of machine learning on
process metrics for defect prediction in mobile application,” in Inf. Syst.
Design Intell. Appl.. Springer, 2016, pp. 81–98.
[35] R. Malhotra, “An empirical framework for defect prediction using machine
learning techniques with android software,” Appl. Soft Comput., vol. 49,
pp. 1034–1050, 2016.
[36] M. Y. Ricky, F. Purnomo, and B. Yulianto, “Mobile application software
defect prediction,” in Proc. 10th IEEE Symp. Service-Oriented Syst. Eng.,
2016, pp. 307–313.
[37] N. Gayatri, S. Nickolas, and A. Reddy, “A frame work for business defect
predictions in mobiles,” Int. J. Comput. Appl., vol. 975, pp. 39–44, 2013.
[38] Y. Fan, X. Cao, J. Xu, S. Xu, and H. Yang, “High-frequency keywords
to predict defects for android applications,” in Proc. 42nd Annu. Comput.
Softw. Appl. Conf., 2018, vol. 2, pp. 442–447.
[39] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for justin-
time defect prediction,” in Proc. IEEE Int. Conf. Softw. Quality, Rel.
Secur., 2015, pp. 17–26.
[40] C. Manjula and L. Florence, “Deep neural network based hybrid approach
for software defect prediction using software metrics,” Cluster Comput.,
vol. 22, no. 4, pp. 9847–9863, 2019.
[41] L. Qiao, X. Li, Q. Umer, and P. Guo, “Deep learning based software defect
prediction,” Neurocomputing, vol. 385, pp. 100–110, 2020.
[42] A. Hasanpour, P. Farzi, A. Tehrani, and R. Akbari, “Software defect
prediction based on deep learning models: Performance study,” 2020,
arXiv:2004.02589.>
[43] S.Wang, T. Liu, and L. Tan, “Automatically learning semantic features for
defect prediction,” in Proc. 38th Int. Conf. Softw. Eng., 2016, pp. 297–308.
[44] S. Wang, T. Liu, J. Nam, and L. Tan, “Deep semantic feature learning
for software defect prediction,” IEEE Trans. Softw. Eng., vol. 46, no. 12,
pp. 1267–1293, Dec. 2020.
[45] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via
convolutional neural network,” in Proc. IEEE Int. Conf. Softw. Quality
Rel. Secur., 2017, pp. 318–328.
[46] H. K.Damet al., “A deep tree-based model for software defect prediction,”
2018, arXiv:1802.00921.
[47] A. V. Phan, M. Le Nguyen, and L. T. Bui, “Convolutional neural networks
over control flow graphs for software defect prediction,” in Proc. 29th Int.
Conf. Tools Artif. Intell., 2017, pp. 45–52.
[48] D. Chen, X. Chen, H. Li, J. Xie, and Y. Mu, “DeepCPDP: Deep learning
based cross-project defect prediction,” IEEE Access, vol. 7, pp. 184 832–
184848, 2019.
[49] G. Fan, X. Diao, H. Yu, K. Yang, and L. Chen, “Deep semantic feature
learning with embedded static metrics for software defect prediction,” in
Proc. 26th Asia-Pacific Softw. Eng. Conf., 2019, pp. 244–251.
[50] S. Jiang, Y. Wu, and Y. Fu, “Deep bidirectional cross-triplet embedding
for online clothing shopping,” ACMTrans. Multimedia Comput.,Commun.
Appl., vol. 14, no. 1, pp. 1–22, 2018.
[51] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Proc. Adv. Neural Inf. Process.
Syst., 2012, pp. 1097–1105.
[52] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in Proc. 3rd Int. Conf. Learn. Representations (ICLR), 2015, pp. 1–15.
[53] M.Yan,Y. Fang, D. Lo, X. Xia, and X. Zhang, “File-level defect prediction:
Unsupervised vs. supervised models,” in Proc. 11th Int. Symp. Empirical
Softw. Eng. Meas., 2017, pp. 344–353.
[54] Z. Xu et al., “Cross version defect prediction with representative data via
sparse subset selection,” in Proc. 26th Int. Conf. Prog. Comprehension,
2018, pp. 132–143.
[55] Z. Xu et al., “TSTSS: A two-stage training subset selection framework for
cross version defect prediction,” J. Syst. Softw., vol. 154, pp. 59–78, 2019.
[56] J. Nam,W. Fu, S. Kim, T. Menzies, and L. Tan, “Heterogeneous defect prediction,”
IEEE Trans. Softw. Eng., vol. 44, no. 9, pp. 874–896, Sep. 2018.
[57] J. Nam and S. Kim, “CLAMI: Defect prediction on unlabeled datasets (t),”
in Proc. 30th Int. Conf. Automated Softw. Eng., 2015, pp. 452–463.
[58] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in Proc. 35th
Int. Conf. Softw. Eng., 2013, pp. 382–391.
[59] X. Xia, D. Lo, S. J. Pan, N. Nagappan, and X. Wang, “Hydra: Massively
compositional model for cross-project defect prediction,” IEEE Trans.
Softw. Eng., vol. 42, no. 10, pp. 977–998, Oct. 2016.
[60] J. Demšar, “Statistical comparisons of classifiers over multiple data sets,”
J. Mach. Learn. Res., vol. 7, no. Jan, pp. 1–30, 2006.
[61] S. Herbold, A. Trautsch, and J. Grabowski, “A comparative study to benchmark
cross-project defect prediction approaches,” IEEE Trans. Softw. Eng.,
vol. 44, no. 9, pp. 811–833, Sep. 2018.
[62] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking classification
models for software defect prediction: A proposed framework
and novel findings,” IEEE Trans. Softw. Eng., vol. 34, no. 4, pp. 485–496,
Jul.–Aug. 2008.
[63] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Softw. Eng., vol. 14, no. 5, pp. 540–578, 2009.
[64] F. Peters, T. Menzies, and A. Marcus, “Better cross company defect
prediction,” in Proc. 10th Work. Conf. Mining Softw. Repositories, 2013,
pp. 409–418.
[65] X. Yu, P. Zhou, J. Zhang, and J. Liu, “A data filtering method based on
agglomerative clustering,” in Proc. 29th Int. Conf. Softw. Eng. Knowl. Eng.,
2017, pp. 392–397.
[66] P. He, B. Li, and Y. Ma, “Towards cross-project defect prediction with
imbalanced feature sets,” 2014, arXiv:1411.4228.
[67] Z. He, F. Shu, Y. Yang, M. Li, and Q. Wang, “An investigation on the
feasibility of cross-project defect prediction,” Automated Softw. Eng.,
vol. 19, no. 2, pp. 167–199, 2012.
[68] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation
via transfer component analysis,” Trans. Neural Netw., vol. 22, no. 2,
pp. 199–210, 2011.
[69] Z. Xu et al., “Cross project defect prediction via balanced distribution
adaptation based transfer learning,” J. Comput. Sci. Technol., vol. 34, no. 5,
pp. 1039–1062, 2019.
[70] M. Long, J.Wang, G. Ding, J. Sun, and P. S. Yu, “Transfer feature learning
with joint distribution adaptation,” in Proc. 14th Int. Conf. Comput. Vis.,
2013, pp. 2200–2207.
[71] Y. Zhang, D. Lo, X. Xia, and J. Sun, “An empirical study of classifier
combination for cross-project defect prediction,” in Proc. 39th Annu.
Comput. Softw. Appl. Conf., 2015, vol. 2, pp. 264–269.
[72] A. Panichella, R. Oliveto, and A. De Lucia, “Cross-project defect prediction
models: L’union fait la force,” in Proc. 21st Softw. Evol. Week Conf.
Softw. Maintenance, Reengineering Reverse Eng., 2014, pp. 164–173.
[73] D. Di Nucci, F. Palomba, and A. De Lucia, “Evaluating the adaptive
selection of classifiers for cross-project bug prediction,” in Proc.
6th Int. Workshop Realizing Artif. Intell. Synergies Softw. Eng., 2018,
pp. 48–54.
[74] Z. Li, X.-Y. Jing, F. Wu, X. Zhu, B. Xu, and S. Ying, “Cost-sensitive
transfer kernel canonical correlation analysis for heterogeneous defect
prediction,” Automated Softw. Eng., vol. 25, no. 2, pp. 201–245,
2018.
[75] Z. Xu et al., “Identifying crashing fault residence based on cross
project model,” in Proc. 30th Int. Symp. Softw. Rel. Eng., 2019,
pp. 183–194.
[76] H. Chen, X.-Y. Jing, Z. Li, D. Wu, Y. Peng, and Z. Huang, “An empirical
study on heterogeneous defect prediction approaches,” IEEE Trans. Softw.
Eng., 2020, doi: 10.1109/TSE.2020.2968520.
[77] R. Ferenc, P. Gyimesi, G. Gyimesi, Z. Tóth, and T. Gyimóthy, “An automatically
created novel bug dataset and its validation in bug prediction,”
J. Syst. and Softw., vol. 169, 2020, Art. no. 110691.


>><[N]>Enhancing Just-in-Time Defect Prediction Using Change Request-based Metrics
[1] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2012.
[2] S. Albahli, “A deep ensemble learning method for effort-aware just-intime
defect prediction,” Future Internet, vol. 11, no. 12, p. 246, 2019.
[3] L. Qiao and Y. Wang, “Effort-aware and just-in-time defect prediction
with neural network,” PloS one, vol. 14, no. 2, p. e0211359, 2019.
[4] D. Sharma and P. Chandra, “Efficient fault prediction using exploratory
and causal techniques,” in 2018 Second World Conference on Smart
Trends in Systems, Security and Sustainability (WorldS4). IEEE, 2018,
pp. 193–197.
[5] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for justin-
time defect prediction,” in 2015 IEEE International Conference on
Software Quality, Reliability and Security. IEEE, 2015, pp. 17–26.
[6] L. Qiao, X. Li, Q. Umer, and P. Guo, “Deep learning based software
defect prediction,” Neurocomputing, vol. 385, pp. 100–110, 2020.
[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
synthetic minority over-sampling technique,” Journal of artificial intelligence
research, vol. 16, pp. 321–357, 2002.
[8] J. Sola and J. Sevilla, “Importance of input data normalization for the
application of neural networks to complex industrial problems,” IEEE
Transactions on nuclear science, vol. 44, no. 3, pp. 1464–1468, 1997.
[9] J. Nam, “Survey on software defect prediction,” Department of Compter
Science and Engineerning, The Hong Kong University of Science and
Technology, Tech. Rep, 2014.
[10] Y. A. Alshehri, K. Goseva-Popstojanova, D. G. Dzielski, and T. Devine,
“Applying machine learning to predict software fault proneness using
change metrics, static code metrics, and a combination of them,” in
SoutheastCon 2018. IEEE, 2018, pp. 1–7.
[11] K. Beck, M. Fowler, and G. Beck, “Bad smells in code,” Refactoring:
Improving the design of existing code, vol. 1, pp. 75–88, 1999.
[12] G. Catolino, F. Palomba, F. A. Fontana, A. De Lucia, A. Zaidman,
and F. Ferrucci, “Improving change prediction models with code smellrelated
information,” Empirical Software Engineering, vol. 25, no. 1, pp.
49–95, 2020.
[13] K. Muthukumaran, A. Choudhary, and N. B. Murthy, “Mining github
for novel change metrics to predict buggy files in software systems,”
in 2015 International Conference on Computational Intelligence and
Networks. IEEE, 2015, pp. 15–20.
[14] A. Bacchelli, M. D’Ambros, and M. Lanza, “Are popular classes more
defect prone?” in International Conference on Fundamental Approaches
to Software Engineering. Springer, 2010, pp. 59–73.



>><A(Y)>Evaluating the impact of falsely detected performance bug-inducing changes in JIT models
Agrawal A, Menzies T (2018) Is ”better data” better than ”better dataminers”? on the benefits of tuning smote
for defect prediction. In: Proceedings of the 40th International Conference on Software Engineering,
ser. ICSE ’18. Association for Computing Machinery, New York, , pp 1050–1061. [Online]. Available:
https://doi.org/10.1145/3180155.3180197
Apache apache/cassandra (2019) [Online]. Available: https://github.com/apache/cassandra
Apache hadoop (2020) [Online]. Available: https://hadoop.apache.org/
Bryant RE, O’Hallaron DR (2015) Computer Systems: A Programmer’s Perspective, 3rd ed. Pearson
Catolino G (2017) Just-in-time bug prediction in mobile applications: The domain matters! pp 05
Catolino G, Di Nucci D, Ferrucci F (2019) Cross-project just-in-time bug prediction for mobile apps:
An empirical assessment. In: 2019 IEEE/ACM 6th International Conference on Mobile Software
Engineering and Systems (MOBILESoft), pp 99–110
Chen J, ShangW(2017) An exploratory study of performance regression introducing code changes. In: 2017
IEEE International Conference on Software Maintenance and Evolution (ICSME), pp 341–352
Chen T.-H., Shang W, Jiang ZM, Hassan AE, Nasser M, Flora P (2014) Detecting performance anti-patterns
for applications developed using object-relational mapping. In: Proceedings of the 36th International
Conference on Software Engineering, ser. ICSE 2014. Association for Computing Machinery, New York,
pp 1001–1012. [Online]. Available: https://doi.org/10.1145/2568225.2568259
Chen J, Shang W, Shihab E (2020) Perfjit: Test-level just-in-time prediction for performance regression
introducing commits. IEEE Trans Softw Eng:1–1
Correlation (pearson kendall, spearman) (2020) [Online]. Available: https://www.statisticssolutions.com/
correlation-pearson-kendall-spearman/
da Costa DA, McIntosh S, Shang W, Kulesza U, Coelho R, Hassan AE (2017) A framework for evaluating the
results of the szz approach for identifying bug-introducing changes. IEEE Trans Softw Eng 43(7):641–
657
Davies S, Roper M, Wood M (2014) Comparing text-based and dependence-based approaches for determining
the origins of bugs. J Softw Evol Process 26:01
Ding Z, Chen J, Shang W (2020) Towards the use of the readily available tests from the release pipeline as
performance tests. are we there yet? In: 42nd International Conference on Software Engineering, Seoul
Dmwr (2020) [Online]. Available: https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/
SMOTE
Fawcett T (2006) An introduction to roc analysis. Pattern Recogn Lett 27(8):861–874
Fukushima T, Kamei Y, McIntosh S, Yamashita K, Ubayashi N (2014) Studying just-in-time defect
prediction using cross-project models. Empir Softw Eng 21:05
Ghotra B, McIntosh S, Hassan AE (2015) Revisiting the impact of classification techniques on the performance
of defect prediction models. In: Proceedings of the 37th International Conference on Software
Engineering - Vol 1, ser. ICSE ’15. IEEE Press, pp 789–800
Guindon C Swt: The standard widget toolkit. [Online]. Available: https://www.eclipse.org/swt/
Guo PJ, Zimmermann T, Nagappan N, Murphy B (2010) Characterizing and Predicting Which Bugs Get
Fixed: An Empirical Study of Microsoft Windows. Association for Computing Machinery, New York,
pp 495–504. [Online]. Available: https://doi.org/10.1145/1806799.1806871
Gyimothy T, Ferenc R, Siket I (2005) Empirical validation of object-orientedmetrics on open source software
for fault prediction. IEEE Trans Softw Eng 31(10):897–910
Hamill M, Goseva-Popstojanova K (2014) Exploring the missing link: An empirical study of software fixes.
Softw Test Verif Reliab 24(8):684–705. [Online]. Available: https://doi.org/10.1002/stvr.1518
Hassan AE (2009) Predicting faults using the complexity of code changes. In: 2009 IEEE 31st International
Conference on Software Engineering, pp. 78–88
Jin G, Song L, Shi X, Scherpelz J, Lu S (2012) Understanding and detecting real-world performance bugs.
SIGPLAN Not. 47(6):77–88
Jpace jpace/diffj [Online]. Available: https://github.com/jpace/diffj
Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A large-scale empirical
study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773
Kim S, Zimmermann T, Pan K, Whitehead Jr. E. J. (2006) Automatic identification of bug-introducing
changes. In: 21st IEEE/ACM International Conference on Automated Software Engineering (ASE’06),
pp 81–90
Kondo M, German D, Mizuno O, Choi E (2020) The impact of context metrics on just-in-time defect
prediction. Empir Softw Eng 25:01
LaToza TD, Venolia G, DeLine R (2006) Maintaining mental models: A study of developer work habits. In:
Proceedings of the 28th International Conference on Software Engineering, ser. ICSE ’06. ACM, New
York, pp 492–501
Li H, Shang W, Zou Y, Hassan AE (2018) Towards just-in-time suggestions for log changes (journalfirst
abstract). In: 2018 IEEE 25th International Conference on Software Analysis, Evolution and
Reengineering (SANER), pp 467–467
McDonald JH (2014) Handbook of biological statistics. sparky house publishing Baltimore. MD 3:186–189
McHughM(2012) Interrater reliability: The kappa statistic, Biochemia medica : ˇc,asopis Hrvatskoga druˇstva
medicinskih biokemiˇcara / HDMB, vol 22, pp 276–82, 10
McIntosh S., Kamei Y (2018) Are fix-inducing changes a moving target? a longitudinal case study of justin-
time defect prediction. IEEE Trans Softw Eng 44(5):412–428
Molyneaux I (2009) The Art of Application Performance Testing: Help for Programmers and Quality
Assurance, 1st ed. O’Reilly Media, Inc.
Nayrolles M, Hamou-Lhadj A (2018) Clever: Combining code metrics with clone detection for just-in-time
fault prevention and resolution in large industrial projects, pp 03
Neto E, Costa D, Kulesza U (2018) The impact of refactoring changes on the szz algorithm: An empirical
study, pp 03
Nistor A, Jiang T, Tan L (2013) Discovering, reporting, and fixing performance bugs. in: 2013 10th working
conference on mining software repositories (MSR), pp 237–246
Ohira M, Kashiwa Y, Yamatani Y, Yoshiyuki H, Maeda Y, Limsettho N, Fujino K, Hata H, Ihara A, Matsumoto K (2015),  
A dataset of high impact bugs: Manually-classified issue reports. In: 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (pp. 518-521). IEEE.
Radu A, Nadi S (2019) A dataset of non-functional bugs. In: Proceedings of the 16th International Conference
on Mining Software Repositories, ser. MSR ’19. IEEE Press, Piscataway, pp 399–403
Rodrıguez-Perez G., Nagappan M, Robles G (2020) Watch out for extrinsic bugs! a case study of their
impact in just-in-time bug prediction models on the openstack project. IEEE Transactions on Software
Engineering
Rosen C, Grawi B, Shihab E (2015) Commit guru: Analytics and risk prediction of software commits. In:
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE
2015. ACM, New York, pp 966–969. [Online]. Available: https://doi.org/10.1145/2786805.2803183
Sawilowsky SS (2009) New effect size rules of thumb. J Modern Appl Stat Methods 8(2):26
Sliwerski J, Zimmermann T, Zeller A (2005) When do changes induce fixes? SIGSOFT Softw Eng Notes
30(4):1–5
Syer MD, Shang W, Jiang ZM, Hassan AE (2017) Continuous validation of performance test workloads.
Autom Softw Engg 2(1):189–231. [Online]. Available: https://doi.org/10.1007/s10515-016-0196-8
Tabassum S (2020) An investigation of cross-project learning in online just-in-time software defect
prediction, pp 06
Tantithamthavorn C, Hassan AE, Matsumoto K (2020) The impact of class rebalancing techniques on the
performance and interpretation of defect prediction models. IEEE Trans Softw Eng 46(11):1200–1219
Team J Eclipse java development tools (jdt). [Online]. Available: https://www.eclipse.org/jdt/
Tsakiltsidis S, Miranskyy A, Mazzawi E (2016) On automatic detection of performance bugs. In: 2016 IEEE
International Symposium on Software Reliability Engineering Workshops (ISSREW), pp 132–139
Williams C, Spacco J (2008) Szz revisited: Verifying when changes induce fixes. In: Proceedings of the 2008
Workshop on Defects in Large Software Systems, ser. DEFECTS ’08. ACM, New York, pp 32–36
Yang Y, Zhou Y, Liu J, Zhao Y, Lu H, Xu L, Xu B, Leung H (2016) Effort-aware just-in-time defect
prediction: Simple unsupervised models could be better than supervised models. In: Proceedings of
the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,
ser. FSE 2016. Association for Computing Machinery, New York, pp 157–168. [Online]. Available:
https://doi.org/10.1145/2950290.2950353
Zaman S, Adams B, Hassan AE (2011) Security versus performance bugs: A case study on firefox. In:
Proceedings of the 8thWorking Conference on Mining Software Repositories, ser. MSR ’11. ACM, New
York, pp 93–102
Zaman S, Adams B, Hassan AE (2012) A qualitative study on performance bugs, 2012 9th IEEE Working
Conference on Mining Software Repositories (MSR), pp 199–208


>><[N]>Just-in-Time Defect Prediction for Android Apps via Imbalanced Deep Learning Model
[1] Yuri Sousa Aurelio, Gustavo Matheus de Almeida, Cristiano Leite de Castro, and
Antonio Padua Braga. 2019. Learning from imbalanced data sets with weighted
cross-entropy function. Neural Processing Letters 50, 2 (2019), 1937–1949.
[2] Cagatay Catal and Banu Diri. 2009. Investigating the effect of dataset size, metrics
sets, and feature selection techniques on software fault prediction problem.
Information Sciences 179, 8 (2009), 1040–1058.
[3] Gemma Catolino. 2017. Just-in-time bug prediction in mobile applications: the
domain matters!. In Proceedings of the 4th IEEE/ACM International Conference on
Mobile Software Engineering and Systems (MOBILESoft). IEEE, 201–202.
[4] Gemma Catolino, Dario Di Nucci, and Filomena Ferrucci. 2019. Cross-project justin-
time bug prediction for mobile apps: an empirical assessment. In Proceedings
of the 6th IEEE/ACM International Conference on Mobile Software Engineering and
Systems (MOBILESoft). IEEE, 99–110.
[5] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The
MIT Press.
[6] Lan Guo, Yan Ma, Bojan Cukic, and Harshinder Singh. 2004. Robust prediction
of fault-proneness by random forests. In Proceedings of the 15th International
Symposium on Software Reliability Engineering (ISSRE). IEEE, 417–428.
[7] Guo Haixiang, Li Yijing, Jennifer Shang, Gu Mingyun, Huang Yuanyue, and
Gong Bing. 2017. Learning from class-imbalanced data: Review of methods and
applications. Expert Systems with Applications 73 (2017), 220–239.
[8] Hong Hu, Jiuyong Li, Hua Wang, Grant Daggard, and Mingren Shi. 2006. A maximally
diversified multiple decision tree algorithm for microarray data classification.
In Proceedings of the 2006 Workshop on Intelligent Systems for Bioinformatics,
Vol. 73. 35–38.
[9] Md Zahidul Islam and Helen Giggins. 2011. Knowledge Discovery through SysFor
- a Systematically Developed Forest of Multiple Decision Trees. In Proceedings of
the 9th Australasian Data Mining Conference, AusDM. 195–204.
[10] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2012. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering (TSE)
39, 6 (2012), 757–773.
[11] Arvinder Kaur, Kamaldeep Kaur, and Harguneet Kaur. 2016. Application of
machine learning on process metrics for defect prediction in mobile application.
In Information Systems Design and Intelligent Applications. Springer, 81–98.
[12] Foutse Khomh, Brian Chan, Ying Zou, Anand Sinha, and Dave Dietz. 2011. Predicting
post-release defects using pre-release field testing results. In Proceedings
of the 27th IEEE International Conference on Software Maintenance (ICSM). IEEE,
253–262.
[13] Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. 2017. Software defect prediction
via convolutional neural network. In Proceedings of the 17th IEEE International
Conference on Software Quality, Reliability and Security (QRS). IEEE, 318–328.
[14] Jinyan Li and Huiqing Liu. 2003. Ensembles of Cascading Trees. In Proceedings
of the 3rd IEEE International Conference on Data Mining (ICDM). 585–588.
[15] Kalai Magal R and Shomona Gracia Jacob. 2015. Improved Random Forest Algorithm
for Software Defect Prediction through Data Mining Techniques. International
Journal of Computer Applications 117, 23 (2015), 18–22.
[16] Ruchika Malhotra. 2016. An empirical framework for defect prediction using
machine learning techniques with Android software. Applied Soft Computing 49
(2016), 1034–1050.
[17] C Manjula and Lilly Florence. 2019. Deep neural network based hybrid approach
for software defect prediction using software metrics. Cluster Computing 22, 4
(2019), 9847–9863.
[18] Stuart McIlroy, Nasir Ali, and Ahmed E Hassan. 2016. Fresh apps: an empirical
study of frequently-updated mobile apps in the Google play store. Empirical
Software Engineering (EMSE) 21, 3 (2016), 1346–1370.
[19] Tim Menzies, Jeremy Greenwald, and Art Frank. 2006. Data mining static code
attributes to learn defect predictors. IEEE Transactions on Software Engineering
(TSE) 33, 1 (2006), 2–13.
[20] Marco Ortu, Giuseppe Destefanis, Stephen Swift, and Michele Marchesi. 2016.
Measuring high and low priority defects on traditional and mobile open source
software. In Proceedings of the 7th International Workshop on Emerging Trends in
Software Metrics. 1–7.
[21] Anh Viet Phan, Minh Le Nguyen, and Lam Thu Bui. 2017. Convolutional neural
networks over control flow graphs for software defect prediction. In Proceedings
of the 29th IEEE International Conference on Tools with Artificial Intelligence (ICTAI).
IEEE, 45–52.
[22] Michael Yoseph Ricky, Fredy Purnomo, and Budi Yulianto. 2016. Mobile application
software defect prediction. In 2016 IEEE Symposium on Service-Oriented
System Engineering. IEEE, 307–313.
[23] Riccardo Scandariato and James Walden. 2012. Predicting vulnerable classes
in an android application. In Proceedings of the 4th International Workshop on
Security Measurements and Metrics. 11–16.
[24] Giuseppe Scanniello, Carmine Gravino, Andrian Marcus, and Tim Menzies. 2013.
Class level fault prediction using software clustering. In Proceedings of the 28th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 640–645.
[25] Michael J. Siers and Md Zahidul Islam. 2015. Software defect prediction using
a cost sensitive decision forest and voting, and a potential solution to the class
imbalance problem. Information Systems 51 (2015), 62–71.
[26] Qinbao Song, Yuchen Guo, and Martin J. Shepperd. 2019. A Comprehensive
Investigation of the Role of Imbalanced Learning for Software Defect Prediction.
IEEE Transactions on Software Engineering (TSE) 45, 12 (2019), 1253–1269.
[27] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2016. An empirical comparison of model validation techniques for
defect prediction models. IEEE Transactions on Software Engineering (TSE) 43, 1
(2016), 1–18.
[28] Xin Xia, David Lo, Shane McIntosh, Emad Shihab, and Ahmed E Hassan. 2015.
Cross-project build co-change prediction. In Proceedings of the 22nd IEEE International
Conference on Software Analysis, Evolution, and Reengineering (SANER).
IEEE, 311–320.
[29] Zhou Xu, Li Li, Meng Yan, Jin Liu, Xiapu Luo, John Grundy, Yifeng Zhang, and
Xiaohong Zhang. 2020. A comprehensive comparative study of clustering-based
unsupervised defect prediction models. Journal of Systems and Software (JSS)
(2020), 110862.
[30] Zhou Xu, Shuai Li, Yutian Tang, Xiapu Luo, Tao Zhang, Jin Liu, and Jun Xu. 2018.
Cross version defect prediction with representative data via sparse subset selection.
In Proceedings of the 26th International Conference on Program Comprehension
(ICPC). 132–143.
[31] Zhou Xu, Shuai Li, Jun Xu, Jin Liu, Xiapu Luo, Yifeng Zhang, Tao Zhang, Jacky
Keung, and Yutian Tang. 2019. LDFR: Learning deep feature representation for
software defect prediction. Journal of Systems and Software (JSS) 158 (2019),
110402.
[32] Zhou Xu, Jin Liu, Xiapu Luo, Zijiang Yang, Yifeng Zhang, Peipei Yuan, Yutian
Tang, and Tao Zhang. 2019. Software defect prediction based on kernel PCA and
weighted extreme learning machine. Information and Software Technology (IST)
106 (2019), 182–200.
[33] Meng Yan, Xin Xia, Yuanrui Fan, Ahmed E Hassan, David Lo, and Shanping
Li. 2020. Just-In-Time Defect Identification and Localization: A Two-Phase
Framework. (2020).
[34] Meng Yan, Xin Xia, Yuanrui Fan, David Lo, Ahmed E Hassan, and Xindong Zhang.
2020. Effort-aware just-in-time defect identification in practice: a case study
at Alibaba. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering
(FSE). 1308–1319.
[35] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning
for just-in-time defect prediction. In Proceedings of the 15th IEEE International
Conference on Software Quality, Reliability and Security (QRS). IEEE, 17–26.
[36] Jingxiu Yao and Martin Shepperd. 2020. Assessing software defection prediction
performance: why using the Matthews correlation coefficient matters. In Proceedings
of the 24th Evaluation and Assessment in Software Engineering (EASE).
120–129.
[37] Tao Zhang, He Jiang, Xiapu Luo, and Alvin TS Chan. 2016. A literature review
of research in bug resolution: Tasks, challenges and future directions. Comput. J.
59, 5 (2016), 741–773.



>><N.>Just-in-time software defect prediction using deep temporal convolutional networks
1. Ackerman LBA, Lewski F (1989) Software inspections: an
effective verification process. IEEE Softw 6:31–36. https://doi.
org/10.1109/52.28121
2. Ahmad J, Farman H, Jan Z (2019) Deep learning methods and
applications, pp 31–42. Springer Singapore. https://doi.org/10.
1007/978-981-13-3459-7_3
3. Aniche M (2015) Java code metrics calculator (CK). Available in
https://github.com/mauricioaniche/ck/
4. Ardimento P, Aversano L, Bernardi ML, Cimitile M (2020)
Temporal convolutional networks for just-in-time software defect
prediction. In M. van Sinderen, H. Fill, L.A. Maciaszek (eds.)
Proceedings of the 15th International Conference on Software
Technologies, ICSOFT 2020, Lieusaint, Paris, France, July 7-9,
2020, pp 384–393. ScitePress. https://doi.org/10.5220/
0009890003840393
5. Ardimento P, Aversano L, Bernardi ML, Cimitile M, Iammarino
M (2021) Temporal convolutional networks for just-in-time
design smells prediction using fine-grained software metrics.
Neurocomput 463:454–471. 10.1016/j.neucom.2021.08.010.
https://www.sciencedirect.com/science/article/pii/
S0925231221011942
6. Ardimento P, Bernardi ML, Cimitile M (2018) A multi-source
machine learning approach to predict defect prone components.
In Proceedings of the 13th International Conference on Software
Technologies, ICSOFT 2018, Porto, Portugal, July 26-28, 2018,
pp 306–313. https://doi.org/10.5220/0006857803060313
7. Aversano L, Bernardi ML, Cimitile M, Iammarino M, Romanyuk
K (2020) Investigating on the relationships between design smells
removals and refactorings. In M. van Sinderen, H. Fill, L.A.
Maciaszek (eds.) Proceedings of the 15th International Conference
on Software Technologies, ICSOFT 2020, Lieusaint, Paris,
France, July 7-9, 2020, pp 212–219. ScitePress. https://doi.org/
10.5220/0009887102120219
8. Bai S, Kolter JZ, Koltun V (2018) An empirical evaluation of
generic convolutional and recurrent networks for sequence
modeling. CoRR arXiv:1803.01271
9. Barnett JG, Gathuru CK, Soldano LS, McIntosh S (2016) The
relationship between commit message detail and defect proneness
in java projects on github. In Proceedings of the 13th International
Conference on Mining Software Repositories, MSR 2016,
Austin, TX, USA, May 14-22, 2016, pp 496–499. https://doi.org/
10.1145/2901739.2903496
10. Basili VR, Briand LC, Melo WL (1996) A validation of objectoriented
design metrics as quality indicators. IEEE Trans Software
Eng 22(10):751–761. https://doi.org/10.1109/32.544352
11. Bengio Y (2000) Gradient-based optimization of hyperparameters.
Neural Comput 12(8):1889–1900. https://doi.org/10.1162/
089976600300015187
12. Bengio Y, Courville A, Vincent P (2014) Representation learning:
a review and new perspectives
13. Bergstra J, Bardenet R, Bengio Y, Ke´gl B (2011) Algorithms for
hyper-parameter optimization. In Proceedings of the 24th International
Conference on Neural Information Processing Systems,
NIPS’11, p 2546–2554. Curran Associates Inc., Red Hook, NY,
USA
14. Bergstra J, Yamins D, Cox DD (2013) Making a science of model
search: Hyperparameter optimization in hundreds of dimensions
for vision architectures. In Proceedings of the 30th International
Conference on International Conference on Machine Learning -
Volume 28, ICML’13, p I–115–I–123. JMLR.org
15. Bernardi M, Cimitile M, Martinelli F, Mercaldo F (2018) Driver
and path detection through time-series classification. J Adv
Transp 2018. https://doi.org/10.1155/2018/1758731
16. Bernardi ML, Cimitile M, Martinelli F, Mercaldo F (2019)
Keystroke analysis for user identification using deep neural networks.
In 2019 International Joint Conference on Neural Networks
(IJCNN), pp 1–8. https://doi.org/10.1109/IJCNN.2019.
8852068
17. Bird C, Nagappan N, Murphy B, Gall H, Devanbu PT (2011)
Don’t touch my code!: examining the effects of ownership on
software quality. In: SIGSOFT/FSE’11 19th ACM SIGSOFT
Symposium on the Foundations of Software Engineering (FSE-
19) and ESEC’11: 13rd European Software Engineering Conference
(ESEC-13), Szeged, Hungary, September 5-9, 2011,
pp 4–14. ACM
18. Boucher A, Badri M (2016) Using software metrics thresholds to
predict fault-prone classes in object-oriented software. In 2016
4th Intl Conf on Applied Computing and Information Technology/
3rd Intl Conf on Computational Science/Intelligence and
Applied Informatics/1st Intl Conf on Big Data, Cloud Computing,
Data Science Engineering (ACIT-CSII-BCD), pp 169–176.
https://doi.org/10.1109/ACIT-CSII-BCD.2016.042
19. Brito e Abreu F, Melo W (1996) Evaluating the impact of objectoriented
design on software quality. In: Proceedings of the 3rd
International Software Metrics Symposium, pp 90–99. https://doi.
org/10.1109/METRIC.1996.492446
20. Cabral GG, Minku LL, Shihab E, Mujahid S (2019) Class
imbalance evolution and verification latency in just-in-time
software defect prediction. In: Proceedings of the 41st International
Conference on Software Engineering, ICSE 2019, Montreal,
QC, Canada, May 25-31, 2019, pp 666–676. https://doi.org/
10.1109/ICSE.2019.00076
21. Chawla NV (2009) Data mining for imbalanced datasets: an
overview. In Data mining and knowledge discovery handbook,
pp 875–886. Springer
22. Chen X, Zhao Y, Wang Q, Yuan Z (2018) Multi: Multi-objective
effort-aware just-in-time software defect prediction. Inf Softw
Technol 93:1–13
23. Chidamber SR, Kemerer CF (1994) A metrics suite for object
oriented design. IEEE Trans Software Eng 20(6):476–493.
https://doi.org/10.1109/32.295895
24. Chidamber SR, Kemerer CF (1994) A metrics suite for object
oriented design. IEEE Trans Softw Eng 20(6):476–493. https://
doi.org/10.1109/32.295895
25. Dabic O, Aghajani E, Bavota G (2021) Sampling projects in
github for MSR studies. In: Proceedings of the 18th International
Conference on Mining Software Repositories, MSR’21, p. To
appear. arXiv:2103.04682
26. Dam HK, Tran T, Pham TTM, Ng SW, Grundy J, Ghose A
(2018) Automatic feature learning for predicting vulnerable
software components. IEEE Trans Softw Eng
27. Ding Z, Xing L (2020) Improved software defect prediction using
pruned histogram-based isolation forest. Reliab Eng Syst Saf
204:107170
28. DAmbros M, Lanza M, Robbes R (2012) Evaluating defect
prediction approaches: a benchmark and an extensive comparison.
Empir Softw Eng 17(4–5):531–577
29. Fischer M, Pinzger M, Gall H (2003) Populating a release history
database from version control and bug tracking systems. In 19th
International Conference on Software Maintenance (ICSM 2003),
The Architecture of Existing Systems, 22-26 September 2003,
Amsterdam, The Netherlands, p 23. IEEE Computer Society
30. Graves TL, Karr AF, Marron JS, Siy H (2000) Predicting fault
incidence using software change history. IEEE Trans Softw Eng
26(7):653–661
31. Greiler, M., Herzig, K., Czerwonka, J. (2015) Code ownership
and software quality: a replication study. In 2015 IEEE/ACM
12th Working Conference on Mining Software Repositories,
pp 2–12. 10.1109/MSR.2015.8
32. Hall MA (1999) Correlation-based feature selection for machine
learning. Ph.D. thesis, Department of Computer Science,
University of Waikato, The address of the publisher
33. Hassan AE (2009) Predicting faults using the complexity of code
changes. In 31st International Conference on Software Engineering,
ICSE 2009, May 16-24, 2009, Vancouver, Canada,
Proceedings, pp 78–88. https://doi.org/10.1109/ICSE.2009.
5070510
34. Hilton R J: Java Source Metrics (2009 (accessed January 16,
2020)). Available in https://github.com/rodhilton/jasome
35. Hoang T, Dam HK, Kamei Y, Lo D, Ubayashi N (2019) Deepjit:
an end-to-end deep learning framework for just-in-time defect
prediction. In 2019 IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR), pp 34–45. IEEE
36. Hochreiter S, Schmidhuber J (1997) Long short-term memory.
Neural Comput 9(8):1735–1780. https://doi.org/10.1162/neco.
1997.9.8.1735
37. Ioffe S, Szegedy C (2015) Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15,
pp 448–456. JMLR.org. http://dl.acm.org/citation.cfm?id=
3045118.3045167
38. Jahanshahi H, Jothimani D, Bas¸ar A, Cevik M (2019) Does
chronology matter in jit defect prediction? a partial replication
study. In Proceedings of the Fifteenth International Conference
on Predictive Models and Data Analytics in Software Engineering,
pp 90–99
39. Jin C (2021) Cross-project software defect prediction based on
domain adaptation learning and optimization. Expert Syst Appl
171(114637):1. 10.1016/j.eswa.2021.114637. https://www.scien
cedirect.com/science/article/pii/S0957417421000786
40. Kamei Y, Fukushima T, McIntosh S, Yamashita K, Ubayashi N,
Hassan AE (2016) Studying just-in-time defect prediction using
cross-project models. Empir Softw Eng 21(5):2072–2106. https://
doi.org/10.1007/s10664-015-9400-x
41. Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A,
Ubayashi N (2013) A large-scale empirical study of just-in-time
quality assurance. IEEE Trans Softw Eng 39(6):757–773. https://
doi.org/10.1109/TSE.2012.70
42. Khatri Y, Singh SK (2021) Cross project defect prediction: a
comprehensive survey with its swot analysis. Innovations in
Systems and Software Engineering pp 1–19
43. Kingma DP, Ba J (2014) Adam: A method for stochastic
optimization
44. Manjula C, Florence L (2019) Deep neural network based hybrid
approach for software defect prediction using software metrics.
Clust Comput 22(4):9847–9863. https://doi.org/10.1007/s10586-
018-1696-z
45. Mannor S, Peleg D, Rubinstein R (2005) The cross entropy
method for classification. In Proceedings of the 22Nd International
Conference on Machine Learning, ICML ’05, pp 561–568.
ACM, New York, NY, USA
46. Misra D M: A self regularized non-monotonic neural activation
function (arXiv pre-print, 2019)
47. Mitchell TM (1997) Machine learning, 1st edn. McGraw-Hill Inc,
New York, NY, USA
48. Mockus A, Weiss DM (2000) Predicting risk of software changes.
Bell Labs Techn J 5(2):169–180
49. Moser R, Pedrycz W, Succi G (2008) Analysis of the reliability of
a subset of change metrics for defect prediction. In Proceedings
of the Second ACM-IEEE international symposium on Empirical
software engineering and measurement, pp 309–311
50. Moser R, Pedrycz W, Succi G (2008) A comparative analysis of
the efficiency of change metrics and static code attributes for
defect prediction. In 30th International Conference on Software
Engineering (ICSE 2008), Leipzig, Germany, May 10-18, 2008,
pp 181–190. https://doi.org/10.1145/1368088.1368114
51. Myers GJ, Sandler C (2004) The art of software testing. Wiley,
Hoboken, NJ, USA
52. Pascarella L, Palomba F, Bacchelli A (2019) Fine-grained just-intime
defect prediction. J Syst Softw 150:22–36. https://doi.org/
10.1016/j.jss.2018.12.001
53. Peters F, Menzies T, Marcus A (2013) Better cross company
defect prediction. In 2013 10th Working Conference on Mining
Software Repositories (MSR), pp 409–418. IEEE
54. Phan AV, Nguyen ML, Bui LT (2018) Convolutional neural
networks over control flow graphs for software defect prediction.
CoRR arXiv:1802.04986
55. Pickerill P, Jungen HJ, Ochodek M, Mackowiak M, Staron M
(2020) PHANTOM: curating github for engineered software
projects using time-series clustering. Empir Softw Eng
25(4):2897–2929. https://doi.org/10.1007/s10664-020-09825-8
56. Porto FR, Simao A (2016) Feature subset selection and instance
filtering for cross-project defect prediction-classification and
ranking. CLEI Electron J 19(3):4
57. Rahman F, Devanbu P (2013) How, and why, process metrics are
better. In 2013 35th International Conference on Software
Engineering (ICSE), pp 432–441. IEEE
58. Ramachandran P, Zoph B, Le QV (2017) Searching for activation
functions. CoRR arXiv:1710.05941
59. Schaul T, Antonoglou I, Silver D (2013) Unit tests for stochastic
optimization
60. Spinellis D (2005) Tool writing: a forgotten art? (software tools).
IEEE Softw 22(4):9–11. https://doi.org/10.1109/MS.2005.111
61. Staudemeyer RC, Rothstein Morris E (2019) Understanding
LSTM – a tutorial into Long Short-Term Memory Recurrent
Neural Networks. arXiv e-prints arXiv:1909.09586
62. Subramanyam R, Krishnan M (2003) Empirical analysis of ck
metrics for object-oriented design complexity: Implications for
software defects. IEEE Trans Softw Eng 29:297–310. https://doi.
org/10.1109/TSE.2003.1191795
63. Sutskever I, Martens J, Dahl G, Hinton G (2013) On the importance
of initialization and momentum in deep learning. In Proceedings
of the 30th International Conference on International
Conference on Machine Learning - Volume 28, ICML’13, pp III–
1139–III–1147. JMLR.org
64. Vani S, Rao TVM (2019) An experimental approach towards the
performance assessment of various optimizers on convolutional
neural network. In 2019 3rd International Conference on Trends
in Electronics and Informatics (ICOEI), pp 331–336. https://doi.
org/10.1109/ICOEI.2019.8862686
65. Varma S, Simon R (2006) Bias in error estimation when using
cross-validation for model selection. BMC Bioinf 7:91. https://
doi.org/10.1186/1471-2105-7-91
66. Wang T, Zhang Z, Jing X, Zhang L (2016) Multiple kernel
ensemble learning for software defect prediction. Autom Softw
Eng 23(4):569–590. https://doi.org/10.1007/s10515-015-0179-1
67. Wang Y, Liu J, Misˇic´ J, Misˇic´ VB, Lv S, Chang X (2019)
Assessing optimizer impact on DNN model sensitivity to adversarial
examples. IEEE Access 7:152766–152776. https://doi.org/
10.1109/ACCESS.2019.2948658
68. Xu Z, Li S, Xu J, Liu J, Luo X, Zhang Y, Zhang T, Keung J, Tang
Y (2019) LDFR: learning deep feature representation for software
defect prediction. J Syst Softw. https://doi.org/10.1016/j.jss.2019.
110402
69. Yang X, Lo D, Xia X, Sun J (2017) Tlel: A two-layer ensemble
learning approach for just-in-time defect prediction. Inf Softw
Technol 87:206–220
70. Yang X, Lo D, Xia X, Zhang Y, Sun J (2015) Deep learning for
just-in-time defect prediction. In 2015 IEEE International Conference
on Software Quality, Reliability and Security, QRS 2015,
Vancouver, BC, Canada, August 3-5, 2015, pp 17–26. https://doi.
org/10.1109/QRS.2015.14
71. Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E (2016)
Hierarchical attention networks for document classification. In
Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, pp 1480–1489. Association for
Computational Linguistics, San Diego, California. https://doi.org/
10.18653/v1/N16-1174. https://www.aclweb.org/anthology/N16-
1174
72. Young S, Abdou T, Bener A (2018) A replication study: just-intime
defect prediction with ensemble learning. In Proceedings of
the 6th International Workshop on Realizing Artificial Intelligence
Synergies in Software Engineering, pp 42–47
73. Zimmermann T, Nagappan N, Gall H, Giger E, Murphy B (2009)
Cross-project defect prediction: a large scale experiment on data
vs. domain vs. process. In Proceedings of the 7th joint meeting of
the European software engineering conference and the ACM
SIGSOFT symposium on The foundations of software engineering,
pp 91–100


>><[N]>Predicting Defects with Latent and Semantic Features from Commit Logs in an Industrial Setting
[1] M. Dsouza, “5 ways artificial intelligence is upgrading software
engineering,” https://hub.packtpub.com/5-ways-artificial-intelligence-isupgrading-
software-engineering/, 2018, [Online; accessed 9-July-2019].
[2] T. Menzies and T. Zimmermann, “Software analytics: Whats next?”
IEEE Software, vol. 35, no. 5, pp. 64–70, 2018.
[3] M. Felleisen, R. B. Findler, M. Flatt, S. Krishnamurthi, E. Barzilay,
J. McCarthy, and S. Tobin-Hochstadt, “The racket manifesto,” in 1st
Summit on Advances in Programming Languages (SNAPL), vol. 32,
2015, pp. 113–128.
[4] A. Scott, J. Bader, and S. Chandra, “Getafix: Learning to fix bugs
automatically,” arXiv preprint arXiv:1902.06111, 2019.
[5] S. Ali, L. C. Briand, H. Hemmati, and R. K. Panesar-Walawege,
“A systematic review of the application and empirical investigation
of search-based test case generation,” IEEE Transactions on Software
Engineering, vol. 36, no. 6, pp. 742–762, 2009.
[6] G. Catolino, F. Palomba, A. Zaidman, and F. Ferrucci, “Not all bugs
are the same: Understanding, characterizing, and classifying bug types,”
Journal of Systems and Software, vol. 152, pp. 165–181, 2019.
[7] R. Malhotra, “A systematic review of machine learning techniques for
software fault prediction,” Applied Soft Computing, vol. 27, pp. 504–518,
2015.
[8] D. Radjenovi´c, M. Heriˇcko, R. Torkar, and A. ˇ Zivkoviˇc, “Software
fault prediction metrics: A systematic literature review,” Information and
Software Technology, vol. 55, no. 8, pp. 1397–1418, 2013.
[9] B. Caglayan, A. Bener, and S. Koch, “Merits of using repository metrics
in defect prediction for open source projects,” in ICSE Workshop on
Emerging Trends in Free/Libre/Open Source Software Research and
Development, 2009, pp. 31–36.
[10] C. Bird, N. Nagappan, H. Gall, B. Murphy, and P. Devanbu, “Putting it
all together: Using socio-technical networks to predict failures,” in 20th
International Symposium on Software Reliability Engineering, 2009, pp.
109–119.
[11] A. T. Mısırlı, A. B. Bener, and B. Turhan, “An industrial case study
of classifier ensembles for locating software defects,” Software Quality
Journal, vol. 19, no. 3, pp. 515–536, 2011.
[12] Y. Kamei and E. Shihab, “Defect prediction: Accomplishments and
future challenges,” in IEEE 23rd Int. Conf. on software analysis,
evolution, and reengineering (SANER), vol. 5, 2016, pp. 33–45.
[13] T. J. McCabe, “A complexity measure,” IEEE Transactions on software
Engineering, no. 4, pp. 308–320, 1976.
[14] A. E. Hassan, “Predicting faults using the complexity of code changes,”
in Proc. of 31st Int. Conf. on Software Engineering, 2009, pp. 78–88.
[15] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Programmer-based fault
prediction,” in Proc. of 6th Int. Conf. on Predictive Models in Software
Engineering, 2010, p. 19.
[16] T. T. Nguyen, T. N. Nguyen, and T. M. Phuong, “Topic-based defect
prediction (nier track),” in Proc. of 33rd Int. Conf. on Software Engineering,
2011, pp. 932–935.
[17] A. T. Misirli, E. Shihab, and Y. Kamei, “Studying high impact fixinducing
changes,” Empirical Software Engineering, vol. 21, no. 2, pp.
605–641, 2016.
[18] D. D. Lee and H. S. Seung, “Learning the parts of objects by nonnegative
matrix factorization,” Nature, vol. 401, no. 6755, p. 788, 1999.
[19] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
Journal of machine Learning research, vol. 3, no. Jan, pp. 993–1022,
2003.
[20] A. G´omez-R´ıos, J. Luengo, and F. Herrera, “A study on the noise label
influence in boosting algorithms: Adaboost, gbm and xgboost,” in Int.
Conf. on Hybrid Artificial Intelligence Systems, 2017, pp. 268–280.
[21] E. Sahal and A. Tosun, “Identifying bug-inducing changes for code
additions,” in Proc. of 12th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement, 2018, p. 57.
[22] J.-S. Zhang, C.-P. Wang, and Y.-Q. Yang, “Learning latent features
by nonnegative matrix factorization combining similarity judgments,”
Neurocomputing, vol. 155, pp. 43–52, 2015.
[23] J. Shen and P. Li, “Learning structured low-rank representation via
matrix factorization,” in Artificial Intelligence and Statistics, 2016, pp.
500–509.
[24] J. G. Barnett, C. K. Gathuru, L. S. Soldano, and S. McIntosh, “The
relationship between commit message detail and defect proneness in
java projects on github,” in Proc. of 13th Int. Conf. on Mining Software
Repositories, 2016, pp. 496–499.
[25] J. C. Campbell, A. Hindle, and E. Stroulia, “Latent dirichlet allocation:
extracting topics from software engineering data,” in The art and science
of analyzing software data. Elsevier, 2015, pp. 139–159.
[26] T. Menzies, J. Greenwald, and A. Frank, “Data mining static code
attributes to learn defect predictors,” IEEE Transactions on Software
Engineering, vol. 33, no. 1, pp. 2–13, 2006.
[27] J. R. Quinlan, “Induction of decision trees,” Machine learning, vol. 1,
no. 1, pp. 81–106, 1986.
[28] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
in Proc. of 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and
Data Mining, 2016, pp. 785–794.
[29] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
IEEE Transactions on Software Engineering, vol. 38, no. 6, pp.
1276–1304, 2011.
[30] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect prediction from static code features: current results, limitations,
new approaches,” Automated Software Engineering, vol. 17, no. 4, pp.
375–407, 2010.
[31] V. Sandulescu and M. Chiru, “Predicting the future relevance of research
institutions-the winning solution of kdd cup 2016,” arXiv preprint
arXiv:1609.02728, 2016.
[32] H. Altinger, S. Herbold, F. Schneemann, J. Grabowski, and F. Wotawa,
“Performance tuning for automotive software fault prediction,” in 24th
Int. Conf. on Software Analysis, Evolution and Reengineering (SANER),
2017, pp. 526–530.
[33] S. E. Sahin and A. Tosun, “A conceptual replication on predicting
the severity of software vulnerabilities,” in Proc. of Evaluation and
Assessment on Software Engineering, 2019, pp. 244–250.
[34] E. Alpaydin, Introduction to machine learning. MIT press, 2009.
[35] B. W. Matthews, “Comparison of the predicted and observed secondary
structure of t4 phage lysozyme,” Biochimica et Biophysica Acta (BBA)-
Protein Structure, vol. 405, no. 2, pp. 442–451, 1975.
[36] J. A. Hanley and B. J. McNeil, “The meaning and use of the area under a
receiver operating characteristic (roc) curve.” Radiology, vol. 143, no. 1,
pp. 29–36, 1982.
[37] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“An empirical comparison of model validation techniques for defect
prediction models,” IEEE Transactions on Software Engineering, vol. 43,
no. 1, pp. 1–18, 2017.
[38] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Software Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[39] B. Eken, “Assessing personalized software defect predictors,” in Proc.
of 40th Int. Conf. on Software Engineering, 2018, pp. 488–491.
[40] T.-H. Chen, S. W. Thomas, M. Nagappan, and A. E. Hassan, “Explaining
software defects using topic models,” in 9th IEEE Working Conference
on Mining Software Repositories (MSR), 2012, pp. 189–198.
[41] M. S. Khan, “A topic modeling approach for code clone detection,”
Master’s thesis, Univ. of North Florida, 2019.
[42] H. U. Asuncion, A. U. Asuncion, and R. N. Taylor, “Software traceability
with topic modeling,” in ACM/IEEE 32nd Int. Conf. on Software
Engineering, vol. 1, 2010, pp. 95–104.
[43] D. Guillamet and J. Vitri`a, “Non-negative matrix factorization for face
recognition,” in Catalonian Conference on Artificial Intelligence, 2002,
pp. 336–344.
[44] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for
recommender systems,” IEEE Computer, no. 8, pp. 30–37, 2009.
[45] O¨ . Bozcan and A. B. Bener, “Handling missing attributes using matrix
factorization,” in 2nd International Workshop on Realizing Artificial
Intelligence Synergies in Software Engineering (RAISE), 2013, pp. 49–
55.
[46] R. Chang, X. Mu, and L. Zhang, “Software defect prediction using nonnegative
matrix factorization,” Journal of Software, vol. 6, no. 11, pp.
2114–2120, 2011.



>><[N]>Predicting Risk of Pre-Release Code Changes with CheckinMentor
[1] S. Kim, T. Zimmermann, K. Pan, J. Whitehead, “Automatic Identification of Bug-Introducing Changes”, Proc. ASE’06, pp. 81-90, 2006
[2] N. Nagappan, B. Murphy, V. Basili, “The influence of organizational structure on software quality”, Proc. ICSE’08, pp. 521-530, 2010
[3] A. Tarvo, “Using Statistical Models to Predict Software Regressions”, Proc. ISSRE’08, pp. 259-264, 2008
[4] A. Tarvo, T. Zimmermann, J. Czerwonka, “An Integration Resolution Algorithm for Mining Multiple Branches in Version Control Systems”, Proc. ICSM’11, pp. 402-411, 2011
[5] T. Fawcett, “An Introduction to ROC analysis”, Pattern Recognition Letters, 26, 2006, pp. 861–874
[6] Hand D.J., Mannila H., Smyth P., “Principles of Data Mining”, The MIT Press, 2001
[7] Larose D. T., “Data Mining Methods and Models”, Wiley-Interscience, Hoboken, NJ , 2006
[8] M. Sumner, E. Frank, M. Hall, “Speeding up Logistic Model Tree Induction”, Proc. ECML-PKDD’05, pp. 675-683, 2005.
[9] A. Mockus, D. Weiss, “Predicting risk of software changes”, Bell Labs Tech Journal, Vol. 5 no. 2, 2000, pp. 169-180
[10] N. Nagappan, T. Ball, “Use of Relative Code Churn Measures to Predict System Defect Density”, Proc. ICSE’05, pp. 284-292, 2005
[11] T. Zimmermann, N. Nagappan, “Predicting Defects Using Network Analysis on Dependency Graphs”, Proc. ICSE’08, pp. 531-540, 2008
[12] C. Bird, N. Nagappan, P. Devanbu, H. Gall, B. Murphy, “Putting It All Together: Using Socio-technical Networks to Predict Failures”, Proc. ISSRE’09, pp. 109-119, 2009
[13] S. Kim, E. J. Whitehead, Y. Zhang, “Classifying Software Changes: Clean or Buggy”, IEEE Transactions on Software Engineering, Vol. 34 no. 2, 2008, pp. 181-196
[14] N. Nagappan, T. Ball, A. Zeller, “Mining Metrics to Predict Component Failures”, Proc. ICSE’06, pp. 452-461, 2006
[15] E. Arisholm, L. C. Briand , “Predicting Fault-prone Components in a Java Legacy System”, Proc. ISESE’06, pp.8-17,2006
[16] J. Munson, T. Khoshgoftaar, “The Detection of Fault-Prone Programs”, IEEE Transactions on Software Engineering, vol. 18 no. 5, pp. 423-433, 1992
[17] T. Menzies, J. Greenwald, A. Frank, "Data Mining Static Code Attributes to Learn Defect Predictors", IEEE Transactions on Software Engineering, vol. 32 no 11, 2007
[18] V. Basili, L. Briand, W. Melo, “A Validation of Object-Oriented Design Metrics as Quality Indicators”, IEEE Transactions on Software Engineering, vol. 32 no 11, pp. 751-761, 2007
[19] A. Hassan, “Predicting Faults Using Complexity of Code Changes”, Proc. ICSE’09, pp. 78-88, 2009
[20] E. Weyuker, T. Ostrand, R. Bell, “Comparing the effectiveness of several modeling methods for fault prediction”, Empirical Software Engineering vol. 15 no. 3, pp. 277-295, 2010
[21] S. Kim, T. Zimmermann, E. Whitehead, A. Zeller, “Predicting Faults from Cacned History”, Proc. ICSE’07, pp. 489-498, 2007
[22] A. Mockus, R. Fielding, J. Herbsleb, “A Case Study of Open Source Software Development: the Apache Server”, Proc. ICSE’00, pp. 263-272, 2000
[23] T. Mende, R. Koschke, “Effort-Aware Defect Prediction Models”, Proc. CSMR’10, pp. 107-116, 2010
[24] E. Shihab, A. Hassan, B. Adams, Z.M. Jiang, “An Industrial Study on the Risk of Software Changes”, Proc. FSE’2012, pp. 1-11, 2012



>><[N]>SZZ Unleashed: An Open Implementation of the SZZ Algorithm - Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the Jenkins Project
[1] K. Berg and O. Svensson. 2018. SZZ Unleashed: Bug Prediction on the Jenkins
Core Repository (Open Source Implementations of Bug Prediction Tools on
Commit Level). https://doi.org/student-papers/search/publication/8971266 Msc
Thesis, Lund University, Sweden.
[2] G. Canfora, L. Cerulo, and M. Di Penta. 2007. Identifying Changed Source Code
Lines from Version Repositories. In Proc. of the 4th International Workshop on
Mining Software Repositories. https://doi.org/10.1109/MSR.2007.14
[3] Y. Cavalcanti, P. Silveira Neto, I. Machado, T. Vale, E. Almeida, and S. Meira.
2014. Challenges and Opportunities for Software Change Request Repositories:
A Systematic Mapping Study. Journal of Software: Evolution and Process 26, 7
(2014), 620–653. https://doi.org/10.1002/smr.1639
[4] J. Correia. 2017. old-szz. https://github.com/intelligentagents/old-szz
[5] J. Czerwonka, R. Das, N. Nagappan, A. Tarvo, and A. Teterev. 2011. CRANE:
Failure Prediction, Change Analysis and Test Prioritization in Practice âĂŞ Experiences
from Windows. In Proc. of the 4th Conference on Software Testing,
Verification and Validation. 357–366. https://doi.org/10.1109/ICST.2011.24
[6] M. D’Ambros, M. Lanza, and R. Robbes. 2009. On the Relationship Between
Change Coupling and Software Defects. In 2009 16th Working Conference on
Reverse Engineering. 135–144. https://doi.org/10.1109/WCRE.2009.19
[7] M. de Freitas Farias, R. Novais, M. Junior, L. da Silva Carvalho, M. Mendonca, and
R. Spinola. 2016. A Systematic Mapping Study on Mining Software Repositories.
In Proc. of the 31st Annual ACM Symposium on Applied Computing. 1472–1479.
https://doi.org/10.1145/2851613.2851786
[8] E. Engström, P. Runeson, and M. Skoglund. 2010. A Systematic Review on
Regression Test Selection Techniques. Information and Software Technology 52, 1
(2010), 14–30. https://doi.org/10.1016/j.infsof.2009.07.001
[9] N. Fenton and M. Neil. 1999. A Critique of Software Defect Prediction Models.
Transactions on Software Engineering 25, 5 (1999), 675–689. https://doi.org/10.
1109/32.815326
[10] M. Godfrey and L. Zou. 2005. Using Origin Analysis to Detect Merging and
Splitting of Source Code Entities. Transactions on Software Engineering 31, 2
(2005), 166–181. https://doi.org/10.1109/TSE.2005.28
[11] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell. 2012. A Systematic
Literature Review on Fault Prediction Performance in Software Engineering.
Transactions on Software Engineering 38, 6 (2012), 1276–1304. https://doi.org/10.
1109/TSE.2011.103
[12] L. Jonsson, M. Borg, D. Broman, K. Sandahl, S. Eldh, and P. Runeson. 2016. Automated
Bug Assignment: Ensemble-based Machine Learning in Large Scale
Industrial Contexts. Empirical Software Engineering 21, 4 (2016), 1533–1578.
[13] Y. Kamei, E. Shihab, B. Adams, A. Hassan, A. Mockus, A. Sinha, and N. Ubayashi.
2013. A Large-scale Empirical Study of Just-in-Time Quality Assurance. In
Transactions on Software Engineering, Vol. 39. 757–773. https://doi.org/10.1109/
TSE.2012.70
[14] Sunghun Kim, Thomas Zimmermann, Kai Pan, and Whitehead James. 2006. Automatic
identification of bug-introducing changes. In Proc. of the 21st International
Conference on Automated Software Engineering. 81–90.
[15] G. Lemaitre, F. Nogueira, and C. Aridas. 2017. Imbalanced-learn: A Python
Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. Journal
of Machine Learning Research 18, 17 (2017), 1–5.
[16] R. Moser,W. Pedrycz, and G. Succi. 2008. A Comparative Analysis of the Efficiency
of Change Metrics and Static Code Attributes for Defect Prediction. In Proc.of
the 30th International Conference on Software Engineering. 181–190. https://doi.
org/10.1145/1368088.1368114
[17] N. Nagappan and T. Ball. 2005. Use of Relative Code Churn Measures to Predict
System Defect Density. In Proc. of the 27th International Conference on Software
Engineering. 284–292.
[18] F. Rahman, S. Khatri, E. Barr, and P. Devanbu. 2014. Comparing Static Bug Finders
and Statistical Prediction. In Proc. of the 36th International Conference on Software
Engineering. 424–434. https://doi.org/10.1145/2568225.2568269
[19] G. Rodriguez-Perez, G. Robles, and J. Gonzalez-Barahona. 2018. Reproducibility
and Credibility in Empirical Software Engineering: A Case Study based on a
Systematic Literature Review of the use of the SZZ algorithm. Information and
Software Technology (2018).
[20] C. Rosen, B. Grawi, and E. Shihab. 2015. Commit Guru: Analytics and Risk Prediction
of Software Commits. In Proc. of the 2015 10th Joint Meeting on Foundations
of Software Engineering. 966–969. https://doi.org/10.1145/2786805.2803183
[21] J. Sliwerski, T. Zimmermann, and A. Zeller. 2005. When Do Changes Induce
Fixes?. In Proc. of the 2005 International Workshop on Mining Software Repositories,
Vol. 30. 1–5.
[22] M. Sohn, S. Pearce, A. Loskutov, C. Aniszczyk, C. Halstrick, C. Ranger, D. Borowitz,
D. Pursehouse, G. Wagenknecht, J. Nieder, K. Sawicki, M. Kinzler, R. Rosenberg,
R. Stocker, S. Zivkov, S. Lay, T. Parker, and T. Wolf. 2017. Eclipse JGit. https:
//github.com/eclipse/jgit
[23] O. Svensson and K. Berg. 2018. SZZ Unleashed. https://github.com/wogscpar/
SZZUnleashed
[24] M. Tan, L. Tan, S. Dara, and C. Mayeux. 2015. Online Defect Prediction for Imbalanced
Data. In Proc. of the 37th International Conference on Software Engineering.
99–108.
[25] A. Tornhill. 2017. code-maat. https://github.com/adamtornhill/code-maat
[26] C. Williams and J. Spacco. 2008. SZZ Revisited: Verifying When Changes Induce
Fixes. In Proc. of the 2008 Workshop on Defects in Large Software Systems. 32–36.
[27] X. Yang, D. Lo, X. Xia, and J. Sun. 2017. TLEL: A Two-layer Ensemble Learning Approach
for Just-in-Time Defect Prediction. Information and Software Technology
87 (2017), 206–220.


>><[N]>The Impact of Data Merging on the Interpretation of Cross-Project Just-In-Time Defect Models
[1] A. Agresti, Categorical data analysis. JohnWiley & Sons,
2013.
[2] N. Bettenburg, M. Nagappan, and A. E. Hassan, “Towards
improving statistical modeling of software engineering
data: think locally, act globally!” Empirical
Software Engineering, vol. 20, no. 2, pp. 294–335, 2015.
[3] B. M. Bolker, M. E. Brooks, C. J. Clark, S. W. Geange,
J. R. Poulsen, M. H. H. Stevens, and J.-S. S. White,
“Generalized linear mixed models: a practical guide for
ecology and evolution,” Trends in ecology & evolution,
vol. 24, no. 3, pp. 127–135, 2009.
[4] P. Devanbu, T. Zimmermann, and C. Bird, “Belief &
evidence in empirical software engineering,” in Proceedings
of the International Conference on Software Engineering
(ICSE). IEEE, 2016, pp. 108–119.
[5] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita,
and N. Ubayashi, “An empirical study of just-in-time
defect prediction using cross-project models,” in Proceedings
of the 11th Working Conference on Mining Software
Repositories. ACM, 2014, pp. 172–181.
[6] P. J. Guo, T. Zimmermann, N. Nagappan, and B. Murphy,
“Characterizing and predicting which bugs get
fixed: an empirical study of microsoft windows,” in
32nd International Conference on Software Engineering,
vol. 1. IEEE, 2010, pp. 495–504.
[7] A. Hajjem, F. Bellavance, and D. Larocque, “Mixedeffects
random forest for clustered data,” Journal of
Statistical Computation and Simulation, vol. 84, no. 6, pp.
1313–1328, 2014.
[8] A. Hajjem, D. Larocque, and F. Bellavance, “Generalized
mixed effects regression trees,” Statistics & Probability
Letters, vol. 126, pp. 114–118, 2017.
[9] A. E. Hassan, “Predicting faults using the complexity
of code changes,” in IEEE 31st International Conference
on Software Engineering (ICSE). IEEE, 2009, pp. 78–88.
[10] S. Hassan, C. Tantithamthavorn, C.-P. Bezemer, and
A. E. Hassan, “Studying the dialogue between users
and developers of free apps in the google play store,”
Empirical Software Engineering (EMSE), 2017.
[11] S. Herbold, A. Trautsch, and J. Grabowski, “A comparative
study to benchmark cross-project defect prediction
approaches,” IEEE Transactions on Software Engineering,
2017.
[12] ——, “Global vs. local models for cross-project defect
prediction,” Empirical Software Engineering, vol. 22,
no. 4, pp. 1866–1902, 2017.
[13] S. Hosseini, B. Turhan, and D. Gunarathna, “A systematic
literature review and meta-analysis on cross
project defect prediction,” IEEE Transactions on Software
Engineering, 2017.
[14] Q. Huang, X. Xia, and D. Lo, “Supervised vs unsupervised
models: A holistic look at effort-aware just-intime
defect prediction,” in 2017 IEEE International Conference
on Software Maintenance and Evolution (ICSME).
IEEE, 2017, pp. 159–170.
[15] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and
J. Grundy, “An empirical study of model-agnostic techniques
for defect prediction models,” IEEE Transactions
on Software Engineering (TSE), 2020.
[16] J. Jiarpakdee, C. Tantithamthavorn, and J. Grundy,
“Practitioners’ perceptions of the goals and visual explanations
of defect prediction models,” in Proceedings
of the International Conference on Mining Software Repositories
(MSR), 2021, p. To Appear.
[17] J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan,
“The impact of correlated metrics on the interpretation
of defect models,” IEEE Transactions on Software Engineering
(TSE), 2019.
[18] J. Jiarpakdee, C. Tantithamthavorn, and C. Treude,
“Autospearman: Automatically mitigating correlated
metrics for interpreting defect models,” in Proceeding of
the International Conference on Software Maintenance and
Evolution (ICSME), 2018, pp. 92–103.
[19] ——, “AutoSpearman: Automatically Mitigating Correlated
Software Metrics for Interpreting Defect Models,”
in ICSME, 2018, pp. 92–103.
[20] ——, “The impact of automated feature selection techniques
on the interpretation of defect models,” EMSE,
2020.
[21] P. C. Johnson, “Extension of nakagawa & schielzeth’s
r2glmm to random slopes models,” Methods in Ecology
and Evolution, vol. 5, no. 9, pp. 944–946, 2014.
[22] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita,
N. Ubayashi, and A. E. Hassan, “Studying just-in-time
defect prediction using cross-project models,” Empirical
Software Engineering, vol. 21, no. 5, pp. 2072–2106, 2016.
[23] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan,
A. Mockus, A. Sinha, and N. Ubayashi, “A large-scale
empirical study of just-in-time quality assurance,” IEEE
Transactions on Software Engineering, vol. 39, no. 6, pp.
757–773, 2012.
[24] C. Khanan, W. Luewichana, K. Pruktharathikoon,
J. Jiarpakdee, C. Tantithamthavorn, M. Choetkiertikul,
C. Ragkhitwetsagul, and T. Sunetnanta, “Jitbot: An
explainable just-in-time defect prediction bot,” in 2020
35th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, 2020, pp. 1336–1339.
[25] S. Kim, E. J. Whitehead Jr, and Y. Zhang, “Classifying
software changes: Clean or buggy?” IEEE Transactions
on Software Engineering, vol. 34, no. 2, pp. 181–196, 2008.
[26] B. A. Kitchenham, E. Mendes, and G. H. Travassos,
“Cross versus within-company cost estimation studies:
A systematic review,” IEEE Transactions on Software
Engineering, vol. 33, no. 5, 2007.
[27] R. Krishna and T. Menzies, “Bellwethers: A Baseline
Method For Transfer Learning,” IEEE Transactions on
Software Engineering, p. To appear, 2018.
[28] S. Lambiase, A. Cupito, F. Pecorelli, A. De Lucia, and
F. Palomba, “Just-in-time test smell detection and refactoring:
The darts project,” in Proceedings of the 28th
International Conference on Program Comprehension, 2020,
pp. 441–445.
[29] D. Lin, C. Tantithamthavorn, and A. E. Hassan,
“Replication package of our paper,” https://github.
com/SAILResearch/suppmaterial-19-dayi-risk data
merging jit, 2019, (last visited: Nov 11, 2019).
[30] J. N. Mandrekar, “Receiver operating characteristic
curve in diagnostic test assessment,” Journal of Thoracic
Oncology, vol. 5, no. 9, pp. 1315–1316, 2010.
[31] S. McIntosh and Y. Kamei, “Are Fix-Inducing Changes
a Moving Target? A Longitudinal Case Study of Just-In-
Time Defect Prediction,” IEEE Transactions on Software
Engineering, p. To appear, 2017.
[32] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan,
“The impact of code review coverage and code review
participation on software quality: A case study of the
qt, vtk, and itk projects,” in Proceedings of the 11th Working
Conference on Mining Software Repositories. ACM,
2014, pp. 192–201.
[33] T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman,
F. Shull, B. Turhan, and T. Zimmermann, “Local versus
global lessons for defect prediction and effort estimation,”
IEEE Transactions on software engineering, vol. 39,
no. 6, pp. 822–834, 2013.
[34] T. Menzies, J. Greenwald, and A. Frank, “Data mining
static code attributes to learn defect predictors,” IEEE
transactions on software engineering, vol. 33, no. 1, pp.
2–13, 2006.
[35] A. Mockus and D. M. Weiss, “Predicting risk of software
changes,” Bell Labs Technical Journal, vol. 5, no. 2,
pp. 169–180, 2000.
[36] R. Moser, W. Pedrycz, and G. Succi, “A comparative
analysis of the efficiency of change metrics and static
code attributes for defect prediction,” in Proceedings of
the 30th international conference on Software engineering.
ACM, 2008, pp. 181–190.
[37] N. Nagappan and T. Ball, “Use of relative code churn
measures to predict system defect density,” in Proceedings
of the 27th international conference on Software
engineering. ACM, 2005, pp. 284–292.
[38] S. Nakagawa and H. Schielzeth, “A general and simple
method for obtaining r2 from generalized linear mixedeffects
models,” Methods in Ecology and Evolution, vol. 4,
no. 2, pp. 133–142, 2013.
[39] J. C. Pinheiro and D. M. Bates, “Mixed-effects models
in s and s-plus springer,” New York, 2000.
[40] C. Pornprasit and C. Tantithamthavorn, “JITLine: A
Simpler, Better, Faster, Finer-grained Just-In-Time Defect
Prediction,” in Proceedings of the International Conference
on Mining Software Repositories (MSR), 2021, p.
To Appear.
[41] R. Purushothaman and D. E. Perry, “Toward understanding
the rhetoric of small source code changes,”
IEEE Transactions on Software Engineering, vol. 31, no. 6,
pp. 511–526, 2005.
[42] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee,
C. Bergmeir, J. Grundy, and W. Buntine, “SQAPlanner:
Generating Data-Informed Software Quality Improvement
Plans,” arXiv preprint arXiv:2102.09687, 2021.
[43] G. Rodr´ıguez-P´erez, G. Robles, and J. M. Gonz´alez-
Barahona, “Reproducibility and credibility in empirical
software engineering: A case study based on a systematic
literature review of the use of the szz algorithm,”
Information and Software Technology, vol. 99, pp. 164–176,
2018.
[44] C. Rosen, B. Grawi, and E. Shihab, “Commit guru:
Analytics and risk prediction of software commits,” in
Proceedings of the 2015 10th Joint Meeting on Foundations
of Software Engineering, ser. ESEC/FSE 2015. New York,
NY, USA: ACM, 2015, pp. 966–969.
[45] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang,
“An industrial study on the risk of software changes,”
in Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering.
ACM, 2012, p. 62.
[46] J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do
changes induce fixes?” in ACM sigsoft software engineering
notes, vol. 30, no. 4. ACM, 2005, pp. 1–5.
[47] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect
prediction for imbalanced data,” in Proceedings of the
37th International Conference on Software Engineering-
Volume 2. IEEE Press, 2015, pp. 99–108.
[48] C. Tantithamthavorn and A. E. Hassan, “An experience
report on defect modelling in practice: Pitfalls and challenges,”
in Proceedings of the 40th International Conference
on Software Engineering: Software Engineering in Practice.
ACM, 2018, pp. 286–295.
[49] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto,
“The impact of class rebalancing techniques on the performance
and interpretation of defect prediction models,”
IEEE Transactions on Software Engineering, 2018.
[50] C. Tantithamthavorn, J. Jiarpakdee, and J. Grundy, “Explainable
AI for Software Engineering,” arXiv preprint
arXiv:2012.01614, 2020.
[51] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and
K. Matsumoto, “Automated Parameter Optimization
of Classification Techniques for Defect Prediction Models,”
in ICSE, 2016, pp. 321–332.
[52] ——, “An empirical comparison of model validation
techniques for defect prediction models,” IEEE Transactions
on Software Engineering, vol. 43, no. 1, pp. 1–18,
2016.
[53] ——, “An Empirical Comparison of Model Validation
Techniques for Defect Prediction Models,” TSE, vol. 43,
no. 1, pp. 1–18, 2017.
[54] ——, “The Impact of Automated Parameter Optimization
on Defect Prediction Models,” TSE, 2018.
[55] P. Thongtanunam and A. E. Hassan, “Review dynamics
and their impact on software quality,” in IEEE Transaction
on Software Engineering (TSE), 2020, p. to appear.
[56] ——, “Review dynamics and their impact on software
quality,” IEEE Transactions on Software Engineering, 2020.
[57] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano,
“On the relative value of cross-company and withincompany
data for defect prediction,” Empirical Software
Engineering, vol. 14, no. 5, pp. 540–578, 2009.
[58] B. Turhan, A. Tosun, and A. Bener, “Empirical evaluation
of mixed-project defect prediction models,” in
37th EUROMICRO Conference on Software Engineering
and Advanced Applications (SEAA). IEEE, 2011, pp. 396–
403.
[59] J. Wang, E. R. Gamazon, B. L. Pierce, B. E. Stranger,
H. K. Im, R. D. Gibbons, N. J. Cox, D. L. Nicolae, and
L. S. Chen, “Imputing gene expression in uncollected
tissues within and beyond gtex,” The American Journal
of Human Genetics, vol. 98, no. 4, pp. 697–708, 2016.
[60] S. Yathish, J. Jiarpakdee, P. Thongtanunam, and C. Tantithamthavorn,
“Mining Software Defects: Should We
Consider Affected Releases?” in ICSE, 2019, pp. 654–
665.
[61] F. Zhang, A. Mockus, I. Keivanloo, and Y. Zou, “Towards
building a universal defect prediction model,”
in Proceedings of the 11th Working Conference on Mining
Software Repositories. ACM, 2014, pp. 182–191.
[62] ——, “Towards building a universal defect prediction
model with rank transformed predictors,” Empirical
Software Engineering, vol. 21, no. 5, pp. 2107–2145, 2016.
[63] F. Zhang, A. Mockus, Y. Zou, F. Khomh, and A. E.
Hassan, “How does context affect the distribution of
software maintainability metrics?” in Software Maintenance
(ICSM), 2013 29th IEEE International Conference
on. IEEE, 2013, pp. 350–359.
[64] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy, “Cross-project defect prediction: a large
scale experiment on data vs. domain vs. process,” in
Proceedings of the 7th joint meeting of the European software
engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering. ACM, 2009,
pp. 91–100.


>><[N]>The Relationship between Commit Message Detail and Defect Proneness in Java Projects on GitHub
[1] S. Bird, E. Loper, and E. Klein. Natural Language Pro-
cessing with Python. O'Reilly Media Inc, 2009.
[2] J. M. Chambers and T. J. Hastie, editors. Statistical
Models in S. Wadsworth and Brooks/Cole, 1992.
[3] R. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen.
Boa: A Language and Infrastructure for Analyzing
Ultra-Large-Scale Software Repositories. In Proc. of
the 35th Int'l Conf. on Software Engineering (ICSE),
pages 422{431, 2013.
[4] B. Efron. How Biased is the Apparent Error Rate of a
Prediction Rule? Journal of the American Statistical
Association, 81(394):461{470, 1986.
[5] T. Fukushima, Y. Kamei, S. McIntosh, K. Yamashita,
and N. Ubayashi. An Empirical Study of Just-in-Time
Defect Prediction using Cross-Project Models. In Proc.
of the 11th Working Conf. on Mining Software Reposi-
tories (MSR), pages 172{181, 2014.
[6] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita,
N. Ubayashi, and A. E. Hassan. Studying Just-In-Time
Defect Prediction using Cross-Project Models. Empir-
ical Software Engineering, To appear, 2016.
[7] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan,
A. Mockus, A. Sinha, and N. Ubayashi. A Large-
Scale Empirical Study of Just-in-Time Quality Assurance.
Transactions on Software Engineering (TSE),
39(6):757{773, 2013.
[8] S. Kim, E. J.Whitehead, Jr., and Y. Zhang. Classifying
software changes: Clean or buggy? Transactions on
Software Engineering (TSE), 34(2):181{196, 2008.
[9] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan.
An Empirical Study of the Impact of Modern Code Review
Practices on Software Quality. Empirical Software
Engineering, In press, 2015.
[10] A. Mockus and D. M. Weiss. Predicting Risk of Software
Changes. Bell Labs Technical Journal, 5(2):169{
180, 2000.
[11] T. Myer and B. Whately. SpamBayes: Effective opensource,
Bayesian based, email classification system. In
Proc. of the 7th Annual Collaboration, Electronic Mes-
saging, Anti-Abuse and Spam Conference, 2004.
[12] C. Rosen, B. Grawi, and E. Shihab. Commit guru:
Analytics and risk prediction of software commits. In
Proceedings of the 2015 10th Joint Meeting on Founda-
tions of Software Engineering, ESEC/FSE 2015, pages
966{969, New York, NY, USA, 2015. ACM.
[13] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Zhang.
An Industrial Study on the Risk of Software Change.
In Proc. of the 20th Symposium on the Foundations of
Software Engineering (FSE), pages 62:1{62:11, 2012.
[14] E. Shihab, A. Ihara, Y. , Kamei, W. M. Ibrahim,
M. Ohira, B. Adams, A. E. Hassan, and K. Matsumoto.
Studying re-opened bugs in open source software. Em-
pirical Software Engineering, 18(5):1005{1042, 2012.
[15] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online Defect
Prediction for Imbalanced Data. In Proc. of the 37th
Int'l Conf. on Software Engineering (ICSE), volume 2,
pages 99{108, 2015.


>><[N]>VulDigger: A Just-in-Time and Cost-Aware Tool for Digging Vulnerability-Contributing Changes
[1] “The heartbleed bug,” [Accessed 20-March-2017]. [Online]. Available:
http://heartbleed.com/
[2] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller, “Predicting
vulnerable software components,” in Proceedings of the 14th ACM
conference on Computer and communications security. ACM, 2007,
pp. 529–540.
[3] Y. Shin and L. Williams, “An empirical model to predict security
vulnerabilities using code complexity metrics,” in Proceedings of the
Second ACM-IEEE international symposium on Empirical software
engineering and measurement. ACM, 2008, pp. 315–317.
[4] Y. Shin, A. Meneely, L. Williams, and J. A. Osborne, “Evaluating
complexity, code churn, and developer activity metrics as indicators of
software vulnerabilities,” IEEE Transactions on Software Engineering,
vol. 37, no. 6, pp. 772–787, 2011.
[5] R. Scandariato, J. Walden, A. Hovsepyan, and W. Joosen, “Predicting
vulnerable software components via text mining,” IEEE Transactions on
Software Engineering, vol. 40, no. 10, pp. 993–1006, 2014.
[6] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y. Acar, “Vccfinder: Finding potential vulnerabilities in open-source
projects to assist code audits,” in Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security. ACM, 2015,
pp. 426–437.
[7] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757–773, 2013.
[8] “Flawfinder homepage,” [Accessed 20-March-2017]. [Online].
Available: http://www.dwheeler.com/flawfinder/
[9] J. Walden, J. Stuckman, and R. Scandariato, “Predicting vulnerable
components: Software metrics vs text mining,” in Software Reliability
Engineering (ISSRE), 2014 IEEE 25th International Symposium on.
IEEE, 2014, pp. 23–33.
[10] A. Meneely, H. Srinivasan, A. Musa, A. R. Tejeda, M. Mokary,
and B. Spates, “When a patch goes bad: Exploring the properties
of vulnerability-contributing commits,” in 2013 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement.
IEEE, 2013, pp. 65–74.
[11] A. Bosu, J. C. Carver, M. Hafiz, P. Hilley, and D. Janni, “Identifying
the characteristics of vulnerable code changes: An empirical study,” in
Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering. ACM, 2014, pp. 257–268.
[12] “Mozilla foundations security advisories,” [Accessed 31-March-2017].
[Online]. Available: https://www.mozilla.org/en-US/security/advisories/
[13] J. ´ Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” in ACM sigsoft software engineering notes, vol. 30, no. 4. ACM,
2005, pp. 1–5.
[14] F. Massacci, S. Neuhaus, and V. H. Nguyen, “After-life vulnerabilities: a
study on firefox evolution, its vulnerabilities, and fixes,” in International
Symposium on Engineering Secure Software and Systems. Springer,
2011, pp. 195–208.
[15] E. Giger, M. Pinzger, and H. C. Gall, “Comparing fine-grained source
code changes and code churn for bug prediction,” in Proceedings of the
8th Working Conference on Mining Software Repositories. ACM, 2011,
pp. 83–92.
[16] “Github api v3 | github developer guide,” [Accessed 12-March-2017].
[Online]. Available: https://developer.github.com/v3/
[17] A. G. Koru, D. Zhang, K. El Emam, and H. Liu, “An investigation into
the functional form of the size-defect relationship for software modules,”
IEEE Transactions on Software Engineering, vol. 35, no. 2, pp. 293–304,
2009.
[18] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell
Labs Technical Journal, vol. 5, no. 2, pp. 169–180, 2000.
[19] M. Piancó, B. Fonseca, and N. Antunes, “Code change history and software
vulnerabilities,” in Dependable Systems and Networks Workshop,
2016 46th Annual IEEE/IFIP International Conference on. IEEE, 2016,
pp. 6–9.
[20] M. P. Fay and M. A. Proschan, “Wilcoxon-mann-whitney or t-test? on
assumptions for hypothesis tests and multiple interpretations of decision
rules,” Statistics surveys, vol. 4, p. 1, 2010.
[21] R. E. McGrath and G. J. Meyer, “When effect sizes disagree: the case
of r and d.” Psychological methods, vol. 11, no. 4, p. 386, 2006.
[22] J. L. Myers, A. Well, and R. F. Lorch, Research design and statistical
analysis. Routledge, 2010.
[23] “scikit-learn: Machine learning in python,” [Accessed 30-March-2017].
[Online]. Available: http://scikit-learn.org/stable/
[24] T. Mende and R. Koschke, “Effort-aware defect prediction models,” in
Software Maintenance and Reengineering (CSMR), 2010 14th European
Conference on. IEEE, 2010, pp. 107–116.
[25] Y. Shin and L. Williams, “Can traditional fault prediction models be used
for vulnerability prediction?” Empirical Software Engineering, vol. 18,
no. 1, pp. 25–59, 2013.



>><[N]>Warning-Introducing Commits vs Bug-Introducing Commits: A tool, statistical models, and a preliminary user study
[1] M. Tufano, F. Palomba, G. Bavota, M. Di Penta, R. Oliveto, A. De Lucia,
and D. Poshyvanyk, “There and back again: Can you compile that
snapshot?” Journal of Software: Evolution and Process, vol. 29, no. 4,
p. e1838, 2017.
[2] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus,
A. Sinha, and N. Ubayashi, “A large-scale empirical study of
just-in-time quality assurance,” vol. 39, no. 6. Piscataway, NJ,
USA: IEEE Press, Jun. 2013, pp. 757–773. [Online]. Available:
http://dx.doi.org/10.1109/TSE.2012.70
[3] F. Rahman, S. Khatri, E. T. Barr, and P. Devanbu, “Comparing static
bug finders and statistical prediction,” in Proceedings of the 36th
International Conference on Software Engineering, ser. ICSE 2014.
New York, NY, USA: ACM, 2014, pp. 424–434. [Online]. Available:
http://doi.acm.org/10.1145/2568225.2568269
[4] L.-P. Querel and P. C. Rigby, “WarningsGuru, research scripts and data
for replication,” https://doi.org/10.5281/zenodo.3747582.
[5] ——, “WarningsGuru tool GitHub Repo,” https://github.com/louisq/
warningsguru.
[6] N. Ayewah, W. Pugh, J. D. Morgenthaler, J. Penix, and Y. Zhou,
“Evaluating static analysis defect warnings on production software,”
in Proceedings of the 7th ACM SIGPLAN-SIGSOFT Workshop on
Program Analysis for Software Tools and Engineering, ser. PASTE
’07. New York, NY, USA: ACM, 2007, pp. 1–8. [Online]. Available:
http://doi.acm.org/10.1145/1251535.1251536
[7] M. Beller, R. Bholanath, S. McIntosh, and A. Zaidman, “Analyzing
the state of static analysis: A large-scale evaluation in open source
software,” in 2016 IEEE 23rd International Conference on Software
Analysis, Evolution, and Reengineering (SANER), vol. 1, March 2016,
pp. 470–481.
[8] C. Couto, J. E. Montandon, C. Silva, and M. T. Valente, “Static
correspondence and correlation between field defects and warnings
reported by a bug finding tool,” vol. 21, no. 2, 2013, pp. 241–257.
[Online]. Available: http://dx.doi.org/10.1007/s11219-011-9172-5
[9] J. Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
fixes?” SIGSOFT Softw. Eng. Notes, vol. 30, no. 4, p. 15, May 2005.
[Online]. Available: https://doi.org/10.1145/1082983.1083147
[10] C. Rosen, B. Grawi, and E. Shihab, “Commit guru: Analytics and risk
prediction of software commits,” in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2015.
New York, NY, USA: ACM, 2015, pp. 966–969. [Online]. Available:
http://doi.acm.org/10.1145/2786805.2803183
[11] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A systematic
literature review on fault prediction performance in software engineering,”
vol. 38, no. 6, Nov 2012, pp. 1276–1304.
[12] L.-P. Querel and P. C. Rigby, “Warningsguru: Integrating statistical
bug models with static analysis to provide timely and specific bug
warnings,” in Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ser. ESEC/FSE 2018. New
York, NY, USA: Association for Computing Machinery, 2018, p.
892895. [Online]. Available: https://doi.org/10.1145/3236024.3264599
[13] The Apache Software Foundation, “Maven - POM Reference,” 2016,
https://maven.apache.org/pom.html.
[14] KDM Analytics, “Blade Tool Output Integration Framework (TOIF),”
2016, http://www.kdmanalytics.com/toif/.
[15] MITRE Corporation, “Common Weakness Enumeration (CWE),” 2016,
https://cwe.mitre.org/.
[16] H. Tang, T. Lan, D. Hao, and L. Zhang, “Enhancing defect
prediction with static defect analysis,” in Proceedings of the 7th
Asia-Pacific Symposium on Internetware, ser. Internetware ’15. New
York, NY, USA: ACM, 2015, pp. 43–51. [Online]. Available:
http://doi.acm.org/10.1145/2875913.2875922
[17] M. G. Nanda, M. Gupta, S. Sinha, S. Chandra, D. Schmidt,
and P. Balachandran, “Making defect-finding tools work for you,”
in Proceedings of the 32Nd ACM/IEEE International Conference
on Software Engineering - Volume 2, ser. ICSE ’10. New
York, NY, USA: ACM, 2010, pp. 99–108. [Online]. Available:
http://doi.acm.org/10.1145/1810295.1810310
[18] S. Kim, T. Zimmermann, K. Pan, and E. J. J. Whitehead,
“Automatic identification of bug-introducing changes,” in Proceedings
of the 21st IEEE/ACM International Conference on Automated
Software Engineering, ser. ASE ’06. Washington, DC, USA:
IEEE Computer Society, 2006, pp. 81–90. [Online]. Available:
http://dx.doi.org/10.1109/ASE.2006.23
[19] A. Mockus, R. T. Fielding, and J. D. Herbsleb, “Two case studies
of open source software development: Apache and mozilla,” vol. 11,
no. 3. New York, NY, USA: ACM, Jul. 2002, pp. 309–346. [Online].
Available: http://doi.acm.org/10.1145/567793.567795
[20] P. C. Rigby, D. M. German, L. Cowen, and M.-A. Storey, “Peer review
on open-source software projects: Parameters, statistical models, and
theory,” ACM Trans. Softw. Eng. Methodol., vol. 23, no. 4, Sep. 2014.
[Online]. Available: https://doi.org/10.1145/2594458
[21] E. Giger, M. Pinzger, and H. C. Gall, “Comparing fine-grained source
code changes and code churn for bug prediction,” in Proceedings
of the 8th Working Conference on Mining Software Repositories, ser.
MSR ’11. New York, NY, USA: ACM, 2011, pp. 83–92. [Online].
Available: http://doi.acm.org/10.1145/1985441.1985456
[22] N. Nagappan and T. Ball, “Use of relative code churn measures
to predict system defect density,” in Proceedings of the 27th
International Conference on Software Engineering, ser. ICSE ’05.
New York, NY, USA: ACM, 2005, pp. 284–292. [Online]. Available:
http://doi.acm.org/10.1145/1062455.1062514
[23] C. Bird, N. Nagappan, P. Devanbu, H. Gall, and B. Murphy,
“Does distributed development affect software quality?: An empirical
case study of windows vista,” vol. 52, no. 8. New York,
NY, USA: ACM, Aug. 2009, pp. 85–93. [Online]. Available:
http://doi.acm.org/10.1145/1536616.1536639
[24] J. D. Herbsleb and A. Mockus, “An empirical study of speed and
communication in globally distributed software development,” vol. 29,
no. 6. Piscataway, NJ, USA: IEEE Press, Jun. 2003, pp. 481–494.
[Online]. Available: http://dx.doi.org/10.1109/TSE.2003.1205177
[25] F. Camilo, A. Meneely, and M. Nagappan, “Do bugs foreshadow
vulnerabilities? a study of the chromium project,” in Mining Software
Repositories (MSR), 2015 IEEE/ACM 12th Working Conference on.
IEEE, 2015, pp. 269–279.
[26] L. Averell and A. Heathcote, “The form of the forgetting curve and the
fate of memories,” vol. 55, no. 1. Elsevier, 2011, pp. 25–35.
[27] P. Devanbu, T. Zimmermann, and C. Bird, “Belief evidence in empirical
software engineering,” in 2016 IEEE/ACM 38th International Conference
on Software Engineering (ICSE), May 2016, pp. 108–119.
[28] F. Wedyan, D. Alrmuny, and J. M. Bieman, “The effectiveness of
automated static analysis tools for fault detection and refactoring
prediction,” in Proceedings of the 2009 International Conference
on Software Testing Verification and Validation, ser. ICST ’09.
Washington, DC, USA: IEEE Computer Society, 2009, pp. 141–150.
[Online]. Available: http://dx.doi.org/10.1109/ICST.2009.21



>><[N]>Watch out for Extrinsic Bugs! A Case Study of their Impact in Just-In-Time Bug Prediction Models on the OpenStack project
[1] G. Rodr´ıguez-P´erez, A. Zaidman, A. Serebrenik, G. Robles, and
J. M. Gonzalez-Barahona, “What if a bug has a different origin?
making sense of bugs without an explicit bug introducing
change,” in Proceedings of the 12th International Symposium on
Empirical Software Engineering and Measurement, p. 52, ACM, 2018.
[2] G. Rodr´ıguez-P´erez, G. Robles, A. Serebrenik, A. Zaidman, D. M.
German, and J. M. Gonzalez-Barahona, “How bugs are born: A
model to identify how bugs are introduced in software components,”
Empirical Software Engineering, 2019.
[3] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time
quality assurance,” IEEE Transactions on Software Engineering,
vol. 39, no. 6, pp. 757–773, 2013.
[4] J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do changes
induce fixes?,” Proceedings of the International Workshop on Mining
software repositories, pp. 1–5, 2005.
[5] S. Kim, T. Zimmermann, K. Pan, and E. J. Whitehead Jr,
“Automatic identification of bug-introducing changes,” in 21st
IEEE/ACM International Conference on Automated Software Engineering,
pp. 81–90, IEEE, 2006.
[6] C.Williams and J. Spacco, “SZZ revisited: Verifying when changes
induce fixes,” in Proceedings of the workshop on Defects in large
software systems, pp. 32–36, ACM, 2008.
[7] E. Giger, M. D’Ambros, M. Pinzger, and H. C. Gall, “Method-level
bug prediction,” in Proceedings of the ACM-IEEE international symposium
on Empirical software engineering and measurement, pp. 171–
180, ACM, 2012.
[8] H. Hata, O. Mizuno, and T. Kikuno, “Bug prediction based on finegrained
module histories,” in Proceedings of the 34th International
Conference on Software Engineering, pp. 200–210, IEEE Press, 2012.
[9] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting defects
for eclipse,” in Predictor Models in Software Engineering, 2007.
PROMISE’07: ICSE Workshops 2007. International Workshop on,
pp. 9–9, IEEE, 2007.
[10] N. Nagappan and T. Ball, “Use of relative code churn measures
to predict system defect density,” in Proceedings of the 27th international
conference on Software engineering, pp. 284–292, ACM, 2005.
[11] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, “A
systematic literature review on fault prediction performance in
software engineering,” IEEE Transactions on Software Engineering,
vol. 38, no. 6, pp. 1276–1304, 2012.
[12] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang, “An industrial
study on the risk of software changes,” in Proceedings of the
ACM SIGSOFT 20th International Symposium on the Foundations of
Software Engineering, p. 62, ACM, 2012.
[13] A. Mockus, “Missing data in software engineering,” in Guide to
advanced empirical software engineering, pp. 185–200, Springer, 2008.
[14] S. McIntosh and Y. Kamei, “Are fix-inducing changes a moving
target? a longitudinal case study of just-in-time defect prediction,”
IEEE Transactions on Software Engineering, vol. 44, no. 5, pp. 412–
428, 2018.
[15] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, A. Ihara, and
K. Matsumoto, “The impact of mislabelling on the performance
and interpretation of defect prediction models,” in Proceedings
of the 37th International Conference on Software Engineering, vol. 1,
pp. 812–823, IEEE, 2015.
[16] G. Rodr´ıguez-P´erez, G. Robles, and J. M. Gonzalez-Barahona, “Reproducibility
and credibility in empirical software engineering: A
case study based on a systematic literature review of the use of the
szz algorithm,” Information and Software Technology, 2018.
[17] A. Zeller, W. Hughes, J. Lavery, K. Doran, C. T. Morrison, R. T.
Snodgrass, and R. F. St¨ark, “Causes and effects in computer programs,”
in Proceedings of the 5th InternationalWorkshop on Computer,
pp. 482–508, 2011.
[18] D. M. German, A. E. Hassan, and G. Robles, “Change impact
graphs: Determining the impact of prior codechanges,” Information
and Software Technology, vol. 51, no. 10, pp. 1394–1408, 2009.
[19] T.-H. Chen, M. Nagappan, E. Shihab, and A. E. Hassan, “An empirical
study of dormant bugs,” in Proceedings of the 11th Working
Conference on Mining Software Repositories, pp. 82–91, ACM, 2014.
[20] L. Prechelt and A. Pepper, “Why software repositories are not
used for defect-insertion circumstance analysis more often: A
case study,” Information and Software Technology, vol. 56, no. 10,
pp. 1377–1389, 2014.
[21] A. Ahluwalia, D. Falessi, and M. Di Penta, “Snoring: a noise in
defect prediction datasets,” in Proceedings of the 16th International
Conference on Mining Software Repositories, pp. 63–67, IEEE Press,
2019.
[22] M. Nayrolles and A. Hamou-Lhadj, “Clever: combining code
metrics with clone detection for just-in-time fault prevention and
resolution in large industrial projects,” in Proceedings of the 15th
International Conference on Mining Software Repositories, pp. 153–
164, ACM, 2018.
[23] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction
for imbalanced data,” in Proceedings of the 37th International
Conference on Software Engineering, vol. 2, pp. 99–108, IEEE, 2015.
[24] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict
component failures,” in Proceedings of the 28th international conference
on Software engineering, pp. 452–461, ACM, 2006.
[25] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy, “Predicting
fault incidence using software change history,” IEEE Transactions
on software engineering, vol. 26, no. 7, pp. 653–661, 2000.
[26] S. Kim, E. J. Whitehead Jr, and Y. Zhang, “Classifying software
changes: Clean or buggy?,” IEEE Transactions on Software Engineering,
vol. 34, no. 2, pp. 181–196, 2008.
[27] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “The impact of
code review coverage and code review participation on software
quality: A case study of the qt, vtk, and itk projects,” in Proceedings
of the 11th Working Conference on Mining Software Repositories,
pp. 192–201, ACM, 2014.
[28] O. Kononenko, O. Baysal, L. Guerrouj, Y. Cao, and M. W. Godfrey,
“Investigating code review quality: Do people and participation
matter?,” in IEEE international conference on software maintenance
and evolution (ICSME), pp. 111–120, IEEE, 2015.
[29] Y. Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using
cross-project models,” Empirical Software Engineering, vol. 21, no. 5,
pp. 2072–2106, 2016.
[30] K. Herzig, S. Just, and A. Zeller, “It’s not a bug, it’s a feature: how
misclassification impacts bug prediction,” in Proceedings of the 35th
international conference on software engineering, pp. 392–401, IEEE
Press, 2013.
[31] D. A. da Costa, S. McIntosh, W. Shang, U. Kulesza, R. Coelho,
and A. E. Hassan, “A framework for evaluating the results of
the szz approach for identifying bug-introducing changes,” IEEE
Transactions on Software Engineering, vol. 43, no. 7, pp. 641–657,
2017.
[32] J. Aranda and G. Venolia, “The secret life of bugs: Going past the
errors and omissions in software repositories,” in Proceedings of
the 31st international conference on software engineering, pp. 298–308,
IEEE Computer Society, 2009.
[33] G. Rodr´ıguez-P´erez, J. M. Gonzalez-Barahona, G. Robles, D. Dalipaj,
and N. Sekitoleko, “Bugtracking: A tool to assist in the
identification of bug reports,” in IFIP International Conference on
Open Source Systems, pp. 192–198, Springer, 2016.
[34] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in
defect prediction,” in Proceedings of the 33rd International Conference
on Software Engineering, pp. 481–490, IEEE, 2011.
[35] C. Seiffert, T. M. Khoshgoftaar, J. Van Hulse, and A. Folleco, “An
empirical study of the classification performance of learners on
imbalanced and noisy software quality data,” Information Sciences,
vol. 259, pp. 571–595, 2014.
[36] F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, “Sample size
vs. bias in defect prediction,” in Proceedings of the 9th joint meeting
on foundations of software engineering, pp. 147–157, ACM, 2013.
[37] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “An empirical
study of the impact of modern code review practices on software
quality,” Empirical Software Engineering, vol. 21, no. 5, pp. 2146–
2189, 2016.
[38] M. Zhou and A. Mockus, “Does the initial environment impact
the future of developers?,” in Proceedings of the 33rd International
Conference on Software Engineering, pp. 271–280, ACM, 2011.
[39] F. Zhang, A. Mockus, I. Keivanloo, and Y. Zou, “Towards building
a universal defect prediction model with rank transformed predictors,”
Empirical Software Engineering, vol. 21, no. 5, pp. 2107–2145,
2016.
[40] P. E. McKight and J. Najab, “Kruskal-wallis test,” The corsini
encyclopedia of psychology, pp. 1–1, 2010.
[41] E. Whitley and J. Ball, “Statistics review 6: Nonparametric methods,”
Critical care, vol. 6, no. 6, p. 509, 2002.
[42] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and A. Zeller, “Predicting
faults from cached history,” in Proceedings of the 29th
international conference on Software Engineering, pp. 489–498, IEEE
Computer Society, 2007.
[43] D. Di Nucci, F. Palomba, G. De Rosa, G. Bavota, R. Oliveto, and
A. De Lucia, “A developer centered bug prediction model,” IEEE
Transactions on Software Engineering, vol. 44, no. 1, pp. 5–24, 2018.
[44] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,”
Empirical Software Engineering, pp. 1–40, 2018.
[45] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-grained just-intime
defect prediction,” Journal of Systems and Software, vol. 150,
pp. 22–36, 2019.
[46] W. Maalej and H. Nabil, “Bug report, feature request, or simply
praise? on automatically classifying app reviews,” in IEEE 23rd
international requirements engineering conference (RE), pp. 116–125,
IEEE, 2015.
[47] A. Bachmann, C. Bird, F. Rahman, P. Devanbu, and A. Bernstein,
“The missing links: bugs and bug-fix commits,” in Proceedings of
the 18th ACM SIGSOFT international symposium on Foundations of
software engineering, pp. 97–106, ACM, 2010.
[48] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, “Relink: recovering
links between bugs and changes,” in Proceedings of the 19th ACM
SIGSOFT symposium and the 13th European conference on Foundations
of software engineering, pp. 15–25, ACM, 2011.
[49] M. Wen, R. Wu, and S.-C. Cheung, “Locus: Locating bugs from
software changes,” in 31st IEEE/ACM International Conference on
Automated Software Engineering (ASE), pp. 262–273, IEEE, 2016.
[50] C. Wohlin, P. Runeson, M. H¨ ost, M. C. Ohlsson, B. Regnell,
and A. Wessl´en, Experimentation in software engineering. Springer
Science & Business Media, 2012.
[51] P. Runeson, M. Host, A. Rainer, and B. Regnell, Case study research
in software engineering: Guidelines and examples. John Wiley & Sons,
2012.
[52] S. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, “Selecting
empirical methods for software engineering research,” in Guide to
advanced empirical software engineering, pp. 285–311, Springer, 2008.





>><N>a study on the changes of dynamic feature code when fixing bugs towards the benefits and costs of python dynamic features
1 Akerblom B, Stendahl J, Tumlin M, et al. Tracing dynamic features in Python programs. In: Proceedings of the 11th
Working Conference on Mining Software Repositories, Hyderabad, 2014. 292–295
2 Holkner A, Harland J. Evaluating the dynamic behaviour of Python applications. In: Proceedings of the 32nd Australasian
Conference on Computer Science, Wellington, 2009. 19–28
3 Bodden E, Sewe A, Sinschek J, et al. Taming reflection: aiding static analysis in the presence of reflection and custom
class loaders categories and subject descriptors. In: Proceedings of the 33rd International Conference on Software
Engineering, Waikiki, 2011. 241–250
4 Richards G, Hammer C, Burg B. The eval that men do: a large-scale study of the use of eval in JavaScript applications.
In: Proceedings of the 25th European Conference on Object-oriented Programming, Lancaster, 2011. 52–78
5 Richards G, Lebresne S, Burg B, et al. An analysis of the dynamic behavior of JavaScript programs. ACM SIGPLAN
Notices, 2010, 45: 1–12
6 Calla´u O, Robbes R, Tanter E, et al. How developers use the dynamic features of programming languages: the case
of Smalltalk. In: Proceedings of the 8th Working Conference on Mining Software Repositories, Waikiki, 2011. 23–32
7 Dufour B, Goard C, Hendren L, et al. Measuring the dynamic behaviour of AspectJ programs. In: Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications,
Vancouver, 2004. 150–169
8 Wang B B, Chen L, Ma W W Y, et al. An empirical study on the impact of Python dynamic features on changeproneness.
In: Proceedings of the 27th International Conference on Software Engineering and Knowledge Engineering, Pittsburgh, 2015. 134–139
9 Park J, Lim I, Ryu S. Battles with false positives in static analysis of JavaScript web applications in the wild. In:
Proceedings of the 38th International Conference on Software Engineering Companion, Austin, 2016. 61–70
10 Sanner M F. Python: a programming language for software integration and development. J Mol Graph Model, 1999, 17: 57–61
11 Chen Z F, Ma W W Y, Lin W, et al. Tracking down dynamic feature code changes against Python software evolution.
In: Proceedings of the 3rd International Conference on Trustworthy Systems and Their Applications, Wuhan, 2016. 54–63
12 Qian J, Chen L, Xu B W. Finding shrink critical section refactoring opportunities for the evolution of concurrent code
in trustworthy software. Sci China Inf Sci, 2013, 56: 012106
13 Chen L, Qian J, Zhou Y M, et al. Identifying extract class refactoring opportunities for internetware. Sci China Inf
Sci, 2014, 57: 072103
14 Feng Y, Liu Q, Dou M Y, et al. Mubug: a mobile service for rapid bug tracking. Sci China Inf Sci, 2016, 59: 013101
15 Zhang J, Wang X, Hao D, et al. A survey on bug-report analysis. Sci China Inf Sci, 2015, 58: 021101
16 Chen L, Ma W W Y, Zhou Y M, et al. Empirical analysis of network measures for predicting high severity software
faults. Sci China Inf Sci, 2016, 59: 122901
17 Kim S, Zimmermann T,Whitehead E J. Predicting faults from cached history. In: Proceedings of the 29th International
Conference on Software Engineering, Minneapolis, 2007. 489–498
18 Fischer M, Pinzger M, Gall H. Populating a release history database from version control and bug tracking systems.
In: Proceedings of the International Conference on Software Maintenance, Amsterdam, 2003. 23–32
19 Hall T. Some code smells have a significant but small effect on faults. ACM Trans Softw Eng Methodol, 2014, 23: 33
20 Khomh F, Penta M D, Gu´eh´eneuc Y G, et al. An exploratory study of the impact of antipatterns on class changeand
fault-proneness. Empir Softw Eng, 2012, 17: 243–275
21 Zhong H, Su Z D. An empirical study on real bug fixes. In: Proceedings of the 37th International Conference on Software Engineering, Florence, 2015. 913–923
22 Monographs B. Statistical methods for research workers. In: Breakthroughs in Statistics. Berlin: Springer-Verlag, 1992. 66–70
23 Xu Z G, Liu P, Zhang X Y, et al. Python predictive analysis for bug detection. In: Proceedings of the 24th ACM
SIGSOFT International Symposium on Foundations of Software Engineering, Seattle,    2016. 121–132
24 Chen Z F, Chen L, Zhou Y M, et al. Dynamic slicing of Python programs. In: Proceedings of the 38th Computer
Software and Applications Conference, Vasteras, 2014. 219–228
25 Xu Z G, Qian J, Chen L, et al. Static slicing for Python first-class objects. In: Proceedings of the 13th International
Conference on Quality Software, Najing, 2013. 117–124
26 Chen Z F, Chen L, Xu B W. Hybrid information flow analysis for Python bytecode. In: Proceedings of the 11th Web
Information System and Application Conference, Tianjin, 2014. 95–100
27 Chen L, Xu B W, Zhou T L, et al. A constraint based bug checking approach for Python. In: Proceedings of the 33rd
Computer Software and Applications Conference, Seattle, 2009. 306–311
28 Vitousek M M, Kent A M, Siek J G, et al. Design and evaluation of gradual typing for Python. In: Proceedings of
the 10th ACM Symposium on Dynamic Languages, Portland, 2014. 45–56
29 Xu Z G, Zhang X Y, Chen L, et al. Python probabilistic type inference with natural language support. In: Proceedings
of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, Seattle, 2016. 607–618
30 Akerblom B, Wrigstad T. Measuring polymorphism in Python programs. In: Proceedings of the 11th Symposium on
Dynamic Languages, Pittsburgh, 2015. 114–128
31 Gorbovitski M, Stoller S D. Alias analysis for optimization of dynamic languages. In: Proceedings of the 6th Symposium
on Dynamic Languages, Reno/Tahoe, 2010. 27–42
32 Lin W, Chen Z F, Ma W W Y, et al. An empirical study on the characteristics of Python fine-grained source code
change types. In: Proceedings of the 32nd International Conference on Software Maintenance and Evolution, Raleigh,
2016. 188–199
33 Member S. Change distilling: tree differencing for fine-grained source code change extraction. IEEE Trans Softw Eng,
2007, 33: 725–743
34 Neamtiu I, Foster J S, Hicks M. Understanding source code evolution using abstract syntax tree matching. In:
Proceedings of the 2005 International Workshop on Mining Software Repositories, St. Louis, 2005. 1–5
35 Sager T, Bernstein A, Pinzger M, et al. Detecting similar Java classes using tree algorithms. In: Proceedings of the 2006 International Workshop on Mining Software Repositories, Shanghai, 2006. 65–71
36 Apiwattanapong T, Orso A, Harrold M J. A differencing algorithm for object-oriented programs. In: Proceedings of
the 19th IEEE International Conference on Automated Software Engineering, Linz, 2004. 2–13
37 Howitz S. Identifying the semantic and textual differences between two versions of a program. In: Proceedings of
the ACM SIGPLAN 1990 Conference on Programming Language Design and Implementation, White Plains, 1990. 234–245
38 Raghavan S, Rohana R, Leon D, et al. Dex: a semantic-graph differencing tool for studying changes in large code
bases. In: Proceedings of the 20th IEEE International Conference on Software Maintenance, Chicago, 2004. 188–197
39 Kim M, Notkin D. Program element matching for multi-version program analyses. In: Proceedings of the 2006
International Workshop on Mining Software Repositories, Shanghai, 2006. 58–64
40 Purushothaman R, Perry D E. Toward understanding the rhetoric of small source code changes. IEEE Trans Softw
Eng, 2005, 31: 511–526
41 Voinea L, Telea A. CVSscan: visualization of code evolution. In: Proceedings of the 2005 ACM Symposium on Software
Visualization, St. Louis, 2005. 47–56
42 Omori T, Maruyama K. A change-aware development environment by recording editing operations of source code. In:
Proceedings of the 2008 International Working Conference on Mining Software Repositories, Leipzig, 2008. 31–34
43 Rastkar S, Murphy G C. Why did this code change? In: Proceedings of the 2013 International Conference on Software
Engineering, San Francisco, 2013. 1193–1196
44 Canfora G, Cerulo L, Cimitile M, et al. How changes affect software entropy: an empirical study. Empir Softw Eng, 2014, 19: 1–38
45 Kamei Y, Shihab E, Adams B, et al. A large-scale empirical study of just-in-time quality assurance. IEEE Trans Softw
Eng, 2013, 39: 757–773
46 Wen M, Wu R X, Cheung S C. Locus: locating bugs from software changes. In: Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engineering, Singapore, 2016. 262–273
47 Zhao Y Y, Leung H, Yang Y B, et al. Towards an understanding of change types in bug fixing code. Inform Softw
Tech, 2017, 86: 37–53
48 Pan K, Kim S, Whitehead E J Jr. Toward an understanding of bug fix patterns. Empir Softw Eng, 2009, 14: 286–315


>><[N]>explainable just-in-time bug prediction are we there yet
[1] Alicja Gosiewska and Przemyslaw Biecek. ibreakdown: Uncertainty
of model explanations for non-additive predictive models. CoRR,
abs/1903.11420, 2019.
[2] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter
Reutemann, and Ian H. Witten. The weka data mining software: An
update. SIGKDD Explor. Newsl., 11(1):10–18, November 2009.
[3] T. Jiang, L. Tan, and S. Kim. Personalized defect prediction. In
2013 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE), pages 279–289, 2013.
[4] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and J. Grundy. An
empirical study of model-agnostic techniques for defect prediction
models. IEEE Transactions on Software Engineering, pages 1–1, 2020.
[5] S. Kim, E. J. Whitehead,, and Y. Zhang. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering,
34(2):181–196, 2008.
[6] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller. Predicting
faults from cached history. In 29th International Conference on Software
Engineering (ICSE’07), pages 489–498, 2007.
[7] T. Mende and R. Koschke. Effort-aware defect prediction models.
In 2010 14th European Conference on Software Maintenance and
Reengineering, pages 107–116, 2010.
[8] N. Nagappan and T. Ball. Using software dependencies and churn
metrics to predict field failures: An empirical case study. In First International
Symposium on Empirical Software Engineering and Measurement
(ESEM 2007), pages 364–373, 2007.
[9] M. F. Porter. Snowball: A language for stemming algorithms, 2001. [Online].
Available at http://snowball.tartarus.org/texts/introduction.html.
[10] Mateusz Staniak and Przemysław Biecek. Explanations of model predictions
with live and breakdown packages. The R Journal, 10(2):395,
2019.
[11] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online defect prediction
for imbalanced data. In 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering, volume 2, pages 99–108, 2015.
[12] S. Wang, T. Liu, J. Nam, and L. Tan. Deep semantic feature learning for
software defect prediction. IEEE Transactions on Software Engineering,
46(12):1267–1293, 2020.
[13] Thomas Zimmermann, Rahul Premraj, and Andreas Zeller. Predicting
defects for eclipse. In Proceedings of the Third International Workshop
on Predictor Models in Software Engineering, PROMISE ’07, page 9,
USA, 2007. IEEE Computer Society.





>><[N]>just-in-time bug prediction in mobile applications the domain matters
[1] W. Albert and T. Tullis, Measuring the user experience: collecting,
analyzing, and presenting usability metrics. Newnes, 2013.
[2] M. Linares-Vásquez et al., “Domain matters: bringing further evidence of
the relationships among anti-patterns, application domains, and qualityrelated
metrics in java mobile apps,” in ICPC 2014.
[3] H. van Heeringen and E. Van Gorp, “Measure the functional size of a
mobile app: Using the cosmic functional size measurement method,” in
IWSM-MENSURA, 2014.
[4] L. D’Avanzo et al., “Cosmic functional measurement of mobile applications
and code size estimation,” in ACM SAC, 2015.
[5] A. Abran et al., “The cosmic functional size measurement method manual,
version 4.0.1,” Tech. Rep., 2015.
[6] Y. Kamei et al., “A large-scale empirical study of just-in-time quality
assurance,” IEEE TSE.
[7] C. Rosen, B. Grawi, and E. Shihab, “Commit guru: analytics and risk
prediction of software commits,” in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. ACM, 2015, pp.
966–969.
[8] R. Kohavi and G. H. John, “Wrappers for feature subset selection,”
Artificial intelligence.
[9] S. Le Cessie and J. C. Van Houwelingen, “Ridge estimators in logistic
regression,” Applied statistics.
[10] M. D’Ambros et al., “Evaluating defect prediction approaches: A
benchmark and an extensive comparison,” EMSE.
[11] P. A. Devijver and J. Kittler, Pattern recognition: A statistical approach.
[12] Baeza-Yates et al., Modern information retrieval.
[13] A. I. Wasserman, “Software engineering issues for mobile application
development,” in FSE/SDP, 2010.


>><[N]>Poster Bridging Effort-Aware Prediction and Strong Classification - a Just-in-Time Software Defect Prediction Study
[1] A. Alali, H. Kagdi, and J. Maletic. 2008. What’s a typical commit? a characterization
of open source software repositories. In 16th IEEE Intl. Conf. on Program
Comprehension. 182–191.
[2] E. Arisholm, L. Briand, and E. Johannessen. 2010. A systematic and comprehensive
investigation of methods to build and evaluate fault prediction models. J. of
Systems & Software 83, 1 (2010), 2–17.
[3] Y. Kamei, S. Matsumoto, and et al. 2010. Revisiting common bug prediction
findings using effort-aware models. In IEEE Intl. Conf. on Softw. Maint. 1–10.
[4] Y. Kamei, E. Shihab, and et al. 2013. A large-scale empirical study of just-in-time
quality assurance. IEEE Trans. on Softw. Eng. 39, 6 (2013), 757–773.
[5] T. Mende and R. Koschke. 2010. Effort-aware defect prediction models. In 14th
European Conf. on Software Maintenance and Re-engineering. 107–116.
[6] Y. Yang, Y. Zhou, and et al. 2015. Are slice-based cohesion metrics actually useful
in effort-aware post-release fault-proneness prediction? An empirical study. IEEE
Trans. on Softw. Eng. 41, 4 (2015), 331–357.
[7] Y. Yang, Y. Zhou, and et al. 2016. Effort-aware just-in-time defect prediction:
simple unsupervised models could be better than supervised models. In 24th
ACM SIGSOFT International Symp. on Foundations of Softw. Eng. 157–168.


>><[N]>Locus: Locating Bugs from Software Changes
[1] http://google-engtools.blogspot.hk/2011/12/
bug-prediction-at-google.html. Accessed: 2015-03-22.
[2] https://bugs.eclipse.org/bugs/buglist.cgi?
classification=Eclipse&component=Core&list id=
11582065&product=JDT&query format=advanced&
resolution=FIXED&version=4.5. Accessed: 2015-03-22.
[3] https://bugs.eclipse.org/bugs/buglist.cgi?
classification=Eclipse&component=UI&list id=
11582038&product=PDE&query format=advanced&
resolution=FIXED&version=4.4. Accessed: 2015-03-22.
[4] https://bz.apache.org/bugzilla/buglist.cgi?product=
Tomcat%208&query format=advanced&resolution=
FIXED. Accessed: 2015-03-22.
[5] R. Abreu, P. Zoeteweij, and A. J. Van Gemund. On the
accuracy of spectrum-based fault localization. In
TAIC-PART’07, pages 89–98, 2007.
[6] A. Alali, H. Kagdi, J. Maletic, et al. What’s a typical
commit? a characterization of open source software
repositories. In ICPC’08, pages 182–191. IEEE, 2008.
[7] H. A. N. An Ngoc Lam, Anh Tuan Nguyen and T. N.
Nguyen. Combining deep learning with information
retrieval to localize buggy files for bug reports. In
ASE’15, pages 151–160. IEEE, 2015.
[8] S. A. Bohner. Software change impact analysis. 1996.
[9] V. Dallmeier and T. Zimmermann. Extraction of bug
localization benchmarks from history. In ASE’07, pages
433–436. ACM, 2007.
[10] B. Fluri, M. Wursch, M. PInzger, and H. C. Gall.
Change distilling: Tree differencing for fine-grained
source code change extraction. IEEE Transactions on
Software Engineering, 33(11):725–743, 2007.
[11] T. L. Graves, A. F. Karr, J. S. Marron, and H. Siy.
Predicting fault incidence using software change history.
IEEE Transactions on Software Engineering,
26(7):653–661, 2000.
[12] G. Jeong, S. Kim, and T. Zimmermann. Improving bug
triage with bug tossing graphs. In FSE’09, pages
111–120. ACM, 2009.
[13] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan,
A. Mockus, A. Sinha, and N. Ubayashi. A large-scale
empirical study of just-in-time quality assurance. IEEE
Transactions on Software Engineering, 39(6):757–773,
2013.
[14] D. Kawrykow and M. P. Robillard. Non-essential
changes in version histories. In ICSE’11, pages 351–360.
ACM, 2011.
[15] D. Kim, Y. Tao, S. Kim, and A. Zeller. Where should
we fix this bug? a two-phase recommendation model.
IEEE Transactions on Software Engineering,
39(11):1597–1610, 2013.
[16] S. Kim, E. J. Whitehead Jr, and Y. Zhang. Classifying
software changes: Clean or buggy? IEEE Transactions
on Software Engineering, 34(2):181–196, 2008.
[17] S. Kim, T. Zimmermann, K. Pan, and E. J.
Whitehead Jr. Automatic identification of
bug-introducing changes. In ASE’06, pages 81–90.
IEEE, 2006.
[18] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and
A. Zeller. Predicting faults from cached history. In
ICSE’07, pages 489–498. IEEE Computer Society, 2007.
[19] T.-D. B. Le, R. J. Oentaryo, and D. Lo. Information
retrieval and spectrum based bug localization: better
together. In FSE’15, pages 579–590. ACM, 2015.
[20] S. K. Lukins, N. A. Kraft, and L. H. Etzkorn. Bug
localization using latent dirichlet allocation.
Information and Software Technology, 52(9):972–990,
2010.
[21] X. Ma, P. Huang, X. Jin, P. Wang, S. Park, D. Shen,
Y. Zhou, L. K. Saul, and G. M. Voelker. edoctor:
Automatically diagnosing abnormal battery drain
issues on smartphones. In NSDI’13, pages 57–70, 2013.
[22] H. B. Mann and D. R. Whitney. On a test of whether
one of two random variables is stochastically larger
than the other. The annals of mathematical statistics,
pages 50–60, 1947.
[23] C. D. Manning and H. Sch¨utze. Foundations of
statistical natural language processing, volume 999. MIT
Press, 1999.
[24] S. Meng, X. Wang, L. Zhang, and H. Mei. A
history-based matching approach to identification of
framework evolution. In ICSE’12, pages 353–363. IEEE,
2012.
[25] L. Moreno, W. Bandara, S. Haiduc, and A. Marcus. On
the relationship between the vocabulary of bug reports
and source code. In ICSE’13, pages 452–455. IEEE,
2013.
[26] L. Moreno, J. J. Treadway, A. Marcus, and W. Shen.
On the use of stack traces to improve text
retrieval-based bug localization. In ICSME’14, pages
151–160. IEEE, 2014.
[27] R. Moser, W. Pedrycz, and G. Succi. A comparative
analysis of the efficiency of change metrics and static
code attributes for defect prediction. In ICSE’08, pages
181–190. IEEE, 2008.
[28] A. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V.
Nguyen, and T. N. Nguyen. A topic-based approach for
narrowing the search space of buggy files from a bug
report. In ASE’11, pages 263–272. IEEE, 2011.
[29] C. Parnin and A. Orso. Are automated debugging
techniques actually helping programmers? In ISSTA’11,
pages 199–209. ACM, 2011.
[30] F. Rahman, D. Posnett, A. Hindle, E. Barr, and
P. Devanbu. Bugcache for inspections: hit or miss? In
FSE’11, pages 322–331. ACM, 2011.
[31] S. Rao and A. Kak. Retrieval from software libraries for
bug localization: a comparative study of generic and
composite text models. In MSR’11, pages 43–52. ACM,
2011.
[32] X. Ren, F. Shah, F. Tip, B. G. Ryder, and O. Chesley.
Chianti: a tool for change impact analysis of java
programs. In ACM Sigplan Notices, volume 39, pages
432–448. ACM, 2004.
[33] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry.
Improving bug localization using structured
information retrieval. In ASE’2013, pages 345–355.
IEEE, 2013.
[34] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce fixes? ACM sigsoft software engineering
notes, 30(4):1–5, 2005.
[35] E. M. Voorhees et al. The trec-8 question answering
track report. In Trec, volume 99, pages 77–82, 1999.
[36] Q. Wang, C. Parnin, and A. Orso. Evaluating the
usefulness of ir-based fault localization techniques. In
ISSTA’15, pages 1–11. ACM, 2015.
[37] S. Wang and D. Lo. Version history, similar report, and
structure: Putting them together for improved bug
localization. In ICPC’14, pages 53–63. ACM, 2014.
[38] S. Wang, D. Lo, and X. Jiang. Understanding
widespread changes: A taxonomic study. In CSMR’13,
pages 5–14. IEEE, 2013.
[39] C.-P. Wong, Y. Xiong, H. Zhang, D. Hao, L. Zhang,
and H. Mei. Boosting bug-report-oriented fault
localization with segmentation and stack-trace analysis.
In ICSME’14, pages 181–190. IEEE, 2014.
[40] R. Wu, H. Zhang, S.-C. Cheung, and S. Kim.
Crashlocator: locating crashing faults based on crash
stacks. In Proceedings of the 2014 International
Symposium on Software Testing and Analysis, pages
204–214, 2014.
[41] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung. Relink:
recovering links between bugs and changes. In FSE’11,
pages 15–25. ACM, 2011.
[42] X. Ye, R. Bunescu, and C. Liu. Learning to rank
relevant files for bug reports using domain knowledge.
In FSE’14, pages 689–699. ACM, 2014.
[43] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and
L. Bairavasundaram. How do fixes become bugs? In
FSE’11, pages 26–36. ACM, 2011.
[44] A. Zeller and R. Hildebrandt. Simplifying and isolating
failure-inducing input. IEEE Transactions on Software
Engineering, 28(2):183–200, 2002.
[45] L. Zhang, M. Kim, and S. Khurshid. Localizing
failure-inducing program edits based on spectrum
information. In ICSM’11, pages 23–32. IEEE, 2011.
[46] J. Zhou, H. Zhang, and D. Lo. Where should the bugs
be fixed? more accurate information retrieval-based
bug localization based on bug reports. In ICSE’12,
pages 14–24. IEEE, 2012.
