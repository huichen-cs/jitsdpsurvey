@inproceedings{10.5555/2667062.2667074,
author = {Lipaczewski, Michael and Struck, Simon and Ortmeier, Frank},
title = {SAML Goes Eclipse: Combining Model-Based Safety Analysis and High-Level Editor Support},
year = {2012},
isbn = {9781467318204},
publisher = {IEEE Press},
abstract = {Software-intensive systems become more and more important in safety critical applications, mainly because of the rising number and complexity of embedded system. Many traditional safety analysis techniques where developed decades ago and thus cannot cope with the complexity of modern systems. Model based analysis techniques where developed to deal with the complexity of software-intensive systems. However, due to the lack of tool support these techniques are currently limited to highly skilled experts. Thus model-based analysis is rarely used by system engineers.Based on the safety analysis modeling language (SAML) framework we propose the S3E, which integrates a complete safety analysis environment into the eclipse platform. S3E covers the whole safety analysis work flow. This implies a powerful editor for model creation, a seamless integration of model-analysis tools and presentation as well as evaluation of the analysis results into one environment. In this paper we present the current state of S3E and first experiences with the eclipse plug-in development.},
booktitle = {Proceedings of the Second International Workshop on Developing Tools as Plug-Ins},
pages = {67–72},
numpages = {6},
location = {Zurich, Switzerland},
series = {TOPI '12}
}

@article{10.1145/1498915.1498921,
author = {Kulathumani, Vinodkrishnan and Arora, Anish and Sridharan, Mukundan and Demirbas, Murat},
title = {Trail: A Distance-Sensitive Sensor Network Service for Distributed Object Tracking},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/1498915.1498921},
doi = {10.1145/1498915.1498921},
abstract = {Distributed observation and control of mobile objects via static wireless sensors demands timely information in a distance-sensitive manner: Information about closer objects is required more often and more quickly than that of farther objects. In this article, we present a wireless sensor network protocol, Trail, that supports distance-sensitive tracking of mobile objects for in-network subscribers upon demand. Trail achieves a find time that is linear in the distance from a subscriber to an object, via a distributed data structure that is updated only locally when the object moves. Notably, Trail does not partition the network into a hierarchy of clusters and clusterheads, and as a result Trail has lower maintenance costs, is more locally fault tolerant, and it better utilizes the network in terms of load balancing and minimizing the size of the data structure needed for tracking. Moreover, Trail is reliable and energy efficient, despite the network dynamics that are typical of wireless sensor networks. Trail can be refined by tuning certain parameters, thereby yielding a family of protocols that are suited for different application settings such as rate of queries, rate of updates, and network size. We evaluate the performance of Trail by analysis, simulations in a 90 \texttimes{} 90 sensor network, and experiments on 105 Mica2 nodes in the context of a pursuer-evader control application.},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
articleno = {15},
numpages = {40},
keywords = {network protocol, applications of sensor actuator networks, data storage and query, Distributed tracking, energy efficiency, scalability, fault tolerance}
}

@inproceedings{10.1145/2371536.2371572,
author = {Dean, Daniel Joseph and Nguyen, Hiep and Gu, Xiaohui},
title = {UBL: Unsupervised Behavior Learning for Predicting Performance Anomalies in Virtualized Cloud Systems},
year = {2012},
isbn = {9781450315203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371536.2371572},
doi = {10.1145/2371536.2371572},
abstract = {Infrastructure-as-a-Service (IaaS) clouds are prone to performance anomalies due to their complex nature. Although previous work has shown the effectiveness of using statistical learning to detect performance anomalies, existing schemes often assume labelled training data, which requires significant human effort and can only handle previously known anomalies. We present an Unsupervised Behavior Learning (UBL) system for IaaS cloud computing infrastructures. UBL leverages Self-Organizing Maps to capture emergent system behaviors and predict unknown anomalies. For scalability, UBL uses residual resources in the cloud infrastructure for behavior learning and anomaly prediction with little add-on cost. We have implemented a prototype of the UBL system on top of the Xen platform and conducted extensive experiments using a range of distributed systems. Our results show that UBL can predict performance anomalies with high accuracy and achieve sufficient lead time for automatic anomaly prevention. UBL supports large-scale infrastructure-wide behavior learning with negligible overhead.},
booktitle = {Proceedings of the 9th International Conference on Autonomic Computing},
pages = {191–200},
numpages = {10},
keywords = {anomaly prediction, unsupervised system behavior learning, cloud computing},
location = {San Jose, California, USA},
series = {ICAC '12}
}

@article{10.1145/2801154,
author = {Thomas, Anna and Pattabiraman, Karthik},
title = {Error Detector Placement for Soft Computing Applications},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/2801154},
doi = {10.1145/2801154},
abstract = {The scaling of Silicon devices has exacerbated the unreliability of modern computer systems, and power constraints have necessitated the involvement of software in hardware error detection. At the same time, emerging workloads in the form of soft computing applications (e.g., multimedia applications) can tolerate most hardware errors as long as the erroneous outputs do not deviate significantly from error-free outcomes. We term outcomes that deviate significantly from the error-free outcomes as Egregious Data Corruptions (EDCs).In this study, we propose a technique to place detectors for selectively detecting EDC-causing errors in an application. We performed an initial study to formulate heuristics that identify EDC-causing data. Based on these heuristics, we developed an algorithm that identifies program locations for placing high coverage detectors for EDCs using static analysis. Our technique achieves an average EDC coverage of 82%, under performance overheads of 10%, while detecting 10% of the Non-EDC and benign faults. We also evaluate the error resilience of these applications under the 14 compiler optimizations.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {8},
numpages = {25},
keywords = {Hardware fault detection, EDCs, static analysis, detector placement}
}

@article{10.1145/3460433,
author = {Sha, Zhibing and Li, Jun and Song, Lihao and Tang, Jiewen and Huang, Min and Cai, Zhigang and Qian, Lianju and Liao, Jianwei and Liu, Zhiming},
title = {Low I/O Intensity-Aware Partial GC Scheduling to Reduce Long-Tail Latency in SSDs},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3460433},
doi = {10.1145/3460433},
abstract = {This article proposes a low I/O intensity-aware scheduling scheme on garbage collection (GC) in SSDs for minimizing the I/O long-tail latency to ensure I/O responsiveness. The basic idea is to assemble partial GC operations by referring to several determinable factors (e.g., I/O characteristics) and dispatch them to be processed together in idle time slots of I/O processing. To this end, it first makes use of Fourier transform to explore the time slots having relative sparse I/O requests for conducting time-consuming GC operations, as the number of affected I/O requests can be limited. After that, it constructs a mathematical model to further figure out the types and quantities of partial GC operations, which are supposed to be dealt with in the explored idle time slots, by taking the factors of I/O intensity, read/write ratio, and the SSD use state into consideration. Through a series of simulation experiments based on several realistic disk traces, we illustrate that the proposed GC scheduling mechanism can noticeably reduce the long-tail latency by between 5.5% and 232.3% at the 99.99th percentile, in contrast to state-of-the-art methods.},
journal = {ACM Trans. Archit. Code Optim.},
month = {aug},
articleno = {46},
numpages = {25},
keywords = {I/O characteristics, I/O responsiveness, SSDs, garbage collection (GC), partial GC operations}
}

@inproceedings{10.1145/504282.504289,
author = {Ogasawara, Takeshi and Komatsu, Hideaki and Nakatani, Toshio},
title = {A Study of Exception Handling and Its Dynamic Optimization in Java},
year = {2001},
isbn = {1581133359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/504282.504289},
doi = {10.1145/504282.504289},
abstract = {Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs iin various categories of Java programs. There are two commonly used exception handling techniques, stack unwinding optimizes the normal path, while stack cutting optimizes the exception handling path. However, there has been no single exception handling technique to optimize both paths.},
booktitle = {Proceedings of the 16th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {83–95},
numpages = {13},
location = {Tampa Bay, FL, USA},
series = {OOPSLA '01}
}

@article{10.1145/504311.504289,
author = {Ogasawara, Takeshi and Komatsu, Hideaki and Nakatani, Toshio},
title = {A Study of Exception Handling and Its Dynamic Optimization in Java},
year = {2001},
issue_date = {11/01/2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/504311.504289},
doi = {10.1145/504311.504289},
abstract = {Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs iin various categories of Java programs. There are two commonly used exception handling techniques, stack unwinding optimizes the normal path, while stack cutting optimizes the exception handling path. However, there has been no single exception handling technique to optimize both paths.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {83–95},
numpages = {13}
}

@article{10.1145/3431803,
author = {Liu, Wenjie and Akram, Shoaib and Sartor, Jennifer B. and Eeckhout, Lieven},
title = {Reliability-Aware Garbage Collection for Hybrid HBM-DRAM Memories},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3431803},
doi = {10.1145/3431803},
abstract = {Emerging workloads in cloud and data center infrastructures demand high main memory bandwidth and capacity. Unfortunately, DRAM alone is unable to satisfy contemporary main memory demands. High-bandwidth memory (HBM) uses 3D die-stacking to deliver 4–8\texttimes{} higher bandwidth. HBM has two drawbacks: (1) capacity is low, and (2) soft error rate is high. Hybrid memory combines DRAM and HBM to promise low fault rates, high bandwidth, and high capacity. Prior OS approaches manage HBM by mapping pages to HBM versus DRAM based on hotness (access frequency) and risk (susceptibility to soft errors). Unfortunately, these approaches operate at a coarse-grained page granularity, and frequent page migrations hurt performance.This article proposes a new class of reliability-aware garbage collectors for hybrid HBM-DRAM systems that place hot and low-risk objects in HBM and the rest in DRAM. Our analysis of nine real-world Java workloads shows that: (1) newly allocated objects in the nursery are frequently written, making them both hot and low-risk, (2) a small fraction of the mature objects are hot and low-risk, and (3) allocation site is a good predictor for hotness and risk. We propose RiskRelief, a novel reliability-aware garbage collector that uses allocation site prediction to place hot and low-risk objects in HBM. Allocation sites are profiled offline and RiskRelief uses heuristics to classify allocation sites as DRAM and HBM. The proposed heuristics expose Pareto-optimal trade-offs between soft error rate (SER) and execution time. RiskRelief improves SER by 9\texttimes{} compared to an HBM-Only system while at the same time improving performance by 29% compared to a DRAM-Only system. Compared to a state-of-the-art OS approach for reliability-aware data placement, RiskRelief eliminates all page migration overheads, which substantially improves performance while delivering similar SER. Reliability-aware garbage collection opens up a new opportunity to manage emerging HBM-DRAM memories at fine granularity while requiring no extra hardware support and leaving the programming model unchanged.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jan},
articleno = {10},
numpages = {25},
keywords = {Soft-error reliability, garbage collection, hybrid memories, high-bandwidth memory}
}

@inproceedings{10.5555/977395.977673,
author = {Lattner, Chris and Adve, Vikram},
title = {LLVM: A Compilation Framework for Lifelong Program Analysis &amp; Transformation},
year = {2004},
isbn = {0769521029},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes LLVM (Low Level Virtual Machine),a compiler framework designed to support transparent, lifelongprogram analysis and transformation for arbitrary programs,by providing high-level information to compilertransformations at compile-time, link-time, run-time, and inidle time between runs.LLVM defines a common, low-levelcode representation in Static Single Assignment (SSA) form,with several novel features: a simple, language-independenttype-system that exposes the primitives commonly used toimplement high-level language features; an instruction fortyped address arithmetic; and a simple mechanism that canbe used to implement the exception handling features ofhigh-level languages (and setjmp/longjmp in C) uniformlyand efficiently.The LLVM compiler framework and coderepresentation together provide a combination of key capabilitiesthat are important for practical, lifelong analysis andtransformation of programs.To our knowledge, no existingcompilation approach provides all these capabilities.We describethe design of the LLVM representation and compilerframework, and evaluate the design in three ways: (a) thesize and effectiveness of the representation, including thetype information it provides; (b) compiler performance forseveral interprocedural problems; and (c) illustrative examplesof the benefits LLVM provides for several challengingcompiler problems.},
booktitle = {Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization},
pages = {75},
location = {Palo Alto, California},
series = {CGO '04}
}

@inproceedings{10.1145/1356058.1356078,
author = {Murphy, Brian R. and Menon, Vijay and Schneider, Florian T. and Shpeisman, Tatiana and Adl-Tabatabai, Ali-Reza},
title = {Fault-Safe Code Motion for Type-Safe Languages},
year = {2008},
isbn = {9781595939784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1356058.1356078},
doi = {10.1145/1356058.1356078},
abstract = {Compilers for Java and other type-safe languages have historically worked to overcome overheads and constraints imposed by runtime safety checks and precise exception semantics. We instead exploit these safety properties to perform code motion optimizations that are even more aggressive than those possible in unsafe languages such as C++.We present a novel framework for speculative motion of dangerous (potentially faulting) instructions in safe, object-oriented languages such as Java and C#. Unlike earlier work, our approach requires no hardware or operating system support. We leverage the properties already provided by a safe language to define fault safety, a more precise notion of safety that guarantees that a dangerous operation (e.g., a memory load) will not fault at a given program point.We illustrate how typical code motion optimizations are easily adapted to exploit our safety framework. First, we modify the standard SSAPRE partial redundancy elimination (PRE) algorithm to use fault safety, rather than the traditional down safety property. Our modified algorithm better exploits profile information by inserting of dangerous instructions on new paths when it is profitable and provably safe. Second, we extend an instruction trace scheduler to use fault safety to safely schedule load instructions across branches to better tolerate memory latency and to more compactly target instruction slots.We implemented these optimizations in StarJIT, a dynamic compiler, and show performance benefits of up to 10% on a set of standard Java benchmarks.},
booktitle = {Proceedings of the 6th Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {144–154},
numpages = {11},
keywords = {safe code motion, speculative code motion, intermediate representations, safety dependences, partial redundancy elimination, code motion, scheduling},
location = {Boston, MA, USA},
series = {CGO '08}
}

@article{10.1145/2807593,
author = {Baudry, Benoit and Monperrus, Martin},
title = {The Multiple Facets of Software Diversity: Recent Developments in Year 2000 and Beyond},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2807593},
doi = {10.1145/2807593},
abstract = {Early experiments with software diversity in the mid 1970s investigated N-version programming and recovery blocks to increase the reliability of embedded systems. Four decades later, the literature about software diversity has expanded in multiple directions: goals (fault tolerance, security, software engineering), means (managed or automated diversity), and analytical studies (quantification of diversity and its impact). Our article contributes to the field of software diversity as the first work that adopts an inclusive vision of the area, with an emphasis on the most recent advances in the field. This survey includes classical work about design and data diversity for fault tolerance, as well as the cybersecurity literature that investigates randomization at different system levels. It broadens this standard scope of diversity to include the study and exploitation of natural diversity and the management of diverse software products. Our survey includes the most recent works, with an emphasis from 2000 to the present. The targeted audience is researchers and practitioners in one of the surveyed fields who miss the big picture of software diversity. Assembling the multiple facets of this fascinating topic sheds a new light on the field.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {16},
numpages = {26},
keywords = {program transformation, design principles, Software diversity}
}

@inproceedings{10.1145/379539.379578,
author = {Veldema, R. and Hofman, R. F. H. and Bhoedjang, R. A. F. and Jacobs, C. J. H. and Bal, H. E.},
title = {Source-Level Global Optimizations for Fine-Grain Distributed Shared Memory Systems},
year = {2001},
isbn = {1581133464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/379539.379578},
doi = {10.1145/379539.379578},
abstract = {This paper describes and evaluates the use of aggressive static analysis in Jackal, a fine-grain Distributed Shared Memory (DSM) system for Java. Jackal uses an optimizing, source-level compiler rather than the binary rewriting techniques employed by most other fine-grain DSM systems. Source-level analysis makes existing access-check optimizations (e.g., access-check batching) more effective and enables two novel fine-grain DSM optimizations: object-graph aggregation and automatic computation migration.The compiler detects situations where an access to a root object is followed by accesses to subobjects. Jackal attempts to aggregate all access checks on objects in such object graphs into a single check on the graph's root object. If this check fails, the entire graph is fetched. Object-graph aggregation can reduce the number of network roundtrips and, since it is an advanced form of access-check batching, improves sequential performance.Computation migration (or function shipping) is used to optimize critical sections in which a single processor owns both the shared data that is accessed and the lock that protects the data. It is usually more efficient to execute such critical sections on the processor that holds the lock and the data than to incur multiple roundtrips for acquiring the lock, fetching the data, writing the data back, and releasing the lock. Jackal's compiler detects such critical sections and optimizes them by generating single-roundtrip computation-migration code rather than standard data-shipping code.Jackal's optimizations improve both sequential and parallel application performance. On average, sequential execution times of instrumented, optimized programs are within 10% of those of uninstrumented programs. Application speedups usually improve significantly and several Jackal applications perform as well as hand-optimized message-passing programs.},
booktitle = {Proceedings of the Eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming},
pages = {83–92},
numpages = {10},
location = {Snowbird, Utah, USA},
series = {PPoPP '01}
}

@article{10.1145/568014.379578,
author = {Veldema, R. and Hofman, R. F. H. and Bhoedjang, R. A. F. and Jacobs, C. J. H. and Bal, H. E.},
title = {Source-Level Global Optimizations for Fine-Grain Distributed Shared Memory Systems},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/568014.379578},
doi = {10.1145/568014.379578},
abstract = {This paper describes and evaluates the use of aggressive static analysis in Jackal, a fine-grain Distributed Shared Memory (DSM) system for Java. Jackal uses an optimizing, source-level compiler rather than the binary rewriting techniques employed by most other fine-grain DSM systems. Source-level analysis makes existing access-check optimizations (e.g., access-check batching) more effective and enables two novel fine-grain DSM optimizations: object-graph aggregation and automatic computation migration.The compiler detects situations where an access to a root object is followed by accesses to subobjects. Jackal attempts to aggregate all access checks on objects in such object graphs into a single check on the graph's root object. If this check fails, the entire graph is fetched. Object-graph aggregation can reduce the number of network roundtrips and, since it is an advanced form of access-check batching, improves sequential performance.Computation migration (or function shipping) is used to optimize critical sections in which a single processor owns both the shared data that is accessed and the lock that protects the data. It is usually more efficient to execute such critical sections on the processor that holds the lock and the data than to incur multiple roundtrips for acquiring the lock, fetching the data, writing the data back, and releasing the lock. Jackal's compiler detects such critical sections and optimizes them by generating single-roundtrip computation-migration code rather than standard data-shipping code.Jackal's optimizations improve both sequential and parallel application performance. On average, sequential execution times of instrumented, optimized programs are within 10% of those of uninstrumented programs. Application speedups usually improve significantly and several Jackal applications perform as well as hand-optimized message-passing programs.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {83–92},
numpages = {10}
}

@inproceedings{10.1109/CCGRID.2017.125,
author = {Reiter, Andreas and Pr\"{u}nster, Bernd and Zefferer, Thomas},
title = {Hybrid Mobile Edge Computing: Unleashing the Full Potential of Edge Computing in Mobile Device Use Cases},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.125},
doi = {10.1109/CCGRID.2017.125},
abstract = {Many different technologies fostering and supporting distributed and decentralized computing scenarios emerged recently. Edge computing provides the necessary on-demand computing power for Internet-of-Things (IoT) devices where it is needed. Computing power is moved closer to the consumer, with the effect of reducing latency and increasing fail-safety due to absent centralized structures. This is an enabler for applications requiring high-bandwidth uplinks and low latencies to computing units. In this paper, a new use case for edge computing is identified. Mobile devices can overcome their battery limitations and performance constraints by dynamically using the edge-computing-provided computational power. We call this new technology Hybrid Mobile Edge Computing. We present a general architecture and framework, which targets the mobile device use case of hybrid mobile edge computing, not only considering the improvement of performance and energy consumption, but also providing means to protect user privacy, sensitive data and computations. The achieved results are backed by the results of our analysis, targeting the energy saving potentials and possible performance improvements.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {935–944},
numpages = {10},
keywords = {XACML, Fog Computing, Mobile Applications, Edge Computing, Mobile Cloud Computing},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3330345.3330388,
author = {Fang, Bo and Halawa, Hassan and Pattabiraman, Karthik and Ripeanu, Matei and Krishnamoorthy, Sriram},
title = {BonVoision: Leveraging Spatial Data Smoothness for Recovery from Memory Soft Errors},
year = {2019},
isbn = {9781450360791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330345.3330388},
doi = {10.1145/3330345.3330388},
abstract = {Detectable but Uncorrectable Errors (DUEs) in the memory subsystem are becoming increasingly frequent. Today, upon encountering a DUE, applications crash, and the recovery methods used incur significant performance, storage, and energy overheads. To mitigate the impact of these errors, we start from two high-level observations that apply to some classes of HPC applications (e.g., stencil computations on regular grids or irregular meshes): first, these applications, display a property we dub spatial data smoothness: i.e., data items that are nearby in the application's logical space are relatively similar. Second, since these data items are generally used together, programmers go to great lengths to place them in nearby memory locations to improve application's performance by improving access locality. Based on these observations we explore the feasibility of a roll-forward recovery scheme that leverages spatial data smoothness to repair the memory location corrupted by a DUE and continues the application execution. We present BonVoision, a run-time system that intercepts DUE events, analyzes the application binary at runtime to identify the data elements in the neighborhood of the memory location that generates a DUE, and uses them to fix the corrupted data. Our evaluation demonstrates that BonVoision is: (i) efficient - it incurs negligible overhead, (ii) effective - it is frequently successful in continuing the application with benign outcomes, and (iii) user friendly - as it does not require programmer input to expose the data layout or access to source code. We demonstrate that using BonVoision can lead to significant savings in the context of a checkpointing/restart schemes by enabling longer checkpoint intervals.},
booktitle = {Proceedings of the ACM International Conference on Supercomputing},
pages = {484–496},
numpages = {13},
location = {Phoenix, Arizona},
series = {ICS '19}
}

@article{10.1145/3360609,
author = {Surbatovich, Milijana and Jia, Limin and Lucia, Brandon},
title = {I/O Dependent Idempotence Bugs in Intermittent Systems},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360609},
doi = {10.1145/3360609},
abstract = {Intermittently-powered, energy-harvesting devices operate on energy collected from their environment and must operate intermittently as energy is available. Runtime systems for such devices often rely on checkpoints or redo-logs to save execution state between power cycles, causing arbitrary code regions to re-execute on reboot. Any non-idempotent program behavior—behavior that can change on each execution—can lead to incorrect results. This work investigates non-idempotent behavior caused by repeating I/O operations, not addressed by prior work. If such operations affect a control statement or address of a memory update, they can cause programs to take different paths or write to different memory locations on re-executions, resulting in inconsistent memory states. We provide the first characterization of input-dependent idempotence bugs and develop IBIS-S, a program analysis tool for detecting such bugs at compile time, and IBIS-D, a dynamic information flow tracker to detect bugs at runtime. These tools use taint propagation to determine the reach of input. IBIS-S searches for code patterns leading to inconsistent memory updates, while IBIS-D detects concrete memory inconsistencies. We evaluate IBIS on embedded system drivers and applications. IBIS can detect I/O-dependent idempotence bugs, giving few (IBIS-S) or no (IBIS-D) false positives and providing actionable bug reports. These bugs are common in sensor-driven applications and are not fixed by existing intermittent systems.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {183},
numpages = {31},
keywords = {intermittent computing, energy harvesting}
}

@inproceedings{10.1145/1134707.1134728,
author = {Jurca, Radu and Faltings, Boi},
title = {Minimum Payments That Reward Honest Reputation Feedback},
year = {2006},
isbn = {1595932364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134707.1134728},
doi = {10.1145/1134707.1134728},
abstract = {Online reputation mechanisms need honest feedback to function effectively. Self interested agents report the truth only when explicit rewards offset the cost of reporting and the potential gains that can be obtained from lying. Side-payment schemes (monetary rewards for submitted feedback) can make truth-telling rational based on the correlation between the reports of different buyers.In this paper we use the idea of automated mechanism design to construct the payments that minimize the budget required by an incentive-compatible reputation mechanism. Such payment schemes are defined by a linear optimization problem that can be solved efficiently in realistic settings. Furthermore, we investigate two directions for further lowering the cost of incentive-compatibility: using several reference reports to construct the side-payments, and filtering out reports that are probably false.},
booktitle = {Proceedings of the 7th ACM Conference on Electronic Commerce},
pages = {190–199},
numpages = {10},
keywords = {reputation mechanisms, mechanism design, honest feedback},
location = {Ann Arbor, Michigan, USA},
series = {EC '06}
}

@inproceedings{10.1145/3404397.3404435,
author = {Zou, Pengfei and Li, Ang and Barker, Kevin and Ge, Rong},
title = {Detecting Anomalous Computation with RNNs on GPU-Accelerated HPC Machines},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404435},
doi = {10.1145/3404397.3404435},
abstract = {This paper presents a workload classification framework that accurately discriminates illicit computation from authorized workloads on GPU-accelerated HPC systems at runtime. As such systems become increasingly powerful and widely-adopted, attackers have begun to run illicit and for-profit programs that typically require extremely high computing capability to be successful, depriving mission-critical and authorized workloads of execution cycles and increasing risks of data leaking and empowered attacks. Traditional measures on CPU hosts are oblivious to such attacks. Our classification framework leverages the distinctive signatures between illicit and authorized GPU workloads, and explores machine learning methods and workload profiling to classify them. We face multiple challenges in designing the framework: achieving high detection accuracy, maintaining low profiling and inference overhead, and overcoming the limitation of lacking data types and volumes typically required by deep learning models. To address these challenges, we use lightweight, non-intrusive, high-level workload profiling, collect multiple sequences of easily obtainable multimodal input data, and build recurrent neural networks (RNNs) to learn from history for online anomalous workload detection. Evaluation results on three generations of GPU machines demonstrate that the workload classification framework can tell apart the illicit workloads with a high accuracy of over 95%. The collected dataset, detection framework, and neural network models are released on github1.},
booktitle = {49th International Conference on Parallel Processing - ICPP},
articleno = {52},
numpages = {11},
keywords = {GPU accelerated systems, workload classification., HPC security},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@article{10.1145/3440754,
author = {Meurisch, Christian and M\"{u}hlh\"{a}user, Max},
title = {Data Protection in AI Services: A Survey},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3440754},
doi = {10.1145/3440754},
abstract = {Advances in artificial intelligence (AI) have shaped today’s user services, enabling enhanced personalization and better support. As such AI-based services inevitably require user data, the resulting privacy implications are de facto the unacceptable face of this technology. In this article, we categorize and survey the cutting-edge research on privacy and data protection in the context of personalized AI services. We further review the different protection approaches at three different levels, namely, the management, system, and AI levels—showing that (i)&nbsp;not all of them meet our identified requirements of evolving AI services and that (ii)&nbsp;many challenges are addressed separately or fragmentarily by different research communities. Finally, we highlight open research challenges and future directions in data protection research, especially that comprehensive protection requires more interdisciplinary research and a combination of approaches at different levels.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {40},
numpages = {38},
keywords = {personalization, data protection, data decentralization, AI services, privacy}
}

@article{10.1145/3358186,
author = {Kim, Minsu and Park, Jeong-Keun and Kim, Sungyeol and Yang, Insu and Jung, Hyunsoo and Moon, Soo-Mook},
title = {Output-Based Intermediate Representation for Translation of Test-Pattern Program},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358186},
doi = {10.1145/3358186},
abstract = {An Intermediate Representation (IR) used by compilers is normally generated statically, as a result of parsing or analyzing the source program. This paper proposes a completely different type of IR, generated as a result of running the source program, the output-based IR. There is a practical translation problem where such an IR is useful, in the domain of test-pattern programs.Test-pattern programs run on ATE (automatic test equipment), a special embedded system to test semiconductors such as DRAMs. They generate a pattern for each clock, a bit vector input to the pins of the chip. One issue is that different ATEs require different programming since each ATE manufacturer has its own programming language. Nonetheless, we should be able to test a memory chip on different ATEs as long as they generate the same patterns with the same speed. Therefore, a memory chipmaker wants to make a pattern program portable across ATEs, to fully utilize their ATE resources.One solution is translating between pattern programs, for which we need an IR since there are multiple source ATEs and target ATEs. Instead of a conventional, static IR, we propose using the output pattern itself as an IR. Since the pattern is independent of ATEs and easily obtainable, the output-based IR obviates designing a static IR considering all ATE programming languages and hardware differences. Moreover, we might synthesize a better target program from the IR, more optimized to the target ATE. However, the full pattern generated by a product-level pattern program is huge, so we propose using an IR of abbreviated patterns, annotated with the repetition information obtained while executing the source program. Our experimental results with product-level pattern programs show that our approach is feasible.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {55},
numpages = {22},
keywords = {translation, automatic test equipment, Intermediate representation, test-pattern program, domain-specific language}
}

@inproceedings{10.1145/2145816.2145845,
author = {Du, Peng and Bouteiller, Aurelien and Bosilca, George and Herault, Thomas and Dongarra, Jack},
title = {Algorithm-Based Fault Tolerance for Dense Matrix Factorizations},
year = {2012},
isbn = {9781450311601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145816.2145845},
doi = {10.1145/2145816.2145845},
abstract = {Dense matrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This paper proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable component and the possibility of loosing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minor modifications. Theoretical analysis shows that the fault tolerance overhead sharply decreases with the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors.},
booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {225–234},
numpages = {10},
keywords = {fault-tolerance, QR, fail-stop failure, LU, ABFT},
location = {New Orleans, Louisiana, USA},
series = {PPoPP '12}
}

@article{10.1145/2370036.2145845,
author = {Du, Peng and Bouteiller, Aurelien and Bosilca, George and Herault, Thomas and Dongarra, Jack},
title = {Algorithm-Based Fault Tolerance for Dense Matrix Factorizations},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2370036.2145845},
doi = {10.1145/2370036.2145845},
abstract = {Dense matrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This paper proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable component and the possibility of loosing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minor modifications. Theoretical analysis shows that the fault tolerance overhead sharply decreases with the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors.},
journal = {SIGPLAN Not.},
month = {feb},
pages = {225–234},
numpages = {10},
keywords = {fail-stop failure, ABFT, LU, fault-tolerance, QR}
}

@inproceedings{10.1145/1352592.1352610,
author = {Bessani, Alysson Neves and Alchieri, Eduardo Pelison and Correia, Miguel and Fraga, Joni Silva},
title = {DepSpace: A Byzantine Fault-Tolerant Coordination Service},
year = {2008},
isbn = {9781605580135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352592.1352610},
doi = {10.1145/1352592.1352610},
abstract = {The tuple space coordination model is one of the most interesting coordination models for open distributed systems due to its space and time decoupling and its synchronization power. Several works have tried to improve the dependability of tuple spaces through the use of replication for fault tolerance and access control for security. However, many practical applications in the Internet require both fault tolerance and security. This paper describes the design and implementation of DepSpace, a Byzantine fault-tolerant coordination service that provides a tuple space abstraction. The service offered by DepSpace is secure, reliable and available as long as less than a third of service replicas are faulty. Moreover, the content-addressable confidentiality scheme developed for DepSpace bridges the gap between Byzantine fault-tolerant replication and confidentiality of replicated data and can be used in other systems that store critical data.},
booktitle = {Proceedings of the 3rd ACM SIGOPS/EuroSys European Conference on Computer Systems 2008},
pages = {163–176},
numpages = {14},
keywords = {tuple space, confidentiality, byzantine fault tolerance},
location = {Glasgow, Scotland UK},
series = {Eurosys '08}
}

@article{10.1145/1357010.1352610,
author = {Bessani, Alysson Neves and Alchieri, Eduardo Pelison and Correia, Miguel and Fraga, Joni Silva},
title = {DepSpace: A Byzantine Fault-Tolerant Coordination Service},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1357010.1352610},
doi = {10.1145/1357010.1352610},
abstract = {The tuple space coordination model is one of the most interesting coordination models for open distributed systems due to its space and time decoupling and its synchronization power. Several works have tried to improve the dependability of tuple spaces through the use of replication for fault tolerance and access control for security. However, many practical applications in the Internet require both fault tolerance and security. This paper describes the design and implementation of DepSpace, a Byzantine fault-tolerant coordination service that provides a tuple space abstraction. The service offered by DepSpace is secure, reliable and available as long as less than a third of service replicas are faulty. Moreover, the content-addressable confidentiality scheme developed for DepSpace bridges the gap between Byzantine fault-tolerant replication and confidentiality of replicated data and can be used in other systems that store critical data.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {163–176},
numpages = {14},
keywords = {confidentiality, tuple space, byzantine fault tolerance}
}

@inproceedings{10.1145/1519065.1519067,
author = {Lagar-Cavilla, Horacio Andr\'{e}s and Whitney, Joseph Andrew and Scannell, Adin Matthew and Patchin, Philip and Rumble, Stephen M. and de Lara, Eyal and Brudno, Michael and Satyanarayanan, Mahadev},
title = {SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing},
year = {2009},
isbn = {9781605584829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1519065.1519067},
doi = {10.1145/1519065.1519067},
abstract = {Virtual Machine (VM) fork is a new cloud computing abstraction that instantaneously clones a VM into multiple replicas running on different hosts. All replicas share the same initial state, matching the intuitive semantics of stateful worker creation. VM fork thus enables the straightforward creation and efficient deployment of many tasks demanding swift instantiation of stateful workers in a cloud environment, e.g. excess load handling, opportunistic job placement, or parallel computing. Lack of instantaneous stateful cloning forces users of cloud computing into ad hoc practices to manage application state and cycle provisioning. We present SnowFlock, our implementation of the VM fork abstraction. To evaluate SnowFlock, we focus on the demanding scenario of services requiring on-the-fly creation of hundreds of parallel workers in order to solve computationally-intensive queries in seconds. These services are prominent in fields such as bioinformatics, finance, and rendering. SnowFlock provides sub-second VM cloning, scales to hundreds of workers, consumes few cloud I/O resources, and has negligible runtime overhead.},
booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
pages = {1–12},
numpages = {12},
keywords = {cloud computing, virtualization},
location = {Nuremberg, Germany},
series = {EuroSys '09}
}

@article{10.1145/1063786.1063787,
author = {Sivathanu, Muthian and Prabhakaran, Vijayan and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
title = {Improving Storage System Availability with D-GRAID},
year = {2005},
issue_date = {May 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/1063786.1063787},
doi = {10.1145/1063786.1063787},
abstract = {We present the design, implementation, and evaluation of D-GRAID, a gracefully degrading and quickly recovering RAID storage array. D-GRAID ensures that most files within the file system remain available even when an unexpectedly high number of faults occur. D-GRAID achieves high availability through aggressive replication of semantically critical data, and fault-isolated placement of logically related data. D-GRAID also recovers from failures quickly, restoring only live file system data to a hot spare. Both graceful degradation and live-block recovery are implemented in a prototype SCSI-based storage system underneath unmodified file systems, demonstrating that powerful “file-system like” functionality can be implemented within a “semantically smart” disk system behind a narrow block-based interface.},
journal = {ACM Trans. Storage},
month = {may},
pages = {133–170},
numpages = {38},
keywords = {RAID, file systems, Block-based storage, Disk array, fault isolation, smart disks}
}

@article{10.1145/1368506.1368526,
author = {Bressoud, Thomas C.},
title = {Session Scribe Notes for Twenty-First ACM Symposium on Operating Systems Principles},
year = {2008},
issue_date = {April 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/1368506.1368526},
doi = {10.1145/1368506.1368526},
abstract = {The following article is divided into nine sections, one for each of the sessions presented at SOSP 2007. For each session, two student volunteers took notes at the conference, capturing the questions and answers following each of the papers presented in that session. Note that the session order follows the program as given at the conference which, due to logistic necessity, differs slightly from the order in the proceedings.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {136–151},
numpages = {16}
}

@inproceedings{10.1109/ICSE-SEET.2019.00022,
author = {Liu, Xiao and Wang, Shuai and Wang, Pei and Wu, Dinghao},
title = {Automatic Grading of Programming Assignments: An Approach Based on Formal Semantics},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET.2019.00022},
doi = {10.1109/ICSE-SEET.2019.00022},
abstract = {Programming assignment grading can be time-consuming and error-prone if done manually. Existing tools generate feedback with failing test cases. However, this method is inefficient and the results are incomplete. In this paper, we present AutoGrader, a tool that automatically determines the correctness of programming assignments and provides counterexamples given a single reference implementation of the problem. Instead of counting the passed tests, our tool searches for semantically different execution paths between a student's submission and the reference implementation. If such a difference is found, the submission is deemed incorrect; otherwise, it is judged to be a correct solution. We use weakest preconditions and symbolic execution to capture the semantics of execution paths and detect potential path differences. AutoGrader is the first automated grading tool that relies on program semantics and generates feedback with counterexamples based on path deviations. It also reduces human efforts in writing test cases and makes the grading more complete. We implement AutoGrader and test its effectiveness and performance with real-world programming problems and student submissions collected from an online programming site. Our experiment reveals that there are no false negatives using our proposed method and we detected 11 errors of online platform judges.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering Education and Training},
pages = {126–137},
numpages = {12},
keywords = {weakest precondition, automatic grader, programming assignments, equivalence checking},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEET '19}
}

@inproceedings{10.1145/1640089.1640105,
author = {Lee, Byeongcheol and Hirzel, Martin and Grimm, Robert and McKinley, Kathryn S.},
title = {Debug All Your Code: Portable Mixed-Environment Debugging},
year = {2009},
isbn = {9781605587660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1640089.1640105},
doi = {10.1145/1640089.1640105},
abstract = {Programmers build large-scale systems with multiple languages to reuse legacy code and leverage languages best suited to their problems. For instance, the same program may use Java for ease-of-programming and C to interface with the operating system. These programs pose significant debugging challenges, because programmers need to understand and control code across languages, which may execute in different environments. Unfortunately, traditional multilingual debuggers require a single execution environment.This paper presents a novel composition approach to building portable mixed-environment debuggers, in which an intermediate agent interposes on language transitions, controlling and reusing single-environment debuggers. We implement debugger composition in Blink, a debugger for Java, C, and the Jeannie programming language. We show that Blink is (1) relatively simple: it requires modest amounts of new code; (2) portable: it supports multiple Java Virtual Machines, C compilers, operating systems, and component debuggers; and (3) powerful: composition eases debugging, while supporting new mixed-language expression evaluation and Java Native Interface (JNI) bug diagnostics. In real-world case studies, we show that language-interface errors require single-environment debuggers to restart execution multiple times, whereas Blink directly diagnoses them with one execution. We also describe extensions for other mixed-environments to show debugger composition will generalize.},
booktitle = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications},
pages = {207–226},
numpages = {20},
keywords = {foreign function interface, composition, JNI},
location = {Orlando, Florida, USA},
series = {OOPSLA '09}
}

@article{10.1145/1639949.1640105,
author = {Lee, Byeongcheol and Hirzel, Martin and Grimm, Robert and McKinley, Kathryn S.},
title = {Debug All Your Code: Portable Mixed-Environment Debugging},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1639949.1640105},
doi = {10.1145/1639949.1640105},
abstract = {Programmers build large-scale systems with multiple languages to reuse legacy code and leverage languages best suited to their problems. For instance, the same program may use Java for ease-of-programming and C to interface with the operating system. These programs pose significant debugging challenges, because programmers need to understand and control code across languages, which may execute in different environments. Unfortunately, traditional multilingual debuggers require a single execution environment.This paper presents a novel composition approach to building portable mixed-environment debuggers, in which an intermediate agent interposes on language transitions, controlling and reusing single-environment debuggers. We implement debugger composition in Blink, a debugger for Java, C, and the Jeannie programming language. We show that Blink is (1) relatively simple: it requires modest amounts of new code; (2) portable: it supports multiple Java Virtual Machines, C compilers, operating systems, and component debuggers; and (3) powerful: composition eases debugging, while supporting new mixed-language expression evaluation and Java Native Interface (JNI) bug diagnostics. In real-world case studies, we show that language-interface errors require single-environment debuggers to restart execution multiple times, whereas Blink directly diagnoses them with one execution. We also describe extensions for other mixed-environments to show debugger composition will generalize.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {207–226},
numpages = {20},
keywords = {foreign function interface, composition, JNI}
}

@article{10.1145/982158.982162,
author = {Keen, Aaron W. and Ge, Tingjian and Maris, Justin T. and Olsson, Ronald A.},
title = {JR: Flexible Distributed Programming in an Extended Java},
year = {2004},
issue_date = {May 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/982158.982162},
doi = {10.1145/982158.982162},
abstract = {Java provides a clean object-oriented programming model and allows for inherently system-independent programs. Unfortunately, Java has a limited concurrency model, providing only threads and remote method invocation (RMI).The JR programming language extends Java to provide a rich concurrency model, based on that of SR. JR provides dynamic remote virtual machine creation, dynamic remote object creation, remote method invocation, asynchronous communication, rendezvous, and dynamic process creation. JR's concurrency model stems from the addition of operations (a generalization of procedures) and JR supports the redefinition of operations through inheritance. JR programs are written in an extended Java and then translated into standard Java programs. The JR run-time support system is also written in standard Java.This paper describes the JR programming language and its implementation. Some initial measurements of the performance of the implementation are also included.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {may},
pages = {578–608},
numpages = {31},
keywords = {Concurrency, SR, concurrent object-oriented programming, Java}
}

@inproceedings{10.1145/1238828.1238839,
author = {Domaschka, J\"{o}rg and Reiser, Hans P. and Hauck, Franz J.},
title = {Towards Generic and Middleware-Independent Support for Replicated, Distributed Objects},
year = {2007},
isbn = {9781595936967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1238828.1238839},
doi = {10.1145/1238828.1238839},
abstract = {Replication is a commonly used approach to increase the availability of distributed services, which is a non-functional requirement. Thus, replication is in principle independent of the application logic. For this reason, support for replication is part of middleware architectures. Each of them provides its own replication infrastructure, although the differences in functionality are rather marginally.In this paper we claim that replication is even independent of the middleware the application uses. We propose a separation of concerns between middleware and replication systems and present a generic architecture that allows the middleware to support replication by using an existing replication framework. We argue that such an approach is equally transparent, but less intrusive than existing approaches.},
booktitle = {Proceedings of the 1st Workshop on Middleware-Application Interaction: In Conjunction with Euro-Sys 2007},
pages = {43–48},
numpages = {6},
keywords = {CORBA, fragmented objects, fault-tolerance, replication, middleware},
location = {Lisbon, Portugal},
series = {MAI '07}
}

@article{10.1145/503112.503115,
author = {Hartel, Pieter H. and Moreau, Luc},
title = {Formalizing the Safety of Java, the Java Virtual Machine, and Java Card},
year = {2001},
issue_date = {December 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/503112.503115},
doi = {10.1145/503112.503115},
abstract = {We review the existing literature on Java safety, emphasizing formal approaches, and the impact of Java safety on small footprint devices such as smartcards. The conclusion is that although a lot of good work has been done, a more concerted effort is needed to build a coherent set of machine-readable formal models of the whole of Java and its implementation. This is a formidable task but we believe it is essential to build trust in Java safety, and thence to achieve ITSEC level 6 or Common Criteria level 7 certification for Java programs.},
journal = {ACM Comput. Surv.},
month = {dec},
pages = {517–558},
numpages = {42},
keywords = {programming, Common criteria}
}

@inproceedings{10.1145/3447786.3456261,
author = {Ahsan, Shegufta B. and Yang, Rui and Noghabi, Shadi A. and Gupta, Indranil},
title = {Home, Safehome: Smart Home Reliability with Visibility and Atomicity},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456261},
doi = {10.1145/3447786.3456261},
abstract = {Smart environments (homes, factories, hospitals, buildings) contain an increasing number of IoT devices, making them complex to manage. Today, in smart homes when users or triggers initiate routines (i.e., a sequence of commands), concurrent routines and device failures can cause incongruent outcomes. We describe SafeHome, a system that provides notions of atomicity and serial equivalence for smart homes. Due to the human-facing nature of smart homes, SafeHome offers a spectrum of visibility models which trade off between responsiveness vs. isolation of the smart home. We implemented SafeHome and performed workload-driven experiments. We find that a weak visibility model, called eventual visibility, is almost as fast as today's status quo (up to 23% slower) and yet guarantees serially-equivalent end states.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {590–605},
numpages = {16},
keywords = {reliability, smart home, fault-tolerance, routines},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@techreport{10.1145/2965631,
author = {The Joint Task Force on Computing Curricula},
title = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
year = {2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The primary purpose of this volume is to provide guidance to academic institutions and accreditation agencies about what should constitute an undergraduate software engineering education. These recommendations have been developed by a broad, internationally based group of volunteer participants. This group has taken into account much of the work that has been done in software engineering education over the last quarter of a century. Software engineering curriculum recommendations are of particular relevance, since there is currently a surge in the creation of software engineering degree programs and accreditation processes for such programs have been established in a number of countries.}
}

@inproceedings{10.5555/3291656.3291746,
author = {Mahmoud, Abdulrahman and Hari, Siva Kumar Sastry and Sullivan, Michael B. and Tsai, Timothy and Keckler, Stephen W.},
title = {Optimizing Software-Directed Instruction Replication for GPU Error Detection},
year = {2018},
publisher = {IEEE Press},
abstract = {Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic. Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents make it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {67},
numpages = {12},
keywords = {software protection, redundancy, high performance computing, fault tolerance},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1109/SC.2018.00070,
author = {Mahmoud, Abdulrahman and Hari, Siva Kumar Sastry and Sullivan, Michael B. and Tsai, Timothy and Keckler, Stephen W.},
title = {Optimizing Software-Directed Instruction Replication for GPU Error Detection},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00070},
doi = {10.1109/SC.2018.00070},
abstract = {Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic. Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents make it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {67},
numpages = {12},
keywords = {software protection, high performance computing, fault tolerance, redundancy},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/2749469.2750375,
author = {Stephenson, Mark and Sastry Hari, Siva Kumar and Lee, Yunsup and Ebrahimi, Eiman and Johnson, Daniel R. and Nellans, David and O'Connor, Mike and Keckler, Stephen W.},
title = {Flexible Software Profiling of GPU Architectures},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750375},
doi = {10.1145/2749469.2750375},
abstract = {To aid application characterization and architecture design space exploration, researchers and engineers have developed a wide range of tools for CPUs, including simulators, profilers, and binary instrumentation tools. With the advent of GPU computing, GPU manufacturers have developed similar tools leveraging hardware profiling and debugging hooks. To date, these tools are largely limited by the fixed menu of options provided by the tool developer and do not offer the user the flexibility to observe or act on events not in the menu. This paper presents SASSI (NVIDIA assembly code "SASS" Instrumentor), a low-level assembly-language instrumentation tool for GPUs. Like CPU binary instrumentation tools, SASSI allows a user to specify instructions at which to inject user-provided instrumentation code. These facilities allow strategic placement of counters and code into GPU assembly code to collect user-directed, fine-grained statistics at hardware speeds. SASSI instrumentation is inherently parallel, leveraging the concurrency of the underlying hardware. In addition to the details of SASSI, this paper provides four case studies that show how SASSI can be used to characterize applications and explore the architecture design space along the dimensions of instruction control flow, memory systems, value similarity, and resilience.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {185–197},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2750375,
author = {Stephenson, Mark and Sastry Hari, Siva Kumar and Lee, Yunsup and Ebrahimi, Eiman and Johnson, Daniel R. and Nellans, David and O'Connor, Mike and Keckler, Stephen W.},
title = {Flexible Software Profiling of GPU Architectures},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2750375},
doi = {10.1145/2872887.2750375},
abstract = {To aid application characterization and architecture design space exploration, researchers and engineers have developed a wide range of tools for CPUs, including simulators, profilers, and binary instrumentation tools. With the advent of GPU computing, GPU manufacturers have developed similar tools leveraging hardware profiling and debugging hooks. To date, these tools are largely limited by the fixed menu of options provided by the tool developer and do not offer the user the flexibility to observe or act on events not in the menu. This paper presents SASSI (NVIDIA assembly code "SASS" Instrumentor), a low-level assembly-language instrumentation tool for GPUs. Like CPU binary instrumentation tools, SASSI allows a user to specify instructions at which to inject user-provided instrumentation code. These facilities allow strategic placement of counters and code into GPU assembly code to collect user-directed, fine-grained statistics at hardware speeds. SASSI instrumentation is inherently parallel, leveraging the concurrency of the underlying hardware. In addition to the details of SASSI, this paper provides four case studies that show how SASSI can be used to characterize applications and explore the architecture design space along the dimensions of instruction control flow, memory systems, value similarity, and resilience.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {185–197},
numpages = {13}
}

@inproceedings{10.1145/381677.381682,
author = {Goff, Tom and Abu-Ghazaleh, Nael B. and Phatak, Dhananjay S. and Kahvecioglu, Ridvan},
title = {Preemptive Routing in Ad Hoc Networks},
year = {2001},
isbn = {1581134223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/381677.381682},
doi = {10.1145/381677.381682},
abstract = {Existing on-demand ad-hoc routing algorithms initiate route discovery only after a path breaks, incurring a significant cost in detecting the disconnection and establishing a new route. In this work, we investigate adding proactive route selection and maintenance to on-demand ad-hoc routing algorithms. More specifically, when a path is likely to be broken, a warning is sent to the source indicating the likelihood of a disconnection. The source can then initiate path discovery early, potentially avoiding the disconnection altogether. A path is considered likely to break when the received packet power becomes close to the minimum detectable power (other approaches are possible). Care must be taken to avoid initiating false route warnings due to fluctuations in received power caused by fading, multipath effects and similar random transient phenomena. Experiments demonstrate that adding proactive route selection and maintenance to DSR and AODV (on-demand ad hoc routing protocols) significantly reduces the number of broken paths, with a small increase in protocol overhead. Packet latency and jitter also goes down in most cases. We also show some experimental results obtained by running TCP on top of the proactive routing schemes proposed. Several improvements and extensions are also discussed. Pro-active route selection and maintenance is general and can be used with other routing algorithms and optimizations to them.},
booktitle = {Proceedings of the 7th Annual International Conference on Mobile Computing and Networking},
pages = {43–52},
numpages = {10},
location = {Rome, Italy},
series = {MobiCom '01}
}

@article{10.1145/2686892,
author = {Bouteiller, Aurelien and Herault, Thomas and Bosilca, George and Du, Peng and Dongarra, Jack},
title = {Algorithm-Based Fault Tolerance for Dense Matrix Factorizations, Multiple Failures and Accuracy},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/2686892},
doi = {10.1145/2686892},
abstract = {Dense matrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This article proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable node and the possibility of losing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minor modifications. Theoretical analysis shows that the fault tolerance overhead decreases inversely to the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors. Applicability to tolerate multiple failures and accuracy after multiple recovery is also considered.},
journal = {ACM Trans. Parallel Comput.},
month = {feb},
articleno = {10},
numpages = {28},
keywords = {linear algebra, high performance computing, fault-tolerance, ABFT}
}

@article{10.1145/3360611,
author = {Sergey, Ilya and Nagaraj, Vaivaswatha and Johannsen, Jacob and Kumar, Amrit and Trunov, Anton and Hao, Ken Chan Guan},
title = {Safer Smart Contract Programming with Scilla},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360611},
doi = {10.1145/3360611},
abstract = {The rise of programmable open distributed consensus platforms based on the blockchain technology has aroused a lot of interest in replicated stateful computations, aka smart contracts. As blockchains are used predominantly in financial applications, smart contracts frequently manage millions of dollars worth of virtual coins. Since smart contracts cannot be updated once deployed, the ability to reason about their correctness becomes a critical task. Yet, the de facto implementation standard, pioneered by the Ethereum platform, dictates smart contracts to be deployed in a low-level language, which renders independent audit and formal verification of deployed code infeasible in practice.  We report an ongoing experiment held with an industrial blockchain vendor on designing, evaluating, and deploying Scilla, a new programming language for safe smart contracts. Scilla is positioned as an intermediate-level language, suitable to serve as a compilation target and also as an independent programming framework. Taking System F as a foundational calculus, Scilla offers strong safety guarantees by means of type soundness. It provides a clean separation between pure computational, state-manipulating, and communication aspects of smart contracts, avoiding many known pitfalls due to execution in a byzantine environment. We describe the motivation, design principles, and semantics of Scilla, and we report on Scilla use cases provided by the developer community. Finally, we present a framework for lightweight verification of Scilla programs, and showcase it with two domain-specific analyses on a suite of real-world use cases.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {185},
numpages = {30},
keywords = {Smart Contracts, Static Analysis, Blockchain, Domain-Specific Languages}
}

@article{10.1145/1925109.1925111,
author = {Lagar-Cavilla, H. Andr\'{e}s and Whitney, Joseph A. and Bryant, Roy and Patchin, Philip and Brudno, Michael and de Lara, Eyal and Rumble, Stephen M. and Satyanarayanan, M. and Scannell, Adin},
title = {SnowFlock: Virtual Machine Cloning as a First-Class Cloud Primitive},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/1925109.1925111},
doi = {10.1145/1925109.1925111},
abstract = {A basic building block of cloud computing is virtualization. Virtual machines (VMs) encapsulate a user’s computing environment and efficiently isolate it from that of other users. VMs, however, are large entities, and no clear APIs exist yet to provide users with programatic, fine-grained control on short time scales.We present SnowFlock, a paradigm and system for cloud computing that introduces VM cloning as a first-class cloud abstraction. VM cloning exploits the well-understood and effective semantics of UNIX fork. We demonstrate multiple usage models of VM cloning: users can incorporate the primitive in their code, can wrap around existing toolchains via scripting, can encapsulate the API within a parallel programming framework, or can use it to load-balance and self-scale clustered servers.VM cloning needs to be efficient to be usable. It must efficiently transmit VM state in order to avoid cloud I/O bottlenecks. We demonstrate how the semantics of cloning aid us in realizing its efficiency: state is propagated in parallel to multiple VM clones, and is transmitted during runtime, allowing for optimizations that substantially reduce the I/O load. We show detailed microbenchmark results highlighting the efficiency of our optimizations, and macrobenchmark numbers demonstrating the effectiveness of the different usage models of SnowFlock.},
journal = {ACM Trans. Comput. Syst.},
month = {feb},
articleno = {2},
numpages = {45},
keywords = {Virtualization, cloud computing}
}

@proceedings{10.1145/2907950,
title = {LCTES 2016: Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems},
year = {2016},
isbn = {9781450343169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santa Barbara, CA, USA}
}

@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare Metal Speed," explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5%, and we believe that the revision processhas improved the quality of the technical program.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3214834,
title = {SIGGRAPH '18: ACM SIGGRAPH 2018 Courses},
year = {2018},
isbn = {9781450358095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Learn New Concepts and SkillsSIGGRAPH courses are learning sessions in which experts from all areas of computer graphics technology and interactive techniques share their knowledge. Course presenters distill key concepts and ideas into self-contained lessons. Courses may lie anywhere on a continuum from conceptual and theoretical to practical and applied.Courses are presented in both long (3.25 hour) or short (1.5 hour) sessions and may include elements of interactive demonstration, performance, or other imaginative approaches to teaching.},
location = {Vancouver, British Columbia, Canada}
}

@proceedings{10.1145/2729094,
title = {ITiCSE '15: Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
year = {2015},
isbn = {9781450334402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ITiCSE 2015 in Vilnius!The ITiCSE conference celebrates its 20th anniversary in Vilnius, the capital of Lithuania and the geographical center of Europe, so declared in 1989 by scientists of the French National Institute of Geography.ITiCSE will be held on July 6--8, starting on Lithuania's Statehood Day (July 6). This is an annual public holiday that commemorates the coronation in 1253 of Mindaugas as the first and only King of Lithuania. The conference venue is the Parliament buildings (Seimas) of the Republic of Lithuania, and the conference dinner is to served in the reconstructed Palace of the Grand Dukes of Lithuania, one of the most famous in Europe in the 15--17th centuries. ITiCSE 2015 is hosted by Vilnius University, one of the oldest and most famous establishments of higher education in Eastern and Central Europe, founded in 1579. The conference organizers represent the Lithuanian research group of Informatics and Informatics Engineering Didactics at the Institute of Mathematics and Informatics of Vilnius University.This conference brings together delegates from all over the world to address pressing issues in computing education. In addition to invited lectures, papers, panels, posters, and tips, techniques &amp; courseware sessions, the conference provides facilities and exposure for working groups and exhibitions.The conference continues to be truly international with a total of 170 submissions from 40 countries on six continents, with authors from Africa (4), Asia (50), Europe (151), North America (119), Oceania (51), and South America (17). These submissions consisted of 124 research papers, 1 panel, 9 working group proposals, and 36 proposals for posters or for tips, techniques &amp; courseware.All research papers were double blind reviewed by at least four reviewers, though most papers received five or six reviews. A meta-review was conducted by the members of the conference committee to ensure the reliability of the reviews and to make recommendations to the chairs. A final selection phase was conducted by the program chairs who reviewed all reviews and meta-review recommendations before making their final decisions. As a result of this process, 54 research papers (43.5%) were selected for presentation and inclusion in the proceedings. The authors of the accepted papers come from 17 different countries on five continents.All poster submissions were blind reviewed by two members of the conference committee, and tips, techniques &amp; courseware submissions were blind reviewed by three members of the conference committee. Submissions in these categories were then reviewed by the conference chair before selection by the program chairs for final inclusion in the conference. Twenty-four were accepted, representing authors from 15 countries.The two keynote speakers address the learning of programming and computational thinking. Professor Mordechai (Moti) Ben-Ari from the Weizmann Institute of Science, Israel, will give a talk titled In Defense of Programming, which defends the (perhaps controversial) position that programming is the fundamental activity of CS. In the other keynote talk Professor Maciej M. Syslo from Nicolaus Copernicus University and University of Wroc\l{}aw, Poland, will address algorithmic nand computational thinking as the way to computing for all students.ITiCSE is famous for its working groups. Participating in a working group provides a unique opportunity to work with people from different countries who are interested and knowledgeable in the area of the working group. It is also one of the best ways to become part of the ITiCSE community. Seven working groups have been accepted over a broad spectrum of topics. The working groups range from general topics, such as computing education terminology, CS education in K-9 and K-12 schools, and designing an IT curriculum framework for graduates in 2025, to more specific topics such as developing a repository for high school CS questions, visual assessment tools and metadata annotations, and how students construct solutions to programming problems. The leaders of the accepted working groups come from over 13 countries.Welcome to Vilnius and enjoy the vicennial ITiCSE conference and Lithuania's Statehood Day!},
location = {Vilnius, Lithuania}
}

@proceedings{10.1145/2837614,
title = {POPL '16: Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg, FL, USA}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3009837,
title = {POPL 2017: Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
year = {2017},
isbn = {9781450346603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

