@inproceedings{10.1145/3106426.3106480,
author = {Batista, Nat\'{e}rcia A. and Brand\~{a}o, Michele A. and Alves, Gabriela B. and da Silva, Ana Paula Couto and Moro, Mirella M.},
title = {Collaboration Strength Metrics and Analyses on GitHub},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106480},
doi = {10.1145/3106426.3106480},
abstract = {We perform social analyses over an important community: the open code collaboration network. Specifically, we study the correlation among features that measure the strength of social coding collaboration on GitHub - a Web-based source code repository that can be modeled as a social coding network. We also make publicly available a curated dataset called GitSED, GitHub Socially Enhanced Dataset. Our results have many practical applications such as to improve the recommendation of developers, the evaluation of team formation and existing analysis algorithms.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {170–178},
numpages = {9},
keywords = {social network analysis, online cooperative work, tie strength},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/1411759.1411762,
author = {Agapie, E. and Chen, G. and Houston, D. and Howard, E. and Kim, J. and Mun, M. Y. and Mondschein, A. and Reddy, S. and Rosario, R. and Ryder, J. and Steiner, A. and Burke, J. and Estrin, E. and Hansen, M. and Rahimi, M.},
title = {Seeing Our Signals: Combining Location Traces and Web-Based Models for Personal Discovery},
year = {2008},
isbn = {9781605581187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411759.1411762},
doi = {10.1145/1411759.1411762},
abstract = {Each of us has a complex and reciprocal relationship with our environment. Based on limited knowledge of this interwoven set of influences and consequences, we constantly make choices: where to live, how to go to work, what brands to buy, what to do with our leisure time. These choices evolve into patterns, and these patterns become driving functions of our relationship with the world around us. With increasing ease, devices we carry can sense, process, and transmit data on these patterns for our own use or to share, carefully, with others. In particular, here we will focus on location time series, gathered from GPS-enabled personal mobile devices. From this capacity emerges a new class of hybrid mobile-web applications that, first, enable personal exploration of our own patterns and, second, use the same data to index our life into other available datasets about the world around us. Such applications, revealing the previously unobservable about our own lives, offer an opportunity to employ mobile technology to illuminate the ramifications of our choices on others and the effects of the "microenvironments" we move through on us [1, 10].},
booktitle = {Proceedings of the 9th Workshop on Mobile Computing Systems and Applications},
pages = {6–10},
numpages = {5},
keywords = {personal impact, location technology, mapmatching, activity diary, personal exposure},
location = {Napa Valley, California},
series = {HotMobile '08}
}

@inproceedings{10.1145/2380445.2380498,
author = {Sousa, Marcelo and Sen, Alper},
title = {Generation of TLM Testbenches Using Mutation Testing},
year = {2012},
isbn = {9781450314268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380445.2380498},
doi = {10.1145/2380445.2380498},
abstract = {Testbench development is a major component of simulation based verification, which is the de-facto verification technique used in the industry. Verification of a TLM design is not complete without a measure of the effectiveness of its testbenches. We devise a coverage driven testbench generation technique where the coverage metric uses that of a fault insertion based approach, namely mutation testing. Mutation testing is a commonly used software testing technique to measure the quality of testbenches. In mutation testing, the goal is to insert mutations (syntactic changes) into the program and check whether the impact of these mutations on the program can be detected by the available testbenches. If the testbenches cannot detect the impact of these inserted mutations then one can potentially add new testbenches to detect these changes. In this work, we automate the process of mutation testing based testbench generation exploiting the properties of concurrent TLM designs. Our framework is novel in that it uses the byte code representation of SystemC TLM models using Low Level Virtual Machine (LLVM) framework. Furthermore, these testbenches can detect concurrency related defects such as deadlocks or race conditions. We perform experiments on TLM models to demonstrate the effectiveness of our test bench generation approach.},
booktitle = {Proceedings of the Eighth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
pages = {323–332},
numpages = {10},
keywords = {verification, systemc, tlm, mutation testing, llvm},
location = {Tampere, Finland},
series = {CODES+ISSS '12}
}

@inproceedings{10.1145/3295500.3356184,
author = {Chang, Chun-Kai and Yin, Wenqi and Erez, Mattan},
title = {Assessing the Impact of Timing Errors on HPC Applications},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356184},
doi = {10.1145/3295500.3356184},
abstract = {Timing errors are a growing concern for system resilience as technology continues to scale. It is problematic to use low-fidelity errors such as single-bit flips to model realistic timing errors. We address the lack of holistic methodology and tool for evaluating resilience of applications against timing errors. The proposed technique is able to rapidly inject high-fidelity and configurable timing errors to applications at the instruction level. Our implementation has no runtime dependencies on proprietary tools, enabling full parallelism of error injection campaign. Furthermore, because an injection point may not generate an actual error for a particular application run, we propose an acceleration technique to maximize the likelihood of generating errors that contribute to the overall campaign with speedup up to 7X. With our tool, we show that realistic timing errors lead to distinct error profiles from those of radiation-induced errors at both the instruction level and the application level.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {70},
numpages = {19},
location = {Denver, Colorado},
series = {SC '19}
}

@article{10.1145/3241737,
author = {Buyya, Rajkumar and Srirama, Satish Narayana and Casale, Giuliano and Calheiros, Rodrigo and Simmhan, Yogesh and Varghese, Blesson and Gelenbe, Erol and Javadi, Bahman and Vaquero, Luis Miguel and Netto, Marco A. S. and Toosi, Adel Nadjaran and Rodriguez, Maria Alejandra and Llorente, Ignacio M. and Vimercati, Sabrina De Capitani Di and Samarati, Pierangela and Milojicic, Dejan and Varela, Carlos and Bahsoon, Rami and Assuncao, Marcos Dias De and Rana, Omer and Zhou, Wanlei and Jin, Hai and Gentzsch, Wolfgang and Zomaya, Albert Y. and Shen, Haiying},
title = {A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3241737},
doi = {10.1145/3241737},
abstract = {The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {38},
keywords = {Cloud computing, application development, data management, Cloud economics, Fog computing, scalability, serverless computing, sustainability, InterCloud}
}

@inproceedings{10.5555/3014904.3014931,
author = {Liu, Qingrui and Jung, Changhee and Lee, Dongyoon and Tiwari, Devesh},
title = {Compiler-Directed Lightweight Checkpointing for Fine-Grained Guaranteed Soft Error Recovery},
year = {2016},
isbn = {9781467388153},
publisher = {IEEE Press},
abstract = {This paper presents Bolt, a compiler-directed soft error recovery scheme, that provides fine-grained and guaranteed recovery without excessive performance and hardware overhead. To get rid of expensive hardware support, the compiler protects the architectural inputs during their entire liveness period by safely checkpointing the last updated value in idempotent regions. To minimize the performance overhead, Bolt leverages a novel compiler analysis that eliminates those checkpoints whose value can be reconstructed by other checkpointed values without compromising the recovery guarantee. As a result, Bolt incurs only 4.7% performance overhead on average which is 57% reduction compared to the state-of-the-art scheme that requires expensive hardware support for the same recovery guarantee as Bolt.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {20},
numpages = {12},
keywords = {compiler, checkpointing, reliability},
location = {Salt Lake City, Utah},
series = {SC '16}
}

@article{10.1145/2094091.2094113,
author = {Raghavan, Karthik and Kamakoti, V.},
title = {ROSY: Recovering Processor and Memory Systems from Hard Errors},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/2094091.2094113},
doi = {10.1145/2094091.2094113},
abstract = {In the nanometer era, there has been a steady decline in the semiconductor chip manufacturing yield due to various contributing factors, such as wearout and defects due to complex processes. One of the strategies to alleviate this issue is to recover and use faulty hardware at gracefully degraded performance. A common, though naive, recovery strategy followed in the context of general purpose multicore systems is to disable the cores with faults and use only the fully functional cores. Such a coarse-granular solution is suboptimal, as the disabled cores would have many working modules which go un-utilized. The Resurrecting Operating SYstem (ROSY) presented in this paper is a step towards the development of an operating system that can work on faulty cores by adapting itself to hardware faults using software workarounds, and and utilize their working components. We consider many realistic fault models and present software workarounds for them. We have developed a framework which can be trivially plugged into a fullyfeatured x86 based OS kernel to demonstrate the feasibility of the proposed ideas. Performance evaluation using SPEC benchmarks and real-world applications show that the performance degradation of the depleted cores executing ROSY is on an average between 1.6x to 4x, depending on the fault type.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jan},
pages = {82–84},
numpages = {3},
keywords = {X86 ISA, instruction set architecture (ISA), manufacturing and aging defects, multicore systems, fault-tolerance, operating system}
}

@inproceedings{10.1145/3429789.3429802,
author = {Feng, Yang and Murata, Koichi},
title = {Exploring Characteristic of Visual Management as Lean Toolbox in Construction Worksite of Apartment House},
year = {2020},
isbn = {9781450387712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429789.3429802},
doi = {10.1145/3429789.3429802},
abstract = {At present, the construction industry is actively using lean production, which was born in the manufacturing industry, in order to improve the level of management and reduce wastes. This paper examines the implementation of visual management based on lean production in a construction site in Japan, and analyzes the use of visualization tools. It aims to solve two main problems: first, which visualization tools of lean production are used in Japanese construction industry? The second is that the author wants to clarify which practical problems these tools are used to solve, whether there is a one-to-one correspondence between the tools and the problem, or there is a phenomenon of overlapped purposes between them. To solve the above problems, the author makes a basic collection of the tools currently used in the construction industry by using methods such as on-site observation of the construction site in Japan, interviews with the managers and workers, and so on. Then he carries on word extraction and system diagram classification to the collected tools, which illustrates that the current visualization tools are used for five basic purposes, and at the same time clarifies the phenomenon that these tools do have overlapped purposes phenomenon. Last but not least, the author also analyzes these tools by using the co-occurrence network, and shows the details of the overlapped purpose phenomenon. These findings help to better understand the characteristics of construction site visual management tools from the perspective of the application of lean technology, laying a foundation for better utilization in the future.},
booktitle = {Proceedings of the International Conference on Engineering and Information Technology for Sustainable Industry},
articleno = {12},
numpages = {6},
keywords = {Visual Management, Data Mining, Construction Industry, Lean Production},
location = {Tangerang, Indonesia},
series = {ICONETSI}
}

@article{10.1145/3241743,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {The ABC of Software Engineering Research},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3241743},
doi = {10.1145/3241743},
abstract = {A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {11},
numpages = {51},
keywords = {Research methodology, research strategy}
}

@inproceedings{10.1145/583810.583819,
author = {Luj\'{a}n, Mikel and Gurd, John R. and Freeman, T. L. and Miguel, Jos\'{e}},
title = {Elimination of Java Array Bounds Checks in the Presence of Indirection},
year = {2002},
isbn = {1581135998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/583810.583819},
doi = {10.1145/583810.583819},
abstract = {The Java language specification states that every access to an array needs to be within the bounds of that array; i.e. between 0 and array length 1. Different techniques for different programming languages have been proposed to eliminate explicit bounds checks. Some of these techniques are implemented in off-the-shelf Java Virtual Machines (JVMs). The underlying principle of these techniques is that bounds checks can be removed when a JVM/compiler has enough information to guarantee that a sequence of accesses (e.g. inside a for-loop) is safe (within the bounds). Most of the techniques for the elimination of array bounds checks have been developed for programming languages that do not support multi-threading and/or enable dynamic class loading. These two characteristics make most of these tech niques unsuitable for Java. Techniques developed specifically for Java have not addressed the elimination of array bounds checks in the presence of indirection, that is, when the index is stored in another array (indirection array). With the objective of optimising applications with array indirection, this paper proposes and evaluates three implementation strategies, each implemented as a Java class. The classes provide the functionality of Java arrays of type int so that objects of the classes can be used instead of indirection arrays. Each strategy enables JVMs, when examining only one of these classes at a time, to obtain enough information to remove array bounds checks.},
booktitle = {Proceedings of the 2002 Joint ACM-ISCOPE Conference on Java Grande},
pages = {76–85},
numpages = {10},
keywords = {Java, array indirection, array bounds check},
location = {Seattle, Washington, USA},
series = {JGI '02}
}

@inproceedings{10.1145/3195970.3196019,
author = {Dietrich, Christian and Schmider, Achim and Pusz, Oskar and Vay\'{a}, Guillermo Pay\'{a} and Lohmann, Daniel},
title = {Cross-Layer Fault-Space Pruning for Hardware-Assisted Fault Injection},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3196019},
doi = {10.1145/3195970.3196019},
abstract = {With shrinking structure sizes, soft-error mitigation has become a major challenge in the design and certification of safety-critical embedded systems. Their robustness is quantified by extensive fault-injection campaigns, which on hardware level can nevertheless cover only a tiny part of the fault space.We suggest Fault-Masking Terms (MATEs) to effectively prune the fault space for gate-level fault injection campaigns by using the (software-induced) hardware state to dynamically cut off benign faults. Our tool applied to an AVR core and a size-optimized MSP430 implementation shows that up to 21 percent of all SEUs on flip-flop level are masked within one clock cycle.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {79},
numpages = {6},
location = {San Francisco, California},
series = {DAC '18}
}

@inproceedings{10.1109/MSR.2019.00071,
author = {Habchi, Sarra and Moha, Naouel and Rouvoy, Romain},
title = {The Rise of Android Code Smells: Who is to Blame?},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00071},
doi = {10.1109/MSR.2019.00071},
abstract = {The rise of mobile apps as new software systems led to the emergence of new development requirements regarding performance. Development practices that do not respect these requirements can seriously hinder app performances and impair user experience, they qualify as code smells. Mobile code smells are generally associated with inexperienced developers who lack knowledge about the framework guidelines. However, this assumption remains unverified and there is no evidence about the role played by developers in the accrual of mobile code smells. In this paper, we therefore study the contributions of developers related to Android code smells. To support this study, we propose SNIFFER, an open-source toolkit that mines Git repositories to extract developers contributions as code smell histories. Using SNIFFER, we analysed 255k commits from the change history of 324 Android apps. We found that the ownership of code smells is spread across developers regardless of their seniority. There are no distinct groups of code smell introducers and removers. Developers who introduce and remove code smells are mostly the same.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {445–456},
numpages = {12},
keywords = {Android, code smells, mobile apps, history mining},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/1851476.1851486,
author = {Chard, Kyle and Bubendorfer, Kris and Komisarczuk, Peter},
title = {High Occupancy Resource Allocation for Grid and Cloud Systems, a Study with DRIVE},
year = {2010},
isbn = {9781605589428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851476.1851486},
doi = {10.1145/1851476.1851486},
abstract = {Economic models have long been advocated as a means of efficient resource allocation, however they are often criticized due to a lack of performance and high overheads. The widespread adoption of utility computing models as seen in commercial Cloud providers has re-motivated the need for economic allocation mechanisms. The aim of this work is to address some of the performance limitations of existing economic allocation models, by reducing the failure/reallocation rate, increasing occupancy and thereby increasing the obtainable utilization of the system. This paper is a study of high performance resource utilization strategies that can be employed in Grid and Cloud systems. In particular we have implemented and quantified the results for strategies including overbooking, advanced reservation, justin-time bidding and using substitute providers for service delivery. These strategies are analyzed in a meta-scheduling context using synthetic workloads derived from a production Grid trace to quantify the performance benefits obtained.},
booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
pages = {73–84},
numpages = {12},
keywords = {grid computing, economic resource allocation, cloud computing},
location = {Chicago, Illinois},
series = {HPDC '10}
}

@article{10.1145/383845.383866,
author = {Pancake, Cherri and Lengauer, Christian},
title = {High-Performance Java},
year = {2001},
issue_date = {Oct. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/383845.383866},
doi = {10.1145/383845.383866},
journal = {Commun. ACM},
month = {oct},
pages = {98–101},
numpages = {4}
}

@inproceedings{10.5555/2433508.2433896,
author = {Tebo, Corey and Mukherjee, Amlan and Onder, Nilufer},
title = {A Multipurpose Simulation Platform for Decision-Making in Construction Management},
year = {2010},
isbn = {9781424498642},
publisher = {Winter Simulation Conference},
abstract = {Research in general purpose and special purpose simulation platforms typically treat model development, experimentation, validation and deployment of simulations as distinct phases. Direct involvement of decision-makers is usually limited to the validation phase, even though their participation significantly improves the effectiveness and applicability of models. Unfortunately, the complexity and sophistication involved in the model development and experimentation phases deters their participation. This also makes the validation problem particularly challenging, and hinders the credibility and successful deployment of such models. In addressing this problem, we introduce an interactive simulation platform called Interactive Construction Decision Making Aid (ICDMA), that integrates decision-maker participation into the model development, simulation deployment, experimentation and validation phases. Effectively it separates the complexities of programming the model and the model development process, thus encouraging the participation of domain experts. We illustrate the usefulness of this platform.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3123–3131},
numpages = {9},
location = {Baltimore, Maryland},
series = {WSC '10}
}

@inproceedings{10.1145/3175536.3175562,
author = {Agrawal, Vivek and Snekkenes, Einar Arthur},
title = {UnRizkNow: An Open Electronic Community of Practice for Information Security Professionals},
year = {2017},
isbn = {9781450354356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175536.3175562},
doi = {10.1145/3175536.3175562},
abstract = {We are establishing UnRizkNow as the open electronic community of practice (eCoP) for information security practitioners (ISP) working in Norway. UnRizkNow will aim to solve the challenges faced by the ISP in the information security domain. The purpose of this study is to analyze the factors that are essential to design UnRizkNow community. A research model based on purpose, motivation, facilitating condition, and preference to share knowledge in the electronic platforms is proposed in the study. Furthermore, an online questionnaire is developed based on the elements of the proposed research model to collect responses from the ISP affiliated with ISACA Norway. We analyzed the responses collected through the online survey to extract the most desirable features of UnRizkNow community. We incorporated the following features into UnRizkNow: a) the information available in the community is easy to search, b) the updated information can be easily accessed, c) verify the information is coming from a reliable member, d) The information is relevant to the problem/ concern of the member, e) useful information can be collected at the same place. The designed features of UnRizkNow will be helpful for ISP to share knowledge effectively.},
booktitle = {Proceedings of the 2017 9th International Conference on Education Technology and Computers},
pages = {191–197},
numpages = {7},
keywords = {Information Security, Knowledge sharing, UnRizkNow, electronic Community of Practice},
location = {Barcelona, Spain},
series = {ICETC 2017}
}

@inproceedings{10.1145/1988008.1988036,
author = {Schneider, Daniel and Becker, Martin and Trapp, Mario},
title = {Approaching Runtime Trust Assurance in Open Adaptive Systems},
year = {2011},
isbn = {9781450305754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988008.1988036},
doi = {10.1145/1988008.1988036},
abstract = {In recent years it has become more and more evident that the ability of systems to adapt themselves is an increasingly important requirement. This is not least driven by emerging computing trends like Ubiquitous Computing, Ambient Intelligence, and Cyber Physical Systems, where systems have to react on changing user needs, service/device availability and resource situations. Despite being open and adaptive it is a common requirement for such systems to be trustworthy, whereas traditional assurance techniques for related system properties like safety, reliability and security are not sufficient in this context. We recently developed the Plug&amp;Safe approach for composition time safety assurance in systems of systems. In this position paper we provide an overview on Plug&amp;Safe, elaborate the different facets of trust, and discuss how our approach can be augmented to enable trust assurance in open adaptive systems.},
booktitle = {Proceedings of the 6th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {196–201},
numpages = {6},
keywords = {trust, conditional certificates, safety, adaptive systems},
location = {Waikiki, Honolulu, HI, USA},
series = {SEAMS '11}
}

@inproceedings{10.1145/2656106.2656129,
author = {Stilkerich, Isabella and Taffner, Philip and Erhardt, Christoph and Dietrich, Christian and Wawersich, Christian and Stilkerich, Michael},
title = {Team up: Cooperative Memory Management in Embedded Systems},
year = {2014},
isbn = {9781450330503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656106.2656129},
doi = {10.1145/2656106.2656129},
abstract = {The use of a managed, type-safe languages such as Java in real-time and embedded systems can offer productivity and, in particular, safety and dependability benefits over the dominating unsafe languages at reasonable costs. A JVM that has dynamic memory-management needs to provide an implicit memory-management strategy, that is, for example, a garbage collector (GC) or stack allocation provided by the escape analysis of the JVM's compiler: Explicit management of dynamically allocated memory (i.e., by use of functions such as C's malloc() and free()) is vulnerable to programming errors such as neglected or false memory release operations causing memory leaks or dangling pointers. Such operations have the potential to break the soundness of the type system and are therefore usually not available for strongly typed languages. Type-safe languages in combination with static analyses -- which respect hardware as well as system-specific information -- can efficiently be employed to provide a runtime system including memory management (MM) that is specifically suited to an embedded application on a particular hardware device. In the context of this paper, we present novel memory-management strategy we implemented in our KESO JVM. It is a latency-aware garbage-collection algorithm called LAGC. Also, we introduce the static analyses that can assist LAGC. The application developers have to ensure that there is enough time for the GCs to run. Hardware characteristics such as soft-error proneness of the hardware or the memory layout can also be taken into consideration as demanded by the system configuration. This is achieved by integrating the GCs in the design process of the whole system just as any other user application, which is the reason why this approach is called cooperative memory management. The suggested strategies require reasonably low overhead.},
booktitle = {Proceedings of the 2014 International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
articleno = {10},
numpages = {10},
keywords = {memory management, garbage collection},
location = {New Delhi, India},
series = {CASES '14}
}

@inproceedings{10.5555/3158161.3158189,
author = {Yoder, Joseph W. and Wirfs-Brock, Rebecca and Washizaki, Hironori},
title = {QA to AQ Part Six: Being Agile at Quality "<i>Enabling and Infusing Quality</i>"},
year = {2016},
publisher = {The Hillside Group},
address = {USA},
abstract = {To achieve quality systems and products, it is vital to enable and infuse quality work throughout the entire process, rather than piling it on at the end. Thus paying attention to when to this, how to do this, and who is involved can increase quality. This paper presents three patterns from the collection of patterns on being agile at quality: System Quality Specialist, Spread the Quality Workload, and Automate As You Go. System Quality Specialists can define, test, and implement system-quality characteristics that are complex or require specialized skills and expertise to get right. Spreading the Quality Workload throughout the development process keeps the team from being overly burdened with quality-related work at any point in time. Automating First enables teams to streamline their build and testing processes, eliminate tedious or mundane tasks, and allow more time for team members to focus on implementing and testing important system qualities.},
booktitle = {Proceedings of the 23rd Conference on Pattern Languages of Programs},
articleno = {23},
numpages = {14},
keywords = {software quality, spread the quality workload, agile software development, system qualities, automate as you go, Quality Assurance, system quality specialist, testing, agile, patterns, agile quality},
location = {Monticello, Illinois},
series = {PLoP '16}
}

@inproceedings{10.5555/1218112.1218355,
author = {Smith, V. Devon and Searles, Donald G. and Thompson, Bruce M. and Cranwell, Robert M.},
title = {SEM: Enterprise Modeling of JSF Global Sustainment},
year = {2006},
isbn = {1424405017},
publisher = {Winter Simulation Conference},
abstract = {The Joint Strike Fighter (JSF) Program is implementing a paradigm shift to a performance-based logisties environment for force sustainment. This approach produces the necessary levels of performance at a significantly reduced cost of ownership. The resulting logistics environment is multi-national, multi-echelon, and multi-service. The magnitude of the change in the support concept requires an enterprise-level model that can instill customer confidence in unproven alternatives to legacy approaches and capture investment/commitment to enable a profitable execution. The Support Enterprise Model (SEM) was developed by Lockheed Martin to provide a consistent/accurate global view for support of strategic decisions during design/implementation of a JSF global sustainment solution. SEM is a discrete event simulation that allows analysts to define operational/support environment, ascertain measures of effectiveness for performance/cost metrics, and characterize sensitivity to changes in Support System architecture, processes, and business approach as well as air vehicle reliability and maintainability characteristics.},
booktitle = {Proceedings of the 38th Conference on Winter Simulation},
pages = {1324–1331},
numpages = {8},
location = {Monterey, California},
series = {WSC '06}
}

@inproceedings{10.1145/3324884.3416532,
author = {Tian, Haoye and Liu, Kui and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416532},
doi = {10.1145/3324884.3416532},
abstract = {A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {981–992},
numpages = {12},
keywords = {distributed representation learning, embeddings, machine learning, patch correctness, program repair},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.5555/3320516.3320844,
author = {Weaver, Gabriel A. and Marla, Lavanya},
title = {Cyber-Physical Simulation and Optimal Mitigation for Shipping Port Operations},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Modern shipping ports require computer systems to accommodate an increasing number of port calls, larger vessel sizes, and tighter supply chains. Disruptions to assets on these networks have the potential to propagate to other critical infrastructures at great economic cost. Such disruptions may be introduced intentionally by adversaries that include nation states, organized crime, hacktivists, and insiders. Area Maritime Security Committees (AMSCs) must develop security plans to minimize disruptions' impact. This paper explores one way to couple a simulation of the flow of commodities through a shipping port with an optimization that minimizes the cost of disruptions to the port transportation system. Our intent is to enable stakeholders to run what-if scenarios, to understand the impact and effect of cyber-physical disruptions, and to optimally mitigate their effect. This research, based on ongoing fieldwork with Port Everglades and the USCG, hopes to improve security policies that integrate cyber and physical effects.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {2747–2758},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/1125170.1125211,
author = {Kolfschoten, Gwendolyn L. and Niederman, Fred and Briggs, Robert O. and de Vreede, Gert-Jan},
title = {Understanding the Job Requirements for Collaboration Technology Support through a Hybrid IT-End User Job Classification Model: The Case of Collaboration Engineering and Facilitation},
year = {2006},
isbn = {1595933492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1125170.1125211},
doi = {10.1145/1125170.1125211},
abstract = {Group Support Systems (GSS) refers broadly to supporting electronically mediated face-to-face and distributed teams throughout organizations performing a wide range of tasks. While the value of GSS has been shown in numerous field studies, their adoption and diffusion remains limited. One explanation for the limited dispersion of these systems is the difficulty faced by facilitators who lead and manage the technology, processes, and people involved with these tasks. Successfully fulfilling the roles of facilitation is complex and difficult. In this study we present a general IT-end user job classification model that we use to examine the various roles that facilitators typically play and discuss these in terms of the skills needed for their performance. We use the model to compare the 'traditional view' on GSS facilitation with that of Collaboration Engineering that proposes different skill sets to overcome organizational dependence on dedicated facilitators in GSS contexts.},
booktitle = {Proceedings of the 2006 ACM SIGMIS CPR Conference on Computer Personnel Research: Forty Four Years of Computer Personnel Research: Achievements, Challenges &amp; the Future},
pages = {150–157},
numpages = {8},
keywords = {job classification, end users, collaboration engineering, collaboration technology, support roles, GSS, facilitation, thinkLets},
location = {Claremont, California, USA},
series = {SIGMIS CPR '06}
}

@inproceedings{10.5555/381473.381546,
author = {Modesitt, Kenneth L. and Bagert, Don and Werth, Laurie},
title = {Academic Software Engineering: What is and What Could Be? Results of the First Annual Survey for International SE Programs},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {According to data received from an international survey, almost 6800 students are enrolled in software engineering degree programs in 11 countries, as of January, 2001. A total of 94 academic programs in software engineering are in place at 60 univcrsities with 350 full-time faculty and nearly 200 part-time faculty teaching hundreds of undergraduate and graduate courses in the discipline. Over 5500 people have obtained degrees in software engineering since 1979. The authors are conducting the first of an ongoing annual survey of international academic software engineering programs, as a joint ACM/IEEE-CS project. This status report covers: history, audience, initial survey, initial partial results available on the WWW, request for evaluation of WWW-site, request for additional questions for next version of survey, time-line for next version of the survey, “lessons learned,” and some future directions. The annual report and survey results will be posted on a wide variety of web pages. A more current report, based on the sabbatical of the first author, will be presented at the conference. The sabbatical involves the initial development of an “International Software Engineering University Consortium - ISEUC.” A sample scenario for an employee in industry who becomes a student in ISEUC is given.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {643–652},
numpages = {10},
keywords = {survey, education, international},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@inproceedings{10.1145/2380790.2380797,
author = {Yoas, Daniel W. and Simco, Greg},
title = {Resource Utilization Prediction: A Proposal for Information Technology Research},
year = {2012},
isbn = {9781450316439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380790.2380797},
doi = {10.1145/2380790.2380797},
abstract = {Research into predicting long-term resource needs has been faced with a very difficult problem of extending the accuracy period beyond the immediate future. Business forecasting has overcome this limitation by successfully incorporating the concept of human interaction as the basis of prediction patterns at the hourly, daily, weekly, monthly, and yearly time frames. Computer resource utilization is also impacted by human interaction therefore influencing research into predictability of resource usage based on human access patterns. Emulated human web server access data was captured in a feasibility study that used time series analysis to predict future resource usage. For prediction beyond several minutes, results indicate that the majority of projected resource usage was within an 80% confidence level thus supporting the foundation of future resource prediction work in this area.},
booktitle = {Proceedings of the 1st Annual Conference on Research in Information Technology},
pages = {25–30},
numpages = {6},
keywords = {prediction methods, demand forecasting},
location = {Calgary, Alberta, Canada},
series = {RIIT '12}
}

@inproceedings{10.1145/3238147.3238214,
author = {Chen, Boyuan and Song, Jian and Xu, Peng and Hu, Xing and Jiang, Zhen Ming (Jack)},
title = {An Automated Approach to Estimating Code Coverage Measures via Execution Logs},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238214},
doi = {10.1145/3238147.3238214},
abstract = {Software testing is a widely used technique to ensure the quality of software systems. Code coverage measures are commonly used to evaluate and improve the existing test suites. Based on our industrial and open source studies, existing state-of-the-art code coverage tools are only used during unit and integration testing due to issues like engineering challenges, performance overhead, and incomplete results. To resolve these issues, in this paper we have proposed an automated approach, called LogCoCo, to estimating code coverage measures using the readily available execution logs. Using program analysis techniques, LogCoCo matches the execution logs with their corresponding code paths and estimates three different code coverage criteria: method coverage, statement coverage, and branch coverage. Case studies on one open source system (HBase) and five commercial systems from Baidu and systems show that: (1) the results of LogCoCo are highly accurate (&gt;96% in seven out of nine experiments) under a variety of testing activities (unit testing, integration testing, and benchmarking); and (2) the results of LogCoCo can be used to evaluate and improve the existing test suites. Our collaborators at Baidu are currently considering adopting LogCoCo and use it on a daily basis.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {305–316},
numpages = {12},
keywords = {empirical studies, logging code, test coverage, software testing, software maintenance},
location = {Montpellier, France},
series = {ASE 2018}
}

@article{10.1145/2795235,
author = {Bhowmik, Tanmay and Niu, Nan and Singhania, Prachi and Wang, Wentao},
title = {On the Role of Structural Holes in Requirements Identification: An Exploratory Study on Open-Source Software Development},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2795235},
doi = {10.1145/2795235},
abstract = {Requirements identification is a human-centric activity that involves interaction among multiple stakeholders. Traditional requirements engineering (RE) techniques addressing stakeholders’ social interaction are mainly part of a centralized process intertwined with a specific phase of software development. However, in open-source software (OSS) development, stakeholders’ social interactions are often decentralized, iterative, and dynamic. Little is known about new requirements identification in OSS and the stakeholders’ organizational arrangements supporting such an activity. In this article, we investigate the theory of structural hole from the context of contributing new requirements in OSS projects. Structural hole theory suggests that stakeholders positioned in the structural holes in their social network are able to produce new ideas. In this study, we find that structural hole positions emerge in stakeholders’ social network and these positions are positively related to contributing a higher number of new requirements. We find that along with structural hole positions, stakeholders’ role is also an important part in identifying new requirements. We further observe that structural hole positions evolve over time, thereby identifying requirements to realize enriched features. Our work advances the fundamental understanding of the RE process in a decentralized environment and opens avenues for improved techniques supporting this process.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {10},
numpages = {30},
keywords = {structural hole, brokerage, social information foraging theory, open-source requirements engineering, stakeholders’ social network, Requirements identification, social capital}
}

@inproceedings{10.1145/3358961.3358962,
author = {Escalante, Maria Alejandra Luj\'{a}n and B\"{u}scher, Monika and Petersen, Katrina and Kerasidou, Xaroula and Gradinar, Adrian and Alter, Hayley},
title = {IsITethical? Board Game: Playing with Speculative Ethics of IT Innovation in Disaster and Risk Management},
year = {2019},
isbn = {9781450376792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358961.3358962},
doi = {10.1145/3358961.3358962},
abstract = {This research presents the design of a board game that explores Ethical, Legal and Social Implications (ELSI) of Information Technologies (IT) for the Disaster and Risk Management (DRM) domain. The game aims to support circumspect discussions of issues emerging at cross-border, cross-sector, interoperation and data management such as privacy, trust, accountability, non-discrimination, and security. IsITethical? board game is both a tool and a process of collaboratively building worlds in which ELSI guidance can be discussed, developed further and applied in preferable near futures. The original idea emerged from the research deliverables of the ELSI work package of SecInCoRe (2014--17), a large-scale EU funded research project concerned with IT for DRM Common Information Spaces (CIS). SecInCoRe's first prototype of the game was further developed and tested in the context of IsITethical?Exchange (2017--2019), a UK Research Innovation funded service co-design project, that explores the idea of ethical impact assessment as a creative collaborative process. This paper offers insights from designing and playing of IsITethical? that begun as a project to make ELSI guidance accessible, but that ultimately opened the way for a much bigger journey, including an online community of practitioners, a living knowledge base, a travelling tool kit for ethical impact assessment service, a methodology to do ethics, and the developing of Ethics through Design framework, that both reformulates ideas of Ethics and ideas of Human-Computer Interaction Design.},
booktitle = {Proceedings of the IX Latin American Conference on Human Computer Interaction},
articleno = {9},
numpages = {8},
keywords = {design research, game design, ELSI, ethics through design},
location = {Panama City, Panama},
series = {CLIHC '19}
}

@inproceedings{10.1145/2593882.2593889,
author = {Mockus, Audris},
title = {Engineering Big Data Solutions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593889},
doi = {10.1145/2593882.2593889},
abstract = { Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles. },
booktitle = {Future of Software Engineering Proceedings},
pages = {85–99},
numpages = {15},
keywords = {Data Quality, Data Science, Analytics, Data Engineering, Game Theory, Operational Data, Statistics},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.5555/3201607.3201769,
author = {Islam, Md Nazmul and Kundu, Sandip},
title = {PMU-Trojan: On Exploiting Power Management Side Channel for Information Leakage},
year = {2018},
publisher = {IEEE Press},
abstract = {Hardware Trojans are malicious, undesired, intentional modifications introduced in an Integrated Circuit (IC) which can be leveraged by a knowledgeable adversary to compromise the security of the IC. Trojans might be designed to modify the functionality of an IC, access sensitive information or even disable or destroy a system. In this paper, we propose PMU-Trojan, a hardware Trojan for leaking confidential information, such as, cryptographic secret key covertly to an adversary. For information leakage by hardware Trojan, we exploit a backdoor created by Power Management Unit (PMU) in Multi Processor System on Chip (MPSoC). PMU is a system block responsible for initiating voltage and the frequency changes to facilitate flexible power management and energy efficiency. It transmits voltage level change request to power supply. In this paper we leverage this facility as an information side-channel to leak information to power-supply co-tenants. While the proposed approach is general and can be applied for any kind of secret information leakage, for the purpose of illustration, in this study, we focus on leaking Advanced Encryption Standard (AES) key. We demonstrate the working principle of this system in Linux environment where a co-tenant thread monitors the voltage level and receives side-channel information from a thread affected by the Trojan. This scheme also defeats Differential Power Analysis (DPA) based Trojan detection due to low information bit rate spread over long duration by a Trojan unit dissipating power at mere pico-Watts level.},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
pages = {709–714},
numpages = {6},
keywords = {advanced encryption standard (AES) key, data center, power management unit (PMU), dynamic voltage and frequency scaling (DVFS), hardware trojan horse (HTH)},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@article{10.1145/1968587.1968592,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1968587.1968592},
doi = {10.1145/1968587.1968592},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {19–25},
numpages = {7}
}

@inproceedings{10.1145/3242887.3242890,
author = {Nielebock, Sebastian and Heum\"{u}ller, Robert and Ortmeier, Frank},
title = {Commits as a Basis for API Misuse Detection},
year = {2018},
isbn = {9781450359757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242887.3242890},
doi = {10.1145/3242887.3242890},
abstract = {Programmers frequently make use of APIs. However, these usages can result in unintended, negative behavior, when developers are not aware of the correct usage or side effects of that API. Detecting those API misuses by means of automatic testing is challenging, as many test suites do not cover this unintended behavior. Instead, API usage patterns are used as specifications to verify the correctness of applications. However, to find meaningful patterns, i.e., those capable of fixing the misuse, the context of the misuse must be considered. Since the developer usually does not know which API is misused, a much larger code section has to be verified against many potential patterns. In this paper, we present a new idea to enhance API misuse detection by means of commits. We discuss the potential of using commits (1) to decrease the size of the code to be considered, (2) to identify suspicious commits, and (3) to contain API usages which can be used to shepherd API specification mining. This paper shows first results on the usability of commits for API misuse detection and some insights into what makes a commit suspicious in terms of exhibiting potential API misuses.},
booktitle = {Proceedings of the 7th International Workshop on Software Mining},
pages = {20–23},
numpages = {4},
keywords = {Commits, Misuses, Application programming interfaces},
location = {Montpellier, France},
series = {SoftwareMining 2018}
}

@inproceedings{10.1145/2212736.2212737,
author = {Grove, David and Tardieu, Olivier and Cunningham, David and Herta, Ben and Peshansky, Igor and Saraswat, Vijay},
title = {A Performance Model for X10 Applications: What's Going on under the Hood?},
year = {2011},
isbn = {9781450307703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212736.2212737},
doi = {10.1145/2212736.2212737},
abstract = {To reliably write high performance code in any programming language, an application programmer must have some understanding of the performance characteristics of the language's core constructs. We call this understanding a performance model for the language. Some aspects of a performance model are fundamental to the programming language and are expected to be true for any plausible implementation of the language. Other aspects are less fundamental and merely represent design choices made in a particular version of the language's implementation.In this paper we present a basic performance model for the X10 programming language. We first describe some performance characteristics that we believe will be generally true of any implementation of the X10 2.2 language specification. We then discuss selected aspects of our implementations of X10 2.2 that have significant implications for the performance model.},
booktitle = {Proceedings of the 2011 ACM SIGPLAN X10 Workshop},
articleno = {1},
numpages = {8},
location = {San Jose, California},
series = {X10 '11}
}

@inproceedings{10.5555/1385089.1385096,
author = {Liu, Vicky and Caelli, William and May, Lauren and Croll, Peter},
title = {Open Trusted Health Informatics Structure (OTHIS)},
year = {2008},
isbn = {9781920682613},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The potential for development and deployment of trusted health information systems (HIS) based upon intrinsically more secure computer system architectures than those in general use, as commodity level systems, in today's marketplace is investigated in this paper. A proposal is made for a viable, trusted architecture for HIS, entitled the "Open Trusted Health Informatics Structure (OTHIS)", based upon a set of separate but connected trusted modules. OTHIS addresses privacy and security requirements at all levels in an HIS. In this paper, we are concerned with the role of trustworthy access control mechanisms in HIS architectures. Our proposed OTHIS architecture gives direction on how trustworthiness in HIS can be achieved.},
booktitle = {Proceedings of the Second Australasian Workshop on Health Data and Knowledge Management - Volume 80},
pages = {35–43},
numpages = {9},
keywords = {security for health systems, information assurance, information security and privacy, access control, mandatory access control, trusted systems},
location = {Wollongong, NSW, Australia},
series = {HDKM '08}
}

@article{10.1145/1880050.1880062,
author = {Zhu, Dakai},
title = {Reliability-Aware Dynamic Energy Management in Dependable Embedded Real-Time Systems},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/1880050.1880062},
doi = {10.1145/1880050.1880062},
abstract = {Recent studies show that voltage scaling, which is an efficient energy management technique, has a direct and negative effect on system reliability because of the increased rate of transient faults (e.g., those induced by cosmic particles). In this article, we propose energy management schemes that explicitly take system reliability into consideration. The proposed reliability-aware energy management schemes dynamically schedule recoveries for tasks to be scaled down to recuperate the reliability loss due to energy management. Based on the amount of available slack, the application size, and the fault rate changes, we analyze when it is profitable to reclaim the slack for energy savings without sacrificing system reliability. Checkpoint technique is further explored to efficiently use the slack. Analytical and simulation results show that the proposed schemes can achieve comparable energy savings as ordinary energy management schemes (which are reliability-ignorant) while preserving system reliability. The ordinary energy management schemes that ignore the effects of voltage scaling on fault rate changes could lead to drastically decreased system reliability.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {26},
numpages = {27},
keywords = {Power management, dynamic voltage scaling}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/335231.335232,
author = {Artigas, Pedro V. and Gupta, Manish and Midkiff, Samuel P. and Moreira, Jos\'{e} E.},
title = {Automatic Loop Transformations and Parallelization for Java},
year = {2000},
isbn = {1581132700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/335231.335232},
doi = {10.1145/335231.335232},
abstract = {From a software engineering perspective, the Java programming language provides an attractive platform for writing numerically intensive applications. A major drawback hampering its widespread adoption in this domain has been its poor performance on numerical codes. This paper describes a prototype Java compiler which demonstrates that it is possible to achieve performance levels approaching those of current state-of-the-art C, C++ and Fortran compilers on numerical codes. We describe a new transformation called alias versioning that takes advantage of the simplicity of pointers in Java. This transformation, combined with other techniques that we have developed, enables the compiler to perform high order loop transformations (for better data locality) and parallelization completely automatically. We believe that our compiler is the first to have such capabilities of optimizing numerical Java codes. We achieve, with Java, between 80 and 100% of the performance of highly optimized Fortran code in a variety of benchmarks. Furthermore, the automatic parallelization achieves speedups of up to 3.8 on four processors. Combining this compiler technology with packages containing the features expected by programmers of numerical applications would enable Java to become a serious contender for implementing new numerical applications.},
booktitle = {Proceedings of the 14th International Conference on Supercomputing},
pages = {1–10},
numpages = {10},
location = {Santa Fe, New Mexico, USA},
series = {ICS '00}
}

@inproceedings{10.1145/1352694.1352712,
author = {Dasilva, Antonio and Mart\'{\i}nez, Jos\'{e}-F and L\'{o}pez, Lourdes and Garc\'{\i}a, Ana-B and Redondo, Luis},
title = {Exhaustif®: A Fault Injection Tool for Distributed Heterogeneous Embedded Systems},
year = {2007},
isbn = {9781595935984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352694.1352712},
doi = {10.1145/1352694.1352712},
abstract = {This paper presents Exhaustif®, a SWIFI fault injection tool for fault tolerance verification and the validation of embedded software in distributed heterogeneous systems. Exhaustif® mainly consists of two parts: EEM and FIK. Exhaustif® Executive Manager (EEM) is a GUI Java application to define the fault injection campaign that uses a SQL database to save the test results obtained from the System under Test (SUT) in order to carry out a post injection data analysis. FIK is under the command of EEM to cary out fault injections in applications running under diverse operating systems using pure SWIFI techniques. Exhaustif® carries out floating point register and memory corruptions using temporary triggers and uses an optimized routine interception mechanism to carry out argument and return value corruption with a minimal time overhead. Two experimental Fault Injector Kernels (FIK) under the RTEMS operating system for an EADS-Astrium SPARC ERC32-based MCM processor board and i386 standard PC mainboard have been developed.},
booktitle = {Proceedings of the 2007 Euro American Conference on Telematics and Information Systems},
articleno = {17},
numpages = {8},
keywords = {fault injection techniques, SWIFI, fault tolerance, distributed embedded systems},
location = {Faro, Portugal},
series = {EATIS '07}
}

@article{10.1145/1709424.1709457,
author = {Cooper, Stephen and Nickell, Christine and Piotrowski, Victor and Oldfield, Brenda and Abdallah, Ali and Bishop, Matt and Caelli, Bill and Dark, Melissa and Hawthorne, E. K. and Hoffman, Lance and P\'{e}rez, Lance C. and Pfleeger, Charles and Raines, Richard and Schou, Corey and Brynielsson, Joel},
title = {An Exploration of the Current State of Information Assurance Education},
year = {2010},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/1709424.1709457},
doi = {10.1145/1709424.1709457},
abstract = {Information Assurance and computer security are serious worldwide concerns of governments, industry, and academia. Computer security is one of the three new focal areas of the ACM/IEEE's Computer Science Curriculum update in 2008. This ACM/IEEE report describes, as the first of its three recent trends, "the emergence of security as a major area of concern." The importance of Information Assurance and Information Assurance education is not limited to the United States. Other nations, including the United Kingdom, Australia, New Zealand, Canada, and other members from NATO countries and the EU, have inquired as to how they may be able to establish Information Assurance education programs in their own country.The goal of this document is to explore the space of various existing Information Assurance educational standards and guidelines, and how they may serve as a basis for helping to define the field of Information Assurance. It was necessary for this working group to study what has been done for other areas of computing. For example, computer science (CS 2008 and associate-degree CS 2009), information technology (IT 2008), and software engineering (SE 2004), all have available curricular guidelines.In its exploration of existing government, industry, and academic Information Assurance guidelines and standards, as well as in its discovery of what guidance is being provided for other areas of computing, the working group has developed this paper as a foundation, or a starting point, for creating an appropriate set of guidelines for Information Assurance education. In researching the space of existing guidelines and standards, several challenges and opportunities to Information Assurance education were discovered. These are briefly described and discussed, and some next steps suggested.},
journal = {SIGCSE Bull.},
month = {jan},
pages = {109–125},
numpages = {17},
keywords = {information assurance, IA, guidelines, standards, education}
}

@inproceedings{10.1145/2145204.2145408,
author = {Phillips, Shaun and Ruhe, Guenther and Sillito, Jonathan},
title = {Information Needs for Integration Decisions in the Release Process of Large-Scale Parallel Development},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145408},
doi = {10.1145/2145204.2145408},
abstract = {Version control branching allows an organization to parallelize its development efforts. Releasing a software system developed in this manner requires release managers, and other project stakeholders, to make decisions about how to integrate the branched work. This group decision-making process becomes very complex in the case of large-scale parallel development. To better understand the information needs of release managers in this context, we conducted an interview study at a large software company. Our analysis of the interviews provides a view into how release managers make integration decisions, organized around ten key factors. Based on these factors, we discuss specific information needs for release managers and how the needs can be met in future work.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1371–1380},
numpages = {10},
keywords = {integration, team meetings, parallel development, decision support, version control, release management},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/3313831.3376309,
author = {Kornfield, Rachel and Zhang, Renwen and Nicholas, Jennifer and Schueller, Stephen M. and Cambo, Scott A. and Mohr, David C. and Reddy, Madhu},
title = {"Energy is a Finite Resource": Designing Technology to Support Individuals across Fluctuating Symptoms of Depression},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376309},
doi = {10.1145/3313831.3376309},
abstract = {While the HCI field increasingly examines how digital tools can support individuals in managing mental health conditions, it remains unclear how these tools can accommodate these conditions' temporal aspects. Based on weekly interviews with five individuals with depression, conducted over six weeks, this study identifies design opportunities and challenges related to extending technology-based support across fluctuating symptoms. Our findings suggest that participants perceive events and contexts in daily life to have marked impact on their symptoms. Results also illustrate that ebbs and flows in symptoms profoundly affect how individuals practice depression self-management. While digital tools often aim to reach individuals while they feel depressed, we suggest they should also engage individuals when they are less symptomatic, leveraging their energy and motivation to build habits, establish plans and goals, and generate and organize content to prepare for symptom onset.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {depression, temporality, tailoring, digital interventions, mental health, motivation, personalization},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/1062455.1062506,
author = {Mockus, Audris and Zhang, Ping and Li, Paul Luo},
title = {Predictors of Customer Perceived Software Quality},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062506},
doi = {10.1145/1062455.1062506},
abstract = {Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {225–233},
numpages = {9},
keywords = {quality, modeling, metrics},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1145/3408896,
author = {Holmes, Josie and Ahmed, Iftekhar and Brindescu, Caius and Gopinath, Rahul and Zhang, He and Groce, Alex},
title = {Using Relative Lines of Code to Guide Automated Test Generation for Python},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3408896},
doi = {10.1145/3408896},
abstract = {Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {28},
numpages = {38},
keywords = {Automated test generation, testing heuristics, static code metrics}
}

@inproceedings{10.1145/2998181.2998336,
author = {Liu, Jiaxin and Weitzman, Elissa R. and Chunara, Rumi},
title = {Assessing Behavior Stage Progression From Social Media Data},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998336},
doi = {10.1145/2998181.2998336},
abstract = {Important work rooted in psychological theory posits that health behavior change occurs through a series of discrete stages. Our work builds on the field of social computing by identifying how social media data can be used to resolve behavior stages at high resolution (e.g. hourly/daily) for key population subgroups and times. In essence this approach opens new opportunities to advance psychological theories and better understand how our health is shaped based on the real, dynamic, and rapid actions we make every day. To do so, we bring together domain knowledge and machine learning methods to form a hierarchical classification of Twitter data that resolves different stages of behavior. We identify and examine temporal patterns of the identified stages, with alcohol as a use case (planning or looking to drink, currently drinking, and reflecting on drinking). Known seasonal trends are compared with findings from our methods. We discuss the potential health policy implications of detecting high frequency behavior stages.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1320–1333},
numpages = {14},
keywords = {hierarchical classification, behavior, social media, natural language processing, health},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/2433140.2433149,
author = {de S\'{a}, Al\'{\i}rio Santos and Silva Freitas, Allan Edgard and de Ara\'{u}jo Mac\^{e}do, Raimundo Jos\'{e}},
title = {Adaptive Request Batching for Byzantine Replication},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2433140.2433149},
doi = {10.1145/2433140.2433149},
abstract = {Castro and Liskov proposed in 1999 a successful solution for byzantine fault-tolerant replication, named PBFT, which overcame performance drawbacks of earlier byzantine faulttolerant replication protocols. Other proposals extended PBFT with further optimizations, improving PBFT performance in certain conditions. One of the key optimizations of PBFT-based protocols is the use a request batching mechanism. If the target distributed system is dynamic, that is, if its underlying characteristics change dynamically, such as workload, channel QoS, network topology, etc., the configuration of the request batching mechanism must follow the dynamics of the system or it may not yield the desired performance improvement. This paper addresses this challenge by proposing an innovative solution to the dynamic configuration of request batching parameters inspired on feedback control theory. In order to evaluate its efficiency, the proposed solution is simulated in various scenarios and compared with the original version used in the PBFT-family protocols.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jan},
pages = {35–42},
numpages = {8},
keywords = {dependability, self-configuration, byzantine fault tolerance, autonomic computing, state machine replication, simulation}
}

@proceedings{10.1145/2970276,
title = {ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2525314.2525347,
author = {Ray, Suprio and Simion, Bogdan and Brown, Angela Demke and Johnson, Ryan},
title = {A Parallel Spatial Data Analysis Infrastructure for the Cloud},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525347},
doi = {10.1145/2525314.2525347},
abstract = {Spatial data analysis applications are emerging from a wide range of domains such as building information management, environmental assessments and medical imaging. Time-consuming computational geometry algorithms make these applications slow, even for medium-sized datasets. At the same time, there is a rapid expansion in available processing cores, through multicore machines and Cloud computing. The confluence of these trends demands effective parallelization of spatial query processing. Unfortunately, traditional parallel spatial databases are ill-equipped to deal with the performance heterogeneity that is common in the Cloud.We introduce Niharika, a parallel spatial data analysis infrastructure that exploits all available cores in a heterogeneous cluster. Niharika first uses a declustering technique that creates balanced spatial partitions. Then, Niharika adapts to performance heterogeneity and processing skew in the spatial dataset using dynamic load-balancing. We evaluate Niharika with three load-balancing algorithms and two different spatial datasets (both from TIGER) using Amazon EC2 instances. Niharika adapts to the performance heterogeneity in the EC2 nodes, thereby achieving excellent speedups (e.g., 63.6X using 64 cores on 16 4-core EC2 nodes, in the best case) and outperforming an approach that does not adapt.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {284–293},
numpages = {10},
keywords = {spatial join, cloud, performance heterogeneity, load balancing},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@inbook{10.1145/3173574.3173836,
author = {Thomas, Tyler W. and Tabassum, Madiha and Chu, Bill and Lipford, Heather},
title = {Security During Application Development: An Application Security Expert Perspective},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173836},
abstract = {Many of the security problems that people face today, such as security breaches and data theft, are caused by security vulnerabilities in application source code. Thus, there is a need to understand and improve the experiences of those who can prevent such vulnerabilities in the first place - software developers as well as application security experts. Several studies have examined developers' perceptions and behaviors regarding security vulnerabilities, demonstrating the challenges they face in performing secure programming and utilizing tools for vulnerability detection. We expand upon this work by focusing on those primarily responsible for application security - security auditors. In an interview study of 32 application security experts, we examine their views on application security processes, their workflows, and their interactions with developers in order to further inform the design of tools and processes to improve application security.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}

@article{10.1145/1640162.1655274,
author = {Schaefer, Robert},
title = {The Epistemology of Computer Security},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1640162.1655274},
doi = {10.1145/1640162.1655274},
abstract = {This paper studies computer security from first principles. The basic questions "Why?", "How do we know what we know?" and "What are the implications of what we believe?"},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {8–10},
numpages = {3}
}

@inproceedings{10.1145/1138063.1138074,
author = {Klein, Florian and Tichy, Matthias},
title = {Building Reliable Systems Based on Self-Organizing Multi-Agent Systems},
year = {2006},
isbn = {1595933956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138063.1138074},
doi = {10.1145/1138063.1138074},
abstract = {High reliability and availability are of utmost importance to the majority of today's software-based systems. Typically, some kind of redundancy is used in order to achieve high reliability and availability in case of faults. Dynamic redundancy setups employ backup systems which take over as soon as the primary system fails. Additionally, central fault tolerance management systems are often used to guide the replication process. In this paper, we present an approach which relies on emergent fault-tolerance. The desired behavior is the result of local reconfigurations of self-organizing agents. This allows us to abandon the classical distinction of primary and backup systems as well as the need for a central management system. Our experimental results show the benefit of our approach.},
booktitle = {Proceedings of the 2006 International Workshop on Software Engineering for Large-Scale Multi-Agent Systems},
pages = {51–58},
numpages = {8},
keywords = {availability, agent, multi-agent systems, dynamic task allocation, reliability},
location = {Shanghai, China},
series = {SELMAS '06}
}

