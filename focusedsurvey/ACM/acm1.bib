@inproceedings{10.1145/3377811.3380361,
author = {Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia},
title = {CC2Vec: Distributed Representations of Code Changes},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380361},
doi = {10.1145/3377811.3380361},
abstract = {Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {518–529},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2556288.2557422,
author = {Trafton, J. Gregory and Ratwani, Raj M.},
title = {The Law of Unintended Consequences: The Case of External Subgoal Support},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557422},
doi = {10.1145/2556288.2557422},
abstract = {Many interfaces have been designed to prevent or reduce errors. These interfaces may, in fact, reduce the error rate of specific error classes, but may also have unintended consequences. In this paper, we show a series of studies where a better interface did not reduce the number of errors but instead shifted errors from one error class (omissions) to another error class (perseverations). We also show that having access to progress tracking (a progress bar) does not reduce the number of errors. We propose and demonstrate a solution -- a predictive error system -- that reduces errors based on the error class, not on the type of interface.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1767–1776},
numpages = {10},
keywords = {interface subgoal support, computer human interaction, error prediction, progress tracking},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3106237.3117766,
author = {Volf, Zahy and Shmueli, Edi},
title = {Screening Heuristics for Project Gating Systems},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117766},
doi = {10.1145/3106237.3117766},
abstract = { Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {872–877},
numpages = {6},
keywords = {Project Gating, Machine Learning, Continuous Integration},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3204493.3204536,
author = {Barz, Michael and Daiber, Florian and Sonntag, Daniel and Bulling, Andreas},
title = {Error-Aware Gaze-Based Interfaces for Robust Mobile Gaze Interaction},
year = {2018},
isbn = {9781450357067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204493.3204536},
doi = {10.1145/3204493.3204536},
abstract = {Gaze estimation error can severely hamper usability and performance of mobile gaze-based interfaces given that the error varies constantly for different interaction positions. In this work, we explore error-aware gaze-based interfaces that estimate and adapt to gaze estimation error on-the-fly. We implement a sample error-aware user interface for gaze-based selection and different error compensation methods: a na\"{\i}ve approach that increases component size directly proportional to the absolute error, a recent model by Feit et al. that is based on the two-dimensional error distribution, and a novel predictive model that shifts gaze by a directional error estimate. We evaluate these models in a 12-participant user study and show that our predictive model significantly outperforms the others in terms of selection rate, particularly for small gaze targets. These results underline both the feasibility and potential of next generation error-aware gaze-based user interfaces.},
booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications},
articleno = {24},
numpages = {10},
keywords = {eye tracking, error-aware, error model, mobile interaction, gaze interaction},
location = {Warsaw, Poland},
series = {ETRA '18}
}

@inproceedings{10.1145/3304079.3310288,
author = {Pirasteh, Parivash and Nowaczyk, Slawomir and Pashami, Sepideh and L\"{o}wenadler, Magnus and Thunberg, Klas and Ydreskog, Henrik and Berck, Peter},
title = {Interactive Feature Extraction for Diagnostic Trouble Codes in Predictive Maintenance: A Case Study from Automotive Domain},
year = {2019},
isbn = {9781450362962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304079.3310288},
doi = {10.1145/3304079.3310288},
abstract = {Predicting future maintenance needs of equipment can be addressed in a variety of ways. Methods based on machine learning approaches provide an interesting platform for mining large data sets to find patterns that might correlate with a given fault. In this paper, we approach predictive maintenance as a classification problem and use Random Forest to separate data readouts within a particular time window into those corresponding to faulty and non-faulty component categories. We utilize diagnostic trouble codes (DTCs) as an example of event-based data, and propose four categories of features that can be derived from DTCs as a predictive maintenance framework. We test the approach using large-scale data from a fleet of heavy duty trucks, and show that DTCs can be used within our framework as indicators of imminent failures in different components.},
booktitle = {Proceedings of the Workshop on Interactive Data Mining},
articleno = {4},
numpages = {10},
keywords = {feature extraction, Predictive maintenance, failure detection, diagnostic trouble codes},
location = {Melbourne, VIC, Australia},
series = {WIDM'19}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2020},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = { Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {19th Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {NASA datasets, SHAP values, explainable models, software defects},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS'20}
}

@inproceedings{10.1145/2901739.2901740,
author = {Guo, Jin and Rahimi, Mona and Cleland-Huang, Jane and Rasin, Alexander and Hayes, Jane Huffman and Vierhauser, Michael},
title = {Cold-Start Software Analytics},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901740},
doi = {10.1145/2901739.2901740},
abstract = {Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as "who is the expert?," "which classes are fault prone?," or even "who are the domain experts for these fault-prone classes?" Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {142–153},
numpages = {12},
keywords = {cold-start, configuration, software analytics},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3338906.3338944,
author = {Najafi, Armin and Rigby, Peter C. and Shang, Weiyi},
title = {Bisecting Commits and Modeling Commit Risk during Testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338944},
doi = {10.1145/3338906.3338944},
abstract = {Software testing is one of the costliest stages in the software development life cycle. One approach to reducing the test execution cost is to group changes and test them as a batch (i.e. batch testing). However, when tests fail in a batch, commits in the batch need to be re-tested to identify the cause of the failure, i.e. the culprit commit. The re-testing is typically done through bisection (i.e. a binary search through the commits in a batch). Intuitively, the effectiveness of batch testing highly depends on the size of the batch. Larger batches require fewer initial test runs, but have a higher chance of a test failure that can lead to expensive test re-runs to find the culprit. We are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry. In this work, we first conduct empirical studies on the effectiveness of batch testing in three large-scale industrial software systems at Ericsson. Using 9 months of testing data, we simulate batch sizes from 1 to 20 and find the most cost-effective BatchSize for each project. Our results show that batch testing saves 72% of test executions compared to testing each commit individually. In a second simulation, we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects. We model the degree of flakiness for each project and find that test flakiness reduces the cost savings to 42%. In a third simulation, we guide bisection to reduce the likelihood of batch-testing failures. We model the riskiness of each commit in a batch using a bug model and a test execution history model. The risky commits are tested individually, while the less risky commits are tested in a single larger batch. Culprit predictions with our approach reduce test executions up to 9% compared to Ericsson’s current bisection approach. The results have been adopted by developers at Ericsson and a tool to guide bisection is in the process of being added to Ericsson’s continuous integration pipeline.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {279–289},
numpages = {11},
keywords = {Empirical software engineering, Batching and biseciton, Culprit risk models, Software testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/1985793.1986007,
author = {Shihab, Emad},
title = {Pragmatic Prioritization of Software Quality Assurance Efforts},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986007},
doi = {10.1145/1985793.1986007},
abstract = {A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1106–1109},
numpages = {4},
keywords = {software metrics, change risk, unit testing},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2393596.2393670,
author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming},
title = {An Industrial Study on the Risk of Software Changes},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393670},
doi = {10.1145/2393596.2393670},
abstract = {Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {62},
numpages = {11},
keywords = {bug inducing changes, code metrics, change risk, change metrics},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.5555/2616606.2616682,
author = {Chen, Hu and Roy, Sanghamitra and Chakraborty, Koushik},
title = {DARP: Dynamically Adaptable Resilient Pipeline Design in Microprocessors},
year = {2014},
isbn = {9783981537024},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {In this paper, we demonstrate that the sensitized path delays in various microprocessor pipe stages exhibit intriguing temporal and spatial variations during the execution of real world applications. To effectively exploit these delay variations, we propose Dynamically Adaptable Resilient Pipeline (DARP)--a series of runtime techniques to boost power performance efficiency and fault tolerance in a pipelined microprocessor. DARP employs early error prediction to avoid a major portion of the timing errors. Using a rigorous circuit-architectural infrastructure, we demonstrate substantial improvements in the performance (9.4--20%) and energy efficiency (6.4--27.9%), compared to state-of-the-art techniques.},
booktitle = {Proceedings of the Conference on Design, Automation &amp; Test in Europe},
articleno = {62},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '14}
}

@article{10.1145/3417958,
author = {Hurley, Nathan C. and Spatz, Erica S. and Krumholz, Harlan M. and Jafari, Roozbeh and Mortazavi, Bobak J.},
title = {A Survey of Challenges and Opportunities in Sensing and Analytics for Risk Factors of Cardiovascular Disorders},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3417958},
doi = {10.1145/3417958},
abstract = {Cardiovascular disorders cause nearly one in three deaths in the United States. Short- and long-term care for these disorders is often determined in short-term settings. However, these decisions are made with minimal longitudinal and long-term data. To overcome this bias towards data from acute care settings, improved longitudinal monitoring for cardiovascular patients is needed. Longitudinal monitoring provides a more comprehensive picture of patient health, allowing for informed decision making. This work surveys sensing and machine learning in the field of remote health monitoring for cardiovascular disorders. We highlight three needs in the design of new smart health technologies: (1) need for sensing technologies that track longitudinal trends of the cardiovascular disorder despite infrequent, noisy, or missing data measurements; (2) need for new analytic techniques designed in a longitudinal, continual fashion to aid in the development of new risk prediction techniques and in tracking disease progression; and (3) need for personalized and interpretable machine learning techniques, allowing for advancements in clinical decision making. We highlight these needs based upon the current state of the art in smart health technologies and analytics. We then discuss opportunities in addressing these needs for development of smart health technologies for the field of cardiovascular disorders and care.},
journal = {ACM Trans. Comput. Healthcare},
month = {dec},
articleno = {9},
numpages = {42},
keywords = {longitudinal monitoring, sensors, Cardiovascular disease, smart health, patient analytics, cardiovascular risk factors}
}

@inproceedings{10.1145/2971648.2971750,
author = {Bae, Sangwon and Dey, Anind K. and Low, Carissa A.},
title = {Using Passively Collected Sedentary Behavior to Predict Hospital Readmission},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971750},
doi = {10.1145/2971648.2971750},
abstract = {Hospital readmissions are a major problem facing health care systems today, costing Medicare alone US$26 billion each year. Being readmitted is associated with significantly shorter survival, and is often preventable. Predictors of readmission are still not well understood, particularly those under the patient's control: behavioral risk factors. Our work evaluates the ability of behavioral risk factors, specifically Fitbit-assessed behavior, to predict readmission for 25 postsurgical cancer inpatients. Our results show that sum of steps, maximum sedentary bouts, frequency, and low breaks in sedentary times during waking hours are strong predictors of readmission. We built two models for predicting readmissions: Steps-only and Behavioral model that adds information about sedentary behaviors. The Behavioral model (88.3%) outperforms the Steps-only model (67.1%), illustrating the value of passively collected information about sedentary behaviors. Indeed, passive monitoring of behavior data, i.e., mobility, after major surgery creates an opportunity for early risk assessment and timely interventions.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {616–621},
numpages = {6},
keywords = {wearable tracker, physical activity, colorectal cancer surgery, sedentary behavior, healthcare outcomes},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/1026533.1026569,
author = {Kienle, Holger M. and German, Daniel and Tilley, Scott and M\"{u}ller, Hausi A.},
title = {Intellectual Property Aspects of Web Publishing},
year = {2004},
isbn = {1581138091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1026533.1026569},
doi = {10.1145/1026533.1026569},
abstract = {This paper addresses how intellectual property affects the Web in general, and content publishing on the Web in particular. Before its commercialization, the Web was perceived as being free and unregulated; this assumption is no longer true. Nowadays, content providers need to know which practices on the Web can result in potential legal problems. The vast majority of Web sites are developed by individual such as technical writers or graphic artists, and small organizations, which receive limited or no legal advice. As a result, these Web sites are developed with little or no regard to the legal constraints of intellectual property law. In order to help this group of people, the paper tries to answer the following question: What are the (typical) legal issues for Web content providers to watch out for? This paper gives an overview of these legal issues for intellectual property (i.e., copyrights, patents, and trademarks) and discusses relevant law cases. As a first step towards a more formal risk assessment of intellectual property issues, we introduce a maturity model that captures a Web site's intellectual property coverage with five different maturity levels.},
booktitle = {Proceedings of the 22nd Annual International Conference on Design of Communication: The Engineering of Quality Documentation},
pages = {136–144},
numpages = {9},
keywords = {open content, hypermedia, link law, copyright, intellectual property, world wide web, patents, trademarks},
location = {Memphis, Tennessee, USA},
series = {SIGDOC '04}
}

@inproceedings{10.1145/2897695.2897696,
author = {Ortu, Marco and Destefanis, Giuseppe and Swift, Stephen and Marchesi, Michele},
title = {Measuring High and Low Priority Defects on Traditional and Mobile Open Source Software},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897696},
doi = {10.1145/2897695.2897696},
abstract = {Software defects are the major cause for system failures. To effectively design tools and provide support for detecting and recovering from software failures, requires a deep understanding of defect features. In this paper we present an analysis of defect characteristics in two different open source software development domains: Mobile and Traditional. Our attention is focused on measuring the differences between High-Priority and Low-Priority defects. High or Low priority of a given defect is decided by a developer when creating a bug report for an issue tracking system. We sampled hundreds of real world bugs in hundreds of large and representative open-source projects. We used natural language text classification techniques to automatically analyse roughly 700,000 bug reports from the Bugzilla, Jira and Google Issues issue tracking systems. Results show that there are differences between High-Priority and Low-Priority defects classification in Mobile and Traditional development domains.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {1–7},
numpages = {7},
keywords = {data mining, bug categorisation, bug reports},
location = {Austin, Texas},
series = {WETSoM '16}
}

@article{10.1145/2755558,
author = {Chen, Hu and Roy, Sanghamitra and Chakraborty, Koushik},
title = {DARP-MP: Dynamically Adaptable Resilient Pipeline Design in Multicore Processors},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/2755558},
doi = {10.1145/2755558},
abstract = {In this article, we demonstrate that the sensitized path delays in various microprocessor pipe stages exhibit intriguing temporal and spatial variations during the execution of real-world applications. To effectively exploit these delay variations, we propose dynamically adaptable resilient pipeline (DARP)—a series of runtime techniques to boost power-performance efficiency and fault tolerance in a pipelined microprocessor. DARP employs early error prediction to avoid a major portion of the timing errors. We combine DARP with the state-of-art topologically homogeneous and power-performance heterogeneous (THPH) architecture to build up a new frontier for the energy efficiency of multicore processors (DARP-MP). Using a rigorous circuit-architectural infrastructure, we demonstrate that DARP substantially improves the multicore processor performance (9.4--20%) and energy efficiency (10--28.6%) compared to state-of-the-art techniques. The energy-efficiency improvements of DARP-MP are 42% and 49.9% compared against the original THPH and another state-of-art multicore power management scheme, respectively.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {dec},
articleno = {3},
numpages = {21},
keywords = {dynamic adjustment, multicore processor, Microprocessor pipeline, sensitized delay variation}
}

@inproceedings{10.1145/2810146.2810152,
author = {An, Le and Khomh, Foutse},
title = {An Empirical Study of Crash-Inducing Commits in Mozilla Firefox},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810152},
doi = {10.1145/2810146.2810152},
abstract = {Software crashes are feared by software organisations and end users. Many software organisations have embedded automatic crash reporting tools in their software systems to help development teams track and fix crash-related bugs. Previous techniques, which focus on the triaging of crash-types and crash-related bugs, can help software practitioners increase their debugging efficiency on crashes. But, these techniques can only be applied after the crashes occurred and already affected a large population of users. To help software organisations detect and address crash-prone code early, we conduct a case study of commits that would lead to crashes, called "crash-inducing commits", in Mozilla Firefox. We found that crash-inducing commits are often submitted by developers with less experience. Developers perform more addition and deletion of lines of code in crash-inducing commits. We built predictive models to help software practitioners detect and fix crash-prone bugs early on. Our predictive models achieve a precision of 61.4% and a recall of 95.0%. Software organisations can use our proposed predictive models to track and fix crash-prone commits early on before they negatively impact users; increasing bug fixing efficiency and user-perceived quality.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {5},
numpages = {10},
keywords = {mining software repositories, prediction model, Crash analysis, bug triaging},
location = {Beijing, China},
series = {PROMISE '15}
}

@inproceedings{10.1145/1835698.1835741,
author = {Tan, Yongmin and Gu, Xiaohui and Wang, Haixun},
title = {Adaptive System Anomaly Prediction for Large-Scale Hosting Infrastructures},
year = {2010},
isbn = {9781605588889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835698.1835741},
doi = {10.1145/1835698.1835741},
abstract = {Large-scale hosting infrastructures require automatic system anomaly management to achieve continuous system operation. In this paper, we present a novel adaptive runtime anomaly prediction system, called ALERT, to achieve robust hosting infrastructures. In contrast to traditional anomaly detection schemes, ALERT aims at raising advance anomaly alerts to achieve just-in-time anomaly prevention. We propose a novel context-aware anomaly prediction scheme to improve prediction accuracy in dynamic hosting infrastructures. We have implemented the ALERT system and deployed it on several production hosting infrastructures such as IBM System S stream processing cluster and PlanetLab. Our experiments show that ALERT can achieve high prediction accuracy for a range of system anomalies and impose low overhead to the hosting infrastructure.},
booktitle = {Proceedings of the 29th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
pages = {173–182},
numpages = {10},
keywords = {anomaly prediction, context-aware prediction model},
location = {Zurich, Switzerland},
series = {PODC '10}
}

@article{10.1007/s00779-012-0573-7,
author = {Ryu, Dong Woo and Kang, Kyung Jin and Yeo, Sang Soo and Park, Sang Oh},
title = {Generating Knowledge for the Identification of Device Failure Causes and the Prediction of the Times-to-Failure in u-Healthcare Environments},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {7},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-012-0573-7},
doi = {10.1007/s00779-012-0573-7},
abstract = {The healthcare industry depends on a large number of medical devices to perform many of its functions, so a considerable amount of effort is spent to deal with failures occurred in medical devices. This paper proposes a method that generates knowledge used to identify the causes of medical device failures and to predict the times-to-failure (i.e., a period during which a medical device operates without failure). To generate knowledge for failure cause identification, morphemes of the failure data in the existing database are analyzed and similar failures (symptoms and causes) are grouped based on the similarity of symptoms. To generate knowledge for the prediction of the times-to-failure, the Weibull distribution parameters are estimated based on a device's previous failure dates. The experiment results show that the proposed method has 69 % accuracy in identifying the cause of failure and 83 % accuracy in predicting the times-to-failure. The proposed method enables medical device users to quickly identify the cause of failure when their devices have problems, thereby reducing the cost of failure. With the predicted time to failure, it is possible to have devices (or device parts) ready just in time for replacement. This leads to decreased inventory costs.},
journal = {Personal Ubiquitous Comput.},
month = {oct},
pages = {1383–1394},
numpages = {12},
keywords = {u-Healthcare, Fault period prediction, Ubiquitous, Fault analysis}
}

@inproceedings{10.1145/1146269.1146285,
author = {Beznosov, Konstantin (Kosta)},
title = {Flooding and Recycling Authorizations},
year = {2005},
isbn = {1595933174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1146269.1146285},
doi = {10.1145/1146269.1146285},
abstract = {The request-response paradigm used for access control solutions commonly leads to point-to-point (PTP) architectures, with security enforcement logic obtaining decisions from authorization servers through remote procedure calls. In massive-scale and complex enterprises, PTP authorization architectures result in fragile and inefficient solutions. They also fail to exploit virtually free CPU resources and network bandwidth. This paper proposes leveraging publish-subscribe architectures for increased reliability and efficiency by flooding delivery channels with speculatively pre-computed authorizations and actively recycling them on a just-in-time basis.},
booktitle = {Proceedings of the 2005 Workshop on New Security Paradigms},
pages = {67–72},
numpages = {6},
keywords = {access control, authorization, authorization flooding, authorization recycling, junk authorizations, publish-subscribe},
location = {Lake Arrowhead, California},
series = {NSPW '05}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {56},
numpages = {36},
keywords = {review, Software vulnerability analysis, survey, data-mining, software security, software vulnerability discovery, machine-learning}
}

@inproceedings{10.1145/349299.349306,
author = {Cierniak, Micha\l{} and Lueh, Guei-Yuan and Stichnoth, James M.},
title = {Practicing JUDO: Java under Dynamic Optimizations},
year = {2000},
isbn = {1581131992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/349299.349306},
doi = {10.1145/349299.349306},
abstract = {A high-performance implementation of a Java  Virtual Machine (JVM) consists of efficient implementation of Just-In-Time (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC).  These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA-32 as the hardware platform, but the optimizations can be generalized to other architectures.},
booktitle = {Proceedings of the ACM SIGPLAN 2000 Conference on Programming Language Design and Implementation},
pages = {13–26},
numpages = {14},
location = {Vancouver, British Columbia, Canada},
series = {PLDI '00}
}

@article{10.1145/358438.349306,
author = {Cierniak, Micha\l{} and Lueh, Guei-Yuan and Stichnoth, James M.},
title = {Practicing JUDO: Java under Dynamic Optimizations},
year = {2000},
issue_date = {May 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/358438.349306},
doi = {10.1145/358438.349306},
abstract = {A high-performance implementation of a Java  Virtual Machine (JVM) consists of efficient implementation of Just-In-Time (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC).  These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA-32 as the hardware platform, but the optimizations can be generalized to other architectures.},
journal = {SIGPLAN Not.},
month = {may},
pages = {13–26},
numpages = {14}
}

@inproceedings{10.1145/2970276.2970359,
author = {Wen, Ming and Wu, Rongxin and Cheung, Shing-Chi},
title = {Locus: Locating Bugs from Software Changes},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970359},
doi = {10.1145/2970276.2970359},
abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {262–273},
numpages = {12},
keywords = {software changes, information retrieval, bug localization, software analytics},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3372297.3417248,
author = {Ahmed, Salman and Xiao, Ya and Snow, Kevin Z. and Tan, Gang and Monrose, Fabian and Yao, Danfeng (Daphne)},
title = {Methodologies for Quantifying (Re-)Randomization Security and Timing under JIT-ROP},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417248},
doi = {10.1145/3372297.3417248},
abstract = {Just-in-time return-oriented programming (JIT-ROP) allows one to dynamically discover instruction pages and launch code reuse attacks, effectively bypassing most fine-grained address space layout randomization (ASLR) protection. However, in-depth questions regarding the impact of code (re-)randomization on code reuse attacks have not been studied. For example, how would one compute the re-randomization interval effectively by considering the speed of gadget convergence to defeat JIT-ROP attacks? ; how do starting pointers in JIT-ROP impact gadget availability and gadget convergence time? ; what impact do fine-grained code randomizations have on the Turing-complete expressive power of JIT-ROP payloads? We conduct a comprehensive measurement study on the effectiveness of fine-grained code randomization schemes, with 5 tools, 20 applications including 6 browsers, 1 browser engine, and 25 dynamic libraries. We provide methodologies to measure JIT-ROP gadget availability, quality, and their Turing-complete expressiveness, as well as to empirically determine the upper bound of re-randomization intervals in re-randomization schemes using the Turing-complete (TC), priority, MOV TC, and payload gadget sets. Experiments show that the upper bound ranges from 1.5 to 3.5 seconds in our tested applications. Besides, our results show that locations of leaked pointers used in JIT-ROP attacks have no impacts on gadget availability but have an impact on how fast attackers find gadgets. Our results also show that instruction-level single-round randomization thwarts current gadget finding techniques under the JIT-ROP threat model.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1803–1820},
numpages = {18},
keywords = {measurement methodology, ASLR measurement, attack surface quantification, security metrics, re-randomization interval, address/code pointer impact analysis, JITROP},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3183440.3183457,
author = {Catolino, Gemma},
title = {Effort-Oriented Methods and Tools for Software Development and Maintenance for Mobile Apps},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183457},
doi = {10.1145/3183440.3183457},
abstract = {The present research project aims to propose methods and tools for mobile applications development and maintenance that rely on effort information (estimations). Specifically, we will focus on two main challenges to overcome existing work: (i) conceiving effort estimation approaches that can be applied earlier in the development cycle and evolve through the development process (ii) prioritizing development and maintenance tasks by relying on effort estimation information.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {450–451},
numpages = {2},
keywords = {maintenance and evolution, effort estimation, mobile apps},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {24},
numpages = {35},
keywords = {SZZ, defect origin, Affected version, developing defects repository}
}

@article{10.1145/1111596.1111598,
author = {Ogasawara, Takeshi and Komatsu, Hideaki and Nakatani, Toshio},
title = {EDO: Exception-Directed Optimization in Java},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/1111596.1111598},
doi = {10.1145/1111596.1111598},
abstract = {Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs written in Java. There are two commonly used exception handling techniques, stack unwinding and stack cutting. Stack unwinding optimizes the normal path by leaving the exception handling path unoptimized, while stack cutting optimizes the exception handling path by adding extra work to the normal path. However, there has been no single exception handling technique to optimize the exception handling path without incurring any overhead to the normal path.We propose a new technique called Exception-Directed Optimization (EDO) that optimizes exception-intensive programs without slowing down exception-minimal programs. It is a feedback-directed dynamic optimization consisting of three steps: exception path profiling, exception path inlining, and throw elimination. Exception path profiling attempts to detect hot exception paths. Exception path inlining embeds every hot exception path into the corresponding catching method. Throw elimination replaces a throw with a branch to the corresponding handler. We implemented EDO in IBM's production Just-in-Time compiler and made several experiments. In summary, it improved the performance of exception-intensive programs by up to 18.3% without decreasing the performance of exception-minimal programs for SPECjvm98. We also found an opportunity for performance improvement using EDO in the startup of a Java application server.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jan},
pages = {70–105},
numpages = {36},
keywords = {inlining, exception handling, dynamic compilers, Feedback-directed dynamic optimization}
}

@article{10.1145/3127360.3127368,
author = {Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish},
title = {Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3127360.3127368},
doi = {10.1145/3127360.3127368},
abstract = {Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1145/1882362.1882410,
author = {Marcus, Andrian and Menzies, Timothy},
title = {Software is Data Too},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882410},
doi = {10.1145/1882362.1882410},
abstract = {Software systems are designed and engineered to process data. However, software is data too. The size and variety of today's software artifacts and the multitude of stakeholder activities result in so much data that individuals can no longer reason about all of it. We argue in this position paper that data mining, statistical analysis, machine learning, information retrieval, data integration, etc., are necessary solutions to deal with software data. New research is needed to adapt existing algorithms and tools for software engineering data and processes, and new ones will have to be created.In order for this type of research to succeed, it should be supported with new approaches to empirical work, where data and results are shared globally among researchers and practitioners. Software engineering researchers can get inspired by other fields, such as, bioinformatics, where results of mining and analyzing biological data are often stored in databases shared across the world.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {229–232},
numpages = {4},
keywords = {statistical analysis, software engineering, machine learning, empirical research, information retrieval, data mining},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@inproceedings{10.1109/MSR.2019.00018,
author = {Kiehn, Max and Pan, Xiangyi and Camci, Fatih},
title = {Empirical Study in Using Version Histories for Change Risk Classification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00018},
doi = {10.1109/MSR.2019.00018},
abstract = {Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {58–62},
numpages = {5},
keywords = {change risk, code ownership, machine learning, file metrics},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election Results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {0},
numpages = {63}
}

@inproceedings{10.1145/3416508.3417120,
author = {Salimi, Solmaz and Ebrahimzadeh, Maryam and Kharrazi, Mehdi},
title = {Improving Real-World Vulnerability Characterization with Vulnerable Slices},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417120},
doi = {10.1145/3416508.3417120},
abstract = {Vulnerability detection is an important challenge in the security community. Many different techniques have been proposed, ranging from symbolic execution to fuzzing in order to help in identifying vulnerabilities. Even though there has been considerable improvement in these approaches, they perform poorly on a large scale code basis. There has also been an alternate approach, where software metrics are calculated on the overall code structure with the hope of predicting code segments more likely to be vulnerable. The logic has been that more complex code with respect to the software metrics, will be more likely to contain vulnerabilities.  In this paper, we conduct an empirical study with a large dataset of vulnerable codes to discuss if we can change the way we measure metrics to improve vulnerability characterization. More specifically, we introduce vulnerable slices as vulnerable code units to measure the software metrics and then use these new measured metrics to characterize vulnerable codes. The result shows that vulnerable slices significantly increase the accuracy of vulnerability characterization. Further, we utilize vulnerable slices to analyze the dataset of known vulnerabilities, particularly to observe how by using vulnerable slices the size and complexity changes in real-world vulnerabilities.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {11–20},
numpages = {10},
keywords = {Vulnerability Prediction, Static Analysis, Program Slicing, Vulnerability Characterization},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3338906.3340457,
author = {Maddila, Chandra and Bansal, Chetan and Nagappan, Nachiappan},
title = {Predicting Pull Request Completion Time: A Case Study on Large Scale Cloud Services},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340457},
doi = {10.1145/3338906.3340457},
abstract = {Effort estimation models have been long studied in software engineering research. Effort estimation models help organizations and individuals plan and track progress of their software projects and individual tasks to help plan delivery milestones better. Towards this end, there is a large body of work that has been done on effort estimation for projects but little work on an individual checkin (Pull Request) level. In this paper we present a methodology that provides effort estimates on individual developer check-ins which is displayed to developers to help them track their work items. Given the cloud development infrastructure pervasive in companies, it has enabled us to deploy our Pull Request Lifetime prediction system to several thousand developers across multiple software families. We observe from our deployment that the pull request lifetime prediction system conservatively helps save 44.61% of the developer time by accelerating Pull Requests to completion.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {874–882},
numpages = {9},
keywords = {Empirical Studies, Prediction, Software Metrics, Case Studies, Effort Estimation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3340482.3342747,
author = {Lenarduzzi, Valentina and Martini, Antonio and Taibi, Davide and Tamburri, Damian Andrew},
title = {Towards Surgically-Precise Technical Debt Estimation: Early Results and Research Roadmap},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342747},
doi = {10.1145/3340482.3342747},
abstract = {The concept of technical debt has been explored from many perspectives but its precise estimation is still under heavy empirical and experimental inquiry. We aim to understand whether, by harnessing approximate, data-driven, machine-learning approaches it is possible to improve the current techniques for technical debt estimation, as represented by a top industry quality analysis tool such as SonarQube. For the sake of simplicity, we focus on relatively simple regression modelling techniques and apply them to modelling the additional project cost connected to the sub-optimal conditions existing in the projects under study. Our results shows that current techniques can be improved towards a more precise estimation of technical debt and the case study shows promising results towards the identification of more accurate estimation of technical debt.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {37–42},
numpages = {6},
keywords = {Empirical Study, Technical Debt, Machine Learning},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3357141.3357147,
author = {Sousa, Bruno L. and Bigonha, Mariza A. S. and Ferreira, Kecia A. M.},
title = {Analysis of Coupling Evolution on Open Source Systems},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357147},
doi = {10.1145/3357141.3357147},
abstract = {Software evolution is an intrinsic process of the software life cycle. The comprehension of this process is a central research topic in Software Engineering. It is widely accepted that as a software system evolves, its internal quality declines, and its complexity increases. However, there is a gap in the comprehension of how this process occurs in a fine-grained view. In this work, we apply a software metric approach to investigate how the internal quality of object-oriented software systems evolves in the aspect of coupling. More specifically, we analyze (i) how the coupling behavior may be described over the software evolution, (ii) how the coupling behavior affects the reusability and complexity of the systems, and (iii) the percentage of classes from the systems that directly impacts on the coupling evolution. The results and observations of this study are compiled in seven properties of coupling evolution, among which stand out: (i) the coupling behavior is better modeled by a cubic function, (ii) the coupling evolution tends to increase the complexity of the systems, (iii) the systems tend to be designed with a high level of complexity, and (iv) the coupling evolution is affected by a small group of classes.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {23–32},
numpages = {10},
keywords = {object-orientation, software metrics, coupling, open source, software evolution, software quality},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1109/ICSE-Companion.2019.00044,
author = {Hoang, Thong and Lawall, Julia and Oentaryo, Richard J. and Tian, Yuan and Lo, David},
title = {PatchNet: A Tool for Deep Patch Classification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00044},
doi = {10.1109/ICSE-Companion.2019.00044},
abstract = {This work proposes PatchNet, an automated tool based on hierarchical deep learning for classifying patches by extracting features from commit messages and code changes. PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of a code change, differentiating it from the existing deep learning models on source code. PatchNet provides several options allowing users to select parameters for the training process. The tool has been validated in the context of automatic identification of stable-relevant patches in the Linux kernel and is potentially applicable to automate other software engineering tasks that can be formulated as patch classification problems. A video demonstrating PatchNet is available at https://goo.gl/CZjG6X. The PatchNet implementation is available at https://github.com/hvdthong/PatchNetTool.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {83–86},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/3400397.3400648,
author = {Weaver, Gabriel A. and Van Moer, Mark and Salo, Glen R.},
title = {Stakeholder-Centric Analyses of Simulated Shipping Port Disruptions},
year = {2019},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {The Maritime Transportation System is crucial to the global economy, accounting for more than 80% of global merchandise trade in volume and 67% of its value in 2017. Within the US economy alone, this system accounted for roughly a quarter of GDP in 2018. This paper defines an approach to measure the degree to which individual stakeholders, when disrupted, affect the commodity flows of other stakeholders and the entire port. Using a simulation model based on heterogeneous datasets gathered from fieldwork with Port Everglades in FL, we look at the effect of varying the timing and location of disruptions, as well as response actions, on the flow of imported commodities. Insights based upon our model inform how and when stakeholders can impact one another's operations and should thereby provide a data-driven, strategic approach to inform the security plans of individual companies and shipping ports as a whole.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3128–3139},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3368089.3417056,
author = {Araujo, Frederico and Taylor, Teryl},
title = {Improving Cybersecurity Hygiene through JIT Patching},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417056},
doi = {10.1145/3368089.3417056},
abstract = {Vulnerability patch management remains one of the most complex issues facing modern enterprises; companies struggle to test and deploy new patches across their networks, often leaving myriad attack vectors vulnerable to exploits. This problem is exacerbated by enterprise server applications, which expose tremendous amounts of information about their security postures, greatly expediting attackers' reconnaissance incursions (e.g., knowledge gathering attacks). Unfortunately, current patching processes offer no insights into attacker activities, and prompt attack remediation is hindered by patch compatibility considerations and deployment cycles.  To reverse this asymmetry, a patch management model is proposed to facilitate the rapid injection of software patches into live, commodity applications without disruption of production workflows, and the transparent sandboxing of suspicious processes for counterreconnaissance and threat information gathering. Our techniques improve workload visibility and vulnerability management, and overcome perennial shortcomings of traditional patching methodologies, such as proneness to attacker fingerprinting, and the high cost of deployment. The approach enables a large variety of novel defense scenarios, including rapid security patch testing with prompt recovery from defective patches and the placement of exploit sensors inlined into production workloads. An implementation for six enterprise-grade server programs demonstrates that our approach is practical and incurs minimal runtime overheads. Moreover, four use cases are discussed, including a practical deployment on two public cloud environments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1421–1432},
numpages = {12},
keywords = {cyber deception, hot patching, security patching},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3196321.3196326,
author = {Li, Xiaochen and Jiang, He and Liu, Dong and Ren, Zhilei and Li, Ge},
title = {Unsupervised Deep Bug Report Summarization},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196326},
doi = {10.1145/3196321.3196326},
abstract = {Bug report summarization is an effective way to reduce the considerable time in wading through numerous bug reports. Although some supervised and unsupervised algorithms have been proposed for this task, their performance is still limited, due to the particular characteristics of bug reports, including the evaluation behaviours in bug reports, the diverse sentences in software language and natural language, and the domain-specific predefined fields. In this study, we conduct the first exploration of the deep learning network on bug report summarization. Our approach, called DeepSum, is a novel stepped auto-encoder network with evaluation enhancement and predefined fields enhancement modules, which successfully integrates the bug report characteristics into a deep neural network. DeepSum is unsupervised. It significantly reduces the efforts on labeling huge training sets. Extensive experiments show that DeepSum outperforms the comparative algorithms by up to 13.2% and 9.2% in terms of F-score and Rouge-n metrics respectively over the public datasets, and achieves the state-of-the-art performance. Our work shows promising prospects for deep learning to summarize millions of bug reports.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {144–155},
numpages = {12},
keywords = {deep learning, unsupervised learning, mining software repositories, bug report summarization},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@article{10.1145/2047478.2047482,
author = {O'Grady, Michael and O'Hare, Gregory and O'Donoghue, John},
title = {Pervasive Computing Technologies for Healthcare},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/2047478.2047482},
doi = {10.1145/2047478.2047482},
abstract = {The conference series on Pervasive Computing Technologies for Healthcare is one of the leading fora for research dissemination in this space. In May 2011, the most recent event took place in Ireland. A brief overview of the conference is now presented.},
journal = {SIGHIT Rec.},
month = {sep},
pages = {27–28},
numpages = {2},
keywords = {conference review, research themes, health information systems}
}

@article{10.1145/3398069,
author = {Thieme, Anja and Belgrave, Danielle and Doherty, Gavin},
title = {Machine Learning in Mental Health: A Systematic Review of the HCI Literature to Support the Development of Effective and Implementable ML Systems},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3398069},
doi = {10.1145/3398069},
abstract = {High prevalence of mental illness and the need for effective mental health care, combined with recent advances in AI, has led to an increase in explorations of how the field of machine learning (ML) can assist in the detection, diagnosis and treatment of mental health problems. ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. Despite the potential opportunities for using ML within mental health, this is an emerging research area, and the development of effective ML-enabled applications that are implementable in practice is bound up with an array of complex, interwoven challenges. Aiming to guide future research and identify new directions for advancing development in this important domain, this article presents an introduction to, and a systematic review of, current ML work regarding psycho-socially based mental health conditions from the computing and HCI literature. A quantitative synthesis and qualitative narrative review of 54 papers that were included in the analysis surfaced common trends, gaps, and challenges in this space. Discussing our findings, we (i) reflect on the current state-of-the-art of ML work for mental health, (ii) provide concrete suggestions for a stronger integration of human-centered and multi-disciplinary approaches in research and development, and (iii) invite more consideration of the potentially far-reaching personal, social, and ethical implications that ML models and interventions can have, if they are to find widespread, successful adoption in real-world mental health contexts.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {aug},
articleno = {34},
numpages = {53},
keywords = {Mental health, real-world interventions, systematic review, health care, AI applications, ethics, interpretability, interaction design, mental illness, society + AI, machine learning}
}

@inproceedings{10.1145/3205651.3208262,
author = {Ebert, Samuel and Farhana, Effat and Heber, Steffen},
title = {A Parallel Island Model for Biogeography-Based Classification Rule Mining in Julia},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208262},
doi = {10.1145/3205651.3208262},
abstract = {In this paper, we present a distributed island model implementation of biogeography-based optimization for classification rule mining (island BBO-RM). Island BBO-RM is an evolutionary algorithm for rule mining that uses Pittsburgh style classification rule encoding, which represents an entire ruleset (classifier) as a single chromosome. Our algorithm relies on biogeography-based optimization (BBO), an optimization technique that is inspired by species migration pattern between habitats. Biogeography-based optimization has been reported to perform well in various applications ranging from function optimization to image classification. A major limitation of evolutionary rule mining algorithms is their high computational cost and running time. To address this challenge, we have applied a distributed island model to parallelize the rule extraction phase via BBO. We have explored several different migration topologies and data windowing techniques. Our algorithm is implemented in Julia, a dynamic programming language designed for high-performance and parallel computation. Our results show that our distributed implementation is able to achieve considerable speedups when compared to a serial implementation. Without data windowing, we obtain speedups up to a factor of nine without a loss of classification accuracy. With data windowing, we obtain speedups up to a factor of 30 with a small loss of accuracy in some cases.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1284–1291},
numpages = {8},
keywords = {evolutionary algorithms, biogeography-based optimization, genetics-based machine learning, island model},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1145/3344256,
author = {Li, Dingwen and Vaidya, Jay and Wang, Michael and Bush, Ben and Lu, Chenyang and Kollef, Marin and Bailey, Thomas},
title = {Feasibility Study of Monitoring Deterioration of Outpatients Using Multimodal Data Collected by Wearables},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3344256},
doi = {10.1145/3344256},
abstract = {In the article, we explore the feasibility of monitoring outpatients using Fitbit Charge HR wristbands and the potential of machine learning models to predict clinical deterioration (readmissions and death) among outpatients discharged from the hospital. We developed and piloted a data collection system in a clinical study that involved 25 heart failure patients recently discharged. The results demonstrated the feasibility of continuously monitoring outpatients using wristbands. We observed high levels of patient compliance in wearing the wristbands regularly and satisfactory yield, latency, and reliability of data collection from the wristbands to a cloud-based database. Finally, we explored a set of machine learning models to predict deterioration based on the Fitbit data. Through fivefold cross-validation, K nearest neighbor achieved the highest accuracy of 0.8667 for identifying patients at risk of deterioration using the data collected from the beginning of the monitoring. Machine learning models based on multimodal data (step, sleep, and heart rate) significantly outperformed the traditional clinical approach based on LACE index. Moreover, our proposed Weighted Samples One-Class SVM model with estimated confidence can reach high accuracy (0.9635) for predicting the deterioration using data collected within a sliding window, which indicates the potential for allowing timely intervention.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {5},
numpages = {22},
keywords = {ubiquitous computing, wearable tracker, heart failure, Medical data mining, deterioration early warning}
}

@inproceedings{10.5555/3242181.3242380,
author = {Zeng, Ningshuang and Dichtl, Maximilian and K\"{o}nig, Markus},
title = {A Scenario-Based Simulation Framework of on- and off-Site Construction Logistics},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Constructing a building is a very complex process including a vast number and variety of participants, tasks, materials and elements. Interdependencies between the on- and off-site logistics frequently lead to an error-prone and chaotic process. However, good communication can reduce the effects by enabling the different actors to react. Simulation is a classic approach to understand complex problems and has been widely applied in the construction field. Nevertheless, the existing construction simulation approaches still do not consider the entire construction logistics process. This paper proposes a concept to model and simulate on- and off-site construction logistics, to facilitate the understanding of the boundary-spanning dependencies of both on- and off-site domains. A framework is provided, including simulation fundamentals (i.e. logistic BIM model), simulation data preparation (i.e. process pattern definition) and implementation of a scenario-based simulation. Finally, a discussion of the possible benefits and improvements of the proposed approaches is provided.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {188},
numpages = {12},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3269206.3271811,
author = {Loyola, Pablo and Gajananan, Kugamoorthy and Satoh, Fumiko},
title = {Bug Localization by Learning to Rank and Represent Bug Inducing Changes},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271811},
doi = {10.1145/3269206.3271811},
abstract = {In software development, bug localization is the process finding portions of source code associated to a submitted bug report. This task has been modeled as an information retrieval task at source code file, where the report is the query. In this work, we propose a model that, instead of working at file level, learns feature representations from source changes extracted from the project history at both syntactic and code change dependency perspectives to support bug localization. To that end, we structured an end-to-end architecture able to integrate feature learning and ranking between sets of bug reports and source code changes. We evaluated our model against the state of the art of bug localization on several real world software projects obtaining competitive results in both intra-project and cross-project settings. Besides the positive results in terms of model accuracy, as we are giving the developer not only the location of the bug associated to the report, but also the change that introduced, we believe this could give a broader context for supporting fixing tasks.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {657–665},
numpages = {9},
keywords = {source code changes, bug localization, information retrieval},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.5555/1874620.1874946,
author = {Holst, Stefan and Wunderlich, Hans-Joachim},
title = {A Diagnosis Algorithm for Extreme Space Compaction},
year = {2009},
isbn = {9783981080155},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {During volume testing, test application time, test data volume and high performance automatic test equipment (ATE) are the major cost factors. Embedded testing including built-in self-test (BIST) and multi-site testing are quite effective cost reduction techniques which may make diagnosis more complex. This paper presents a test response compaction scheme and a corresponding diagnosis algorithm which are especially suited for BIST and multi-site testing. The experimental results on industrial designs show, that test time and response data volume reduces significantly and the diagnostic resolution even improves with this scheme. A comparison with X-Compact indicates, that simple parity information provides higher diagnostic resolution per response data bit than more complex signatures.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {1355–1360},
numpages = {6},
keywords = {diagnosis, multi-site test, design-for-test, embedded diagnosis, compaction},
location = {Nice, France},
series = {DATE '09}
}

@inproceedings{10.1145/3230833.3230851,
author = {Salfer, Martin and Eckert, Claudia},
title = {Attack Graph-Based Assessment of Exploitability Risks in Automotive On-Board Networks},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3230851},
doi = {10.1145/3230833.3230851},
abstract = {High-end vehicles incorporate about one hundred computers; physical and virtualized ones; self-driving vehicles even more. This allows a plethora of attack combinations. This paper demonstrates how to assess exploitability risks of vehicular on-board networks via automatically generated and analyzed attack graphs. Our stochastic model and algorithm combine all possible attack vectors and consider attacker resources more efficiently than Bayesian networks. We designed and implemented an algorithm that assesses a compilation of real vehicle development documents within only two CPU minutes, using an average of about 100 MB RAM. Our proof of concept "Security Analyzer for Exploitability Risks" (SAlfER) is 200 to 5 000 times faster and 40 to 200 times more memory-efficient than an implementation with UnBBayes1. Our approach aids vehicle development by automatically re-checking the architecture for attack combinations that may have been enabled by mistake and which are not trivial to spot by the human developer. Our approach is intended for and relevant for industrial application. Our research is part of a collaboration with a globally operating automotive manufacturer and is aimed at supporting the security of autonomous, connected, electrified, and shared vehicles.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {21},
numpages = {10},
keywords = {Security Evaluation, Vehicle Security, Vulnerability Assessment, Attack Graph Construction, Network Hardening, Probabilistic Model},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/2810103.2810129,
author = {Backes, Michael and Schranz, Oliver and von Styp-Rekowsky, Philipp},
title = {POSTER: Towards Compiler-Assisted Taint Tracking on the Android Runtime (ART)},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2810129},
doi = {10.1145/2810103.2810129},
abstract = {Dynamic analysis and taint tracking on Android was typically implemented by instrumenting the Dalvik Virtual Machine. However, the new Android Runtime (ART) introduced in Android 5 replaces the interpreter with an on-device compiler suite. Therefore as of Android 5, the applicability of interpreter instrumentation-based approaches like TaintDroid is limited to Android versions up to 4.4 Kitkat. In this poster, we present ongoing work on re-enabling taint tracking for apps by instrumenting the Optimizing backend, used by the new ART compiler suite for code generation. As Android now compiles apps ahead-of-time from dex bytecode to platform specific native code on the device itself, an instrumented compiler provides the opportunity to emit additional instructions that enable the actual taint tracking. The result is a custom compiler that takes arbitrary app APKs and transforms them into self-taint tracking native code, executable by the Android Runtime.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1629–1631},
numpages = {3},
keywords = {android, taint tracking, ART, information flow control, compiler},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.5555/3021426.3021429,
author = {Guan, Qiang and BeBardeleben, Nathan and Wu, Panruo and Eidenbenz, Stephan and Blanchard, Sean and Monroe, Laura and Baseman, Elisabeth and Tan, Li},
title = {Design, Use and Evaluation of P-FSEFI: A Parallel Soft Error Fault Injection Framework for Emulating Soft Errors in Parallel Applications},
year = {2016},
isbn = {9781631901201},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Future exascale application programmers and users have a need to quantity an application's resilience and vulnerability to soft errors before running their codes on production supercomputers due to the cost of failures and hazards from silent data corruption. Barring a deep understanding of the resiliency of a particular application, vulnerability evaluation is commonly done through fault injection tools at either the software or hardware level. Hardware fault injection, while most realistic, is relegated to customized vendor chips and usually applications cannot be evaluated at scale. Software fault injection can be done more practically and efficiently and is the approach that many researchers use as a reasonable approximation. With a sufficiently sophisticated software fault injection framework, an application can be studied to see how it would handle many of the errors that manifest at the application level. Using such a tool, a developer can progressively improve the resilience at targeted locations they believe are important for their target hardware.},
booktitle = {Proceedings of the 9th EAI International Conference on Simulation Tools and Techniques},
pages = {9–17},
numpages = {9},
keywords = {fault injection, soft error, MPI, resilience, High Performance Computing, vulnerability},
location = {Prague, Czech Republic},
series = {SIMUTOOLS'16}
}

@inproceedings{10.1109/IWoR.2019.00015,
author = {Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
title = {Toward Proactive Refactoring: An Exploratory Study on Decaying Modules},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IWoR.2019.00015},
doi = {10.1109/IWoR.2019.00015},
abstract = {Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells.To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.},
booktitle = {Proceedings of the 3rd International Workshop on Refactoring},
pages = {39–46},
numpages = {8},
keywords = {code smell, code quality, refactoring},
location = {Montreal, Quebec, Canada},
series = {IWOR '19}
}

