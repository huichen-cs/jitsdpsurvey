# Summary of JIT-SDP Studies

This table lists JIT-SDP studies and for each, we provide a one-sentence
summary and assign a category of the following 5 categories,

1. Data. The study examines noise reduction in data, and distribution/characteristics of data
2. Model. The study proposes a new JIT-SDP model.
3. Feature/Metrics. The study proposes a new software metrics or examines existing ones
    in JIT-SDP
4. Tool. The study proposes a new JIT-SDP tool or system.
5. Field Test. The study investigates effectiveness JIT-SDP in a particular
    domain of software applications.


| No.  | Paper                                                        | Category (Tags)                                              | Summary                                                      |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | The Impact of Duplicate Changes on Just-in-Time Defect Prediction | Data (noise reduction, duplicate changes)                    | Impact of duplicate changes, i.e., identical changes applied to multiple SCM branches on prediction performance |
| 2    | Simplified Deep Forest Model Based Just-in-Time Defect Prediction for Android Mobile Apps | Model (within-project model, application domain)             | Assessment of a custom deep forest model for Android mobile apps |
| 3    | CC2Vec: Distributed representations of code changes          | Feature/Metrics (feature representation)                     | Building a convolutional network to extract feature representations of software changes considering change structure and to use the features for defect prediction |
| 4    | Predicting just-in-time software defects to reduce post-release quality costs in the maritime industry | Field Test (SDLC, application domain)                        | Within and cross-project SDP comparison and cost-benefit analysis for post-release changes in maritime software |
| 5    | An investigation of cross-project learning in online just-in-time software defect prediction | Model (cross-project model, online model)                    | Assessment of cross-project SDP with an online learning (data stream learning) model |
| 6    | How Well Just-In-Time Defect Prediction Techniques Enhance Software Reliability? | Model (within-project model, software reliability, SDLC)     | Evaluations of long-term JIT-SDP for reliability improvement in terms of the usage-weighted defects and short-term JIT-SDP for early defect prediction |
| 7    | Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction | Feature/Metrics (sub-change prediction, metrics, SZZ)        | Investigation of extensive list of metrics including static analysis warnings and improved SZZ algorithm for sub-change (files in a change) defect prediction |
| 8    | Revisiting the Impact of Concept Drift on Just-in-Time Quality Assurance | Data (concept drift)                                         | Investigation of concept drift within software projects      |
| 9    | Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network | Model  (within-project model, cross-project model)           | Investigation of effectiveness of convolutional neural networks on defect prediction using software metrics as input features |
| 10   | Effort-aware just-in-time defect identification in practice: a case study at Alibaba | Field Test (prediction setting, application domain)          | Investigation of the effectiveness of supervised (CBS+, CBS, OneWay, and EALR) and unsupervised (LT and Code Churn) effort-aware JIT-SDP in an industry setting (on Alibaba projects) |
| 11   | JITBot: An Explainable Just-In-Time Defect Prediction Bot    | Tool (prediction setting, applications of JIT-SDP, explainable) | Design of explainable JIT-SDP bot that “explains” a defect prone change with the “contribution” of software metrics to the defect proneness |
| 12   | Effort-Aware semi-Supervised just-in-Time defect prediction  | Model (EATT, learning setting)                               | Investigation of semi-supervised effort-aware JIT-SDP using a tri-training method (also see Zhang et al. [paper 20]) |
| 13   | Cross-project just-in-time bug prediction for mobile apps: an empirical assessment | Model (application domain, cross-project model)              | Investigation of cross-project JIT-SDP in mobile platforms and comparison of four classifiers and four ensemble techniques |
| 14   | The impact of context metrics on just-in-time defect prediction | Feature/Metrics (change context metrics, indentation metrics) | Design and investigation of “context metrics”, a metric measure the complexity or the size of the surrounding lines of a change |
| 15   | The impact of changes mislabeled by SZZ on just-in-time defect prediction | Data (noise reduction, SZZ)                                  | Investigation of impact of SZZ algorithms and labeling errors |
| 16   | DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction | Model (defect proneness, deep learning, within-project)      | Investigation of cross-validation, short-term, and long-term prediction of convolutional neural networks using tokens (words) from both commit messages and changes as input |
| 17   | Fine-grained just-in-time defect prediction                  | Model (prediction setting, sub-change prediction)            | Predicting whether in the specific files, contained in a commit, that are defect-inducing |
| 18   | Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction | Model (CBS+, effort-aware)                                   | Investigation of a supervised effort-aware model (called CBS+) combining Kamei et al.’s supervised EALR model [paper 39] and Yang et al.’s unsupervised LT [paper 32] |
| 19   | Class imbalance evolution and verification latency in just-in-time software defect prediction | Model (online learning, concept drift, class imbalance evolution) | Investigation of an Oversampling Online Bagging (ORB) to tackle class imbalance evolution in an online JIT-SDP scenario while considering verification latency |
| 20   | Effort-Aware Tri-Training for Semi-supervised Just-in-Time Defect Prediction | Model (EATT, learning setting)                               | Investigation of semi-supervised effort-aware JITSDP using a tri-training method (also see Li et al. [paper 12]) |
| 21   | Bridging effort-aware prediction and strong classification: a just-in-time software defect prediction study | Model (with-project)                                         | Investigation of the relationship between classification performance and the cost-effectiveness performance metrics to obtain insights, e.g., that there is great variability in repair effort. |
| 22   | A replication study: just-in-time defect prediction with ensemble learning | Model (DSL, with-project)                                    | Comparison of the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning algorithms |
| 23   | MULTI: Multi-objective effort-aware just-in-time software defect prediction | Model (MULTI, effort-aware, with-project)                    | Formulating prediction as a dual-objective optimization problem based on logistic regression and NSGA-II to balance the benefit, i.e., the predicted defects and the cost, i.e., the review efforts |
| 24   | Revisiting unsupervised learning for defect prediction       | Model (OneWay, effort-aware, within-project)                 | Investigation of Yang et al.’s unsupervised models (LT) and the proposed OneWay that uses the supervise models to prune unsupervised models |
| 25   | Supervised vs unsupervised models: A holistic look at effort-aware just-in-time defect prediction | Model (CBS, effort-ware, within-project)                     | Investigation of a supervised effort-aware model (called CBS) combining Kamei et al.’s supervised EALR model [paper 39] and Yang et al.’s unsupervised LT [paper 32] |
| 26   | Code churn: A neglected metric in effort-aware just-in-time defect prediction | Model (CCUM, effort-aware, unsupervised)                     | Investigation of the effectiveness of code churn based unsupervised defect prediction model (CCUM) for effort-aware prediction |
| 27   | Are fix-inducing changes a moving target? a longitudinal case study of just-in-time defect prediction | Model (prediction setting, long-period, short-period)        | Investigation of evolving nature of software project with software projects with insights, such as, JIT models that should be retrained using recently recorded data |
| 28   | TLEL: A two-layer ensemble learning approach for just-in-time defect prediction | Model (TLEL, effort-aware, within-project)                   | Investigation of a two-layer ensemble model (TLEL) for effort-aware prediction |
| 29   | Studying just-in-time defect prediction using cross-project models | Model (prediction setting, cross-project)                    | Examination of an ensemble approach for cross-project JIT-SDP with random forest base learner (also see Fukushima et al. [paper 30]) |
| 30   | The impact of human discussions on just-in-time quality assurance: An empirical study on OpenStack and Eclipse | Feature/Metrics (human discussions, issue reports, issue discussions, code reviews) | Investigation of using issue and review discussions to predict the defect-proneness of software patches |
| 31   | The impact of tangled code changes on defect prediction models | Data (denoising, tangled changes)                            | investigation of tangled changes on defect prediction and examination of a multi-predictor approach to untangle changes |
| 32   | Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models | Model (LT, AGE, NUC, Entropy, unsupervised model, effort-aware) | investigation of the predictive power of simple unsupervised models, such as, LT and AGE in effort-aware JIT defect prediction and comparison of a variety of supervised and unsupervised models |
| 33   | Commit Guru: analytics and risk prediction of software commits | Tool                                                         | Describing a publicly available defect prediction tool called Commit Guru |
| 34   | Online defect prediction for imbalanced data                 | Model (online)                                               | Proposing an online JIT-SDP model and investigating class imbalance problem and time-sensitive change classification for defect prediction where bag-of-words feature of commit message, static code metrics, the node type in abstract syntax trees and meta data features. |
| 35   | Deep learning for just-in-time defect prediction             | Model (Deeper, defect-proneness, within-project)             | Proposing a model called Deeper consisting of a deep belief network and a logistic regression classifier to predict defect proneness of software changes |
| 36   | An empirical study of just-in-time defect prediction using cross-project models | Model (prediction setting, cross-project)                    | Examination of an ensemble approach for cross-project JIT-SDP with random forest base learner (also see Kamei et al. [paper 29]) |
| 37   | Personalized defect prediction                               | Model (PCC+, personalized model, within-project)             | Building (file) change-level defect prediction model for each developer from file modification histories (i.e., a personalized defect prediction) |
| 38   | Improving the quality of software by quantifying the code change metric and predicting the bugs | Feature/Metrics (entropy metrics)                            | Investigation of entropy change metrics, metrics decay (aging) function, and defect prediction model using linear regression and support vector machine for defect prediction |
| 39   | A large-scale empirical study of just-in-time quality assurance | Model (defect-proneness, effort-aware, within-project)       | Predicting defect-proneness of software changes with logistic regression and quality assurance effort of software changes with linear regression (EALR) from software metrics |
| 40   | Classifying software changes: Clean or buggy?                | Model (defect-proneness)                                     | Predicting with a Support Vector Machine (SVM) using the bag-of-words features of the identifiers in added and deleted source code and the words in file change logs to classify changes as being defect-inducing or clean |
| 41   | Predicting risk of software changes                          | Model (defect-proneness, prediction setting)                 | Predicting from software change metrics with logistic regression the defect-proneness of the Initial Modification Requests (IMR) in 5ESS network switch project where IMRs may consist of multiple Modification Requests (MR) corresponding to multiple changes |


## References
1. Kwabena E Bennin, Nauman bin Ali, Jürgen Börstler, and Xiao Yu. 2020. Revisiting the Impact of Concept Drift on Just-in-Time Quality Assurance. In 2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS). IEEE, 53–59.
2. George G Cabral, Leandro L Minku, Emad Shihab, and Suhaib Mujahid. 2019. Class imbalance evolution and verifica- tion latency in just-in-time software defect prediction. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 666–676.
3. Gemma Catolino, Dario Di Nucci, and Filomena Ferrucci. 2019. Cross-project just-in-time bug prediction for mobile apps: an empirical assessment. In 2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft). IEEE, 99–110.
4. Xiang Chen, Yingquan Zhao, Qiuping Wang, and Zhidan Yuan. 2018. MULTI: Multi-objective effort-aware just-in-time software defect prediction. Information and Software Technology 93 (2018), 1–13.
5. Ruifeng Duan, Haitao Xu, Yuanrui Fan, and Meng Yan. 2021. The Impact of Duplicate Changes on Just-in-Time Defect Prediction. IEEE Transactions on Reliability (2021).
6. Yuanrui Fan, Xin Xia, Daniel Alencar da Costa, David Lo, Ahmed E Hassan, and Shanping Li. 2019. The impact of changes mislabeled by SZZ on just-in-time defect prediction. IEEE transactions on software engineering (2019).
7. Wei Fu and Tim Menzies. 2017. Revisiting unsupervised learning for defect prediction. In Proceedings of the 2017 11th joint meeting on foundations of software engineering. 72–83.
8. TakafumiFukushima,YasutakaKamei,ShaneMcIntosh,KazuhiroYamashita,andNaoyasuUbayashi.2014. Anempirical study of just-in-time defect prediction using cross-project models. In Proceedings of the 11th Working Conference on Mining Software Repositories. 172–181.
9. Yuchen Guo, Martin Shepperd, and Ning Li. 2018. Bridging effort-aware prediction and strong classification: a just- in-time software defect prediction study. In Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings. 325–326.
10. Kim Herzig, Sascha Just, and Andreas Zeller. 2016. The impact of tangled code changes on defect prediction models. Empirical Software Engineering 21, 2 (2016), 303–336.
11. Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 34–45.
12. Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 518–529.
13. Qiao Huang, Xin Xia, and David Lo. 2017. Supervised vs unsupervised models: A holistic look at effort-aware just-in- time defect prediction. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 159–170.
14. Qiao Huang, Xin Xia, and David Lo. 2019. Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction. Empirical Software Engineering 24, 5 (2019), 2823–2862.
15. Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction. In 2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE). Ieee, 279–289.
16. Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita, Naoyasu Ubayashi, and Ahmed E Hassan. 2016. Studying just-in-time defect prediction using cross-project models. Empirical Software Engineering 21, 5 (2016), 2072–2106.
17. Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus, Anand Sinha, and Naoyasu Ubayashi. 2012. A large-scale empirical study of just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6 (2012), 757–773.
18. Jonggu Kang, Duksan Ryu, and Jongmoon Baik. 2020. Predicting just-in-time software defects to reduce post-release quality costs in the maritime industry. Software: Practice and Experience (2020).
19. Chaiyakarn Khanan, Worawit Luewichana, Krissakorn Pruktharathikoon, Jirayus Jiarpakdee, Chakkrit Tantithamtha- vorn, Morakot Choetkiertikul, Chaiyong Ragkhitwetsagul, and Thanwadee Sunetnanta. 2020. JITBot: An Explainable Just-In-Time Defect Prediction Bot. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1336–1339.
20. SunghunKim,EJamesWhitehead,andYiZhang.2008. Classifyingsoftwarechanges:Cleanorbuggy? IEEETransactions on Software Engineering 34, 2 (2008), 181–196.
21. Masanari Kondo, Daniel M German, Osamu Mizuno, and Eun-Hye Choi. 2020. The impact of context metrics on just-in-time defect prediction. Empirical Software Engineering 25, 1 (2020), 890–939.
22. Weiwei Li, Wenzhou Zhang, Xiuyi Jia, and Zhiqiu Huang. 2020. Effort-aware semi-supervised just-in-time defect prediction. Information and Software Technology 126 (2020), 106364.
23. Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. 2017. Code churn: A neglected metric in effort- aware just-in-time defect prediction. In 2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). IEEE, 11–19.
24. Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a moving target? a longitudinal case study of just-in-time defect prediction. IEEE Transactions on Software Engineering 44, 5 (2017), 412–428.
25. Audris Mockus and David M Weiss. 2000. Predicting risk of software changes. Bell Labs Technical Journal 5, 2 (2000), 169–180.
26. Luca Pascarella, Fabio Palomba, and Alberto Bacchelli. 2019. Fine-grained just-in-time defect prediction. Journal of Systems and Software 150 (2019), 22–36.
27. Christoffer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit Guru: analytics and risk prediction of software commits. In Proceedings of the 2015 10th joint meeting on foundations of software engineering. 966–969.
28. VB Singh and KK Chaturvedi. 2013. Improving the quality of software by quantifying the code change metric and predicting the bugs. In International conference on computational science and its applications. Springer, 408–426.
29. Sadia Tabassum, Leandro L Minku, Danyi Feng, George G Cabral, and Liyan Song. 2020. An investigation of cross- project learning in online just-in-time software defect prediction. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEE, 554–565.
30. Ming Tan, Lin Tan, Sashank Dara, and Caleb Mayeux. 2015. Online defect prediction for imbalanced data. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 2. IEEE, 99–108.
31. Yuli Tian, Ning Li, Jeff Tian, and Wei Zheng. 2020. How Well Just-In-Time Defect Prediction Techniques Enhance Software Reliability?. In 2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS). IEEE, 212–221.
32. Parastou Tourani and Bram Adams. 2016. The impact of human discussions on just-in-time quality assurance: An empirical study on openstack and eclipse. In 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER), Vol. 1. IEEE, 189–200.
33. Alexander Trautsch, Steffen Herbold, and Jens Grabowski. 2020. Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 127–138.
34. Meng Yan, Xin Xia, Yuanrui Fan, David Lo, Ahmed E Hassan, and Xindong Zhang. 2020. Effort-aware just-in-time defect identification in practice: a case study at Alibaba. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1308–1319.
35. Xinli Yang, David Lo, Xin Xia, and Jianling Sun. 2017. TLEL: A two-layer ensemble learning approach for just-in-time defect prediction. Information and Software Technology 87 (2017), 206–220.
36. Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning for just-in-time defect prediction. In 2015 IEEE International Conference on Software Quality, Reliability and Security. IEEE, 17–26.
37. Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei Xu, Baowen Xu, and Hareton Leung. 2016. Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models. In Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering. 157–168.
38. Steven Young, Tamer Abdou, and Ayse Bener. 2018. A replication study: just-in-time defect prediction with ensemble learning. In Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering. 42–47.
39. Wenzhou Zhang, Weiwei Li, and Xiuyi Jia. 2019. Effort-Aware Tri-Training for Semi-supervised Just-in-Time Defect Prediction. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 293–304.
40. K. Zhao, Z. Xu, T. Zhang, Y. Tang, and M. Yan. 2021. Simplified Deep Forest Model Based Just-in-Time Defect Prediction for Android Mobile Apps. IEEE Transactions on Reliability (2021), 1–12. https://doi.org/10.1109/TR.2021.3060937
41. Kun Zhu, Nana Zhang, Shi Ying, and Dandan Zhu. 2020. Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network. IET Software 14, 3 (2020), 185–195.

